[
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "it's a rough forum post: https://ethresear.ch/t/scalable-gossip-for-state-network/8958\n\nI wasn't inclined to put too much polish on it at this stage because it's a really rough idea.  happy to answer questions.  this approach feels promising.",
        "created_at": "2021-03-18T18:59:53.695000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I'm most excited about the piece at the end.  Providers push the proofs for the leaf data to the nodes that care about them.  Each of those nodes then peels the leaf data off and splits the proof into proofs for the layer of intermediate trie data that sits above the leaves.  They then seek out the nodes that are interested in that data and push the proofs to them.  Those nodes repeat this process until the only thing left is the root node (the state root).\n\n- This reduces the load placed on proof providers, only requiring them to push leaf proofs.\n- It spreads the responsibility for pushing the intermediate trie data out broadly across the network, and to parties that should have an intrinsic interest in making sure that data becomes available.\n- It has a natural \"redundancy\" mechanism built in as you move up the trie towards the state root, effectively ensuring that the most accessed data (the intermediate stuff closest to the root) is the mostly thoroughly distributed.\n\nAnd biggest of all, the network stays incredibly simple.  No relaying or routing of messages.  No complex topology to enforce.  DOS/Amplification mitigation looks unchanged and simple.",
        "created_at": "2021-03-18T19:10:33.463000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "This is all built on the presumption of uTP as a streaming transport layer on top of DiscV5 which allows any node to establish a stream with any other node very cheaply.",
        "created_at": "2021-03-18T19:11:57.547000+00:00",
        "attachments": null
    },
    {
        "author": "levi.korg",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "This is really fascinatingâ€”if I am reading it all correctly. I like the elegance of proving what you care about and distributing the work fairly",
        "created_at": "2021-03-18T19:23:47.346000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "There's also a \"thundering herd\" problem that probably needs to be solved.  In theory, the node responsible for the state root is going to get a *lot* of nodes pushing that data towards them.  Assuming we have a replication factor of 10x and are still on the hexary trie, with up to 16 nodes at a given level of the trie, that means we could have something like 160 different nodes trying to push the parent proofs, and I think this applies to every level of the trie, not just the state root.  The redundancy is good, but we might need to find a way \"reduce/limit\" this.",
        "created_at": "2021-03-18T19:26:23.091000+00:00",
        "attachments": null
    },
    {
        "author": "ryanleeschneider",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "This got me thinking: is there some way the provider can embed a \"coinbase\" in their proof, which would then get some sort of reward for new txs submitted using that proof?  nodes with full state could produce their own proofs and thus not need to pay this reward.  And I suppose since proofs are layered the reward could be shared between the creators of the leaf, intermediate and root proofs.",
        "created_at": "2021-03-18T20:13:05.005000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We are staying away from any form of incentives right now.  It's complex, and provides perverse incentives for people to \"game\" the system to maximize how much value they extract from the system.",
        "created_at": "2021-03-18T20:14:00.732000+00:00",
        "attachments": null
    },
    {
        "author": "ryanleeschneider",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "makes sense, thanks",
        "created_at": "2021-03-18T20:14:11.966000+00:00",
        "attachments": null
    },
    {
        "author": "ryanleeschneider",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e Given that we know individual nodes are expected to only care about 0.002% of the data\nCan you explain where this number comes from?",
        "created_at": "2021-03-18T20:15:52.620000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Looking forward beyond the hexary trie, protocol research is pointing heavily towards a unified Verkle trie.  This means that we'll no longer have the problem of dealing with imbalanced contract storage, and that proof overhead is going to be tiny.\n\nUnder this model, I think we will need to evolve the network away from `GetNodeData` style storage, and towards people storing contiguous chunks of the trie (the parts closest to their node_id).  New data entering the network would be in a format that would allow people to update their data in-place (and maybe retain reverse diffs for recent state roots).  Overall storage requirements in this type of network drop significantly, but it comes at the cost of nodes having to shoulder the load of updating their proofs in-place (or distributing entirely new proofs at each block).",
        "created_at": "2021-03-18T20:17:59.985000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yes, ~200GB of trie data, 10x replication, 100mb of storage on average provided by each network participant -\u003e 20,000 nodes needed to store all of the state data.  The total number of nodes and replication factor don't actually matter but they are good contextual information.  100MB / 200GB -\u003e `0.0004` \u003e 0.04%  \n\ni'm not sure where `0.002%` came from (i was whiteboarding yesterday...) but it's in roughly the same magnitude as `0.04` and the conclusion still holds that the nodes in the network  will only care about a very small percentage of the data.",
        "created_at": "2021-03-18T20:21:18.040000+00:00",
        "attachments": null
    }
]