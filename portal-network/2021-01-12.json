[
    {
        "author": "__lithp__",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Taking out a reservation: I'll be spending this week working on (2). TurboGeth has a mechanism for running analytics over the chain, I'll be trying to get something working which goes from block # -\u003e list of touched accounts and storage locations. From there we can easily figure out things like how cold each access is and how quickly the network can forget things without losing track of important data",
        "created_at": "2021-01-12T03:07:46.931000+00:00",
        "attachments": []
    },
    {
        "author": "guilhermesalgado9479",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e I'll be trying to get something working which goes from block # -\u003e list of touched accounts and storage locations\n\u003c@!296630428754771968\u003e would that also allow us to answer to (3)?",
        "created_at": "2021-01-12T09:06:07.224000+00:00",
        "attachments": []
    },
    {
        "author": "__lithp__",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yes, good point, seems like it's most of the way there!",
        "created_at": "2021-01-12T09:14:58.928000+00:00",
        "attachments": []
    },
    {
        "author": "guilhermesalgado9479",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I was going to work on that, but probably no point then",
        "created_at": "2021-01-12T11:35:38.224000+00:00",
        "attachments": []
    },
    {
        "author": "nick.ghita",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e TurboGeth has a mechanism for running analytics over the chain\n\u003c@!296630428754771968\u003e what mechanism are you referring to here? like a custom script/lmdb to iterate through the \"change sets\" table? or is there a native tool in turbo-geth that i'm missing?",
        "created_at": "2021-01-12T15:20:59.629000+00:00",
        "attachments": []
    },
    {
        "author": "guilhermesalgado9479",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!364458974906548225\u003e since \u003c@!296630428754771968\u003e's work is likely to answer (2) and (3), I set out to investigate (1) and think about how I'd approach it, but I'm a bit lost so I'd appreciate if you have any suggestions",
        "created_at": "2021-01-12T16:14:24.500000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I know I should know, but where are we getting these numbers from?",
        "created_at": "2021-01-12T16:26:16.020000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "lol, never mind, I scrolled up",
        "created_at": "2021-01-12T16:26:26.918000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I think we can do a simpler version of what Jason did for beam sync.  \n\nTake a look at the PausingVM thingy here: https://github.com/ethereum/trinity/blob/65609f8fda7d880c0efe859ea84a7d0935c02edb/trinity/sync/beam/importer.py",
        "created_at": "2021-01-12T16:29:00.033000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We'd want to have some set of representative mainnet transactions that we use as benchmarks.  simple value transfers, ERC20 transfers, more complex uniswap stuff....   things like that",
        "created_at": "2021-01-12T16:29:56.136000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We'd want to be able to run the transactions once to collect all the state needed, and then run them again injecting artificial latency each time we go get some state.",
        "created_at": "2021-01-12T16:30:45.311000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We can probably drop the `async` complexity entirely and just do blocking calls",
        "created_at": "2021-01-12T16:32:46.580000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!482518216019542016\u003e ^  let me know where you've got questions",
        "created_at": "2021-01-12T16:33:07.884000+00:00",
        "attachments": []
    },
    {
        "author": "nick.ghita",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "it seems to me like brian's work might also provide some (rough) insight into #1? if all we're looking for is a very general relationship between # of db tries hit / unit of gas?",
        "created_at": "2021-01-12T16:35:19.587000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "There is overlap, but I think there is still value in seeing if our numbers line up when we come at things from different directions.",
        "created_at": "2021-01-12T16:37:25.877000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Also, if you're using Py-EVM, lets make sure we get good timing numbers on the execution parts, specifically mega-gas-per-second, so that we can adjust our numbers to what would be expected on a faster EVM implementation.",
        "created_at": "2021-01-12T16:38:42.368000+00:00",
        "attachments": []
    },
    {
        "author": "guilhermesalgado9479",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "ok, I was thinking you wanted to analyse a more extensive dataset, which, AIUI, would require modifying the evm of a fully synced geth node. I should be able to get started, but I'm sure I'll have more questions soon",
        "created_at": "2021-01-12T16:38:42.728000+00:00",
        "attachments": []
    },
    {
        "author": "nick.ghita",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "i've been exploring nethermind which seems to have some tidy built in metrics - but i'm still trying to figure out if it's possible to adjust the metrics collection for a /tx execution - https://docs.nethermind.io/nethermind/ethereum-client/metrics/modules/store",
        "created_at": "2021-01-12T16:39:49.095000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!482518216019542016\u003e yes, a decently extensive dataset of representative mainnet transactions is what I'm wanting.  I just listed a few examples.",
        "created_at": "2021-01-12T16:41:54.069000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Another thing with the latency numbers.  It's looking like it may be possible to model the Account trie using a more efficient `(state_root, trie_path) -\u003e trie_node` structure which would allow us `O(1)` lookups on accounts.  Contract storage would still be `node_hash -\u003e node`.  It would be nice if we could see both numbers:\n\n- How many network round trips if everything is `node_hash -\u003e node`\n- How many round trips if account lookups are `O(1)` and contract storage is `node_hash -\u003e node`\n\nAlso, we should assume that trie nodes are cached during speculative execution.  First account lookup may end up requesting the state root node, next lookup shouldn't have to re-fetch that node.",
        "created_at": "2021-01-12T16:44:51.057000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!453997683431178243\u003e would it be accurate to say that the account trie can be done using the more efficient mechanism and that we could model accounts efficiently and contract storage the other way?",
        "created_at": "2021-01-12T16:46:18.878000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "(based on your previous work)",
        "created_at": "2021-01-12T16:46:25.936000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "If I understand your question correctly, I think both should be modeled the same way. The difference between contract storage and account storage is that contract storage has one more level of indirection, because you need to first lookup the `(state_root, trie_path) -\u003e trie_node (ie account?) -\u003e storage_root` and from there do the mapping to the specific storage keys/addrs using `(storage_root, trie_path) -\u003e trie_node`. So I guess the question is why use two different methods to fetch relatively similar data?",
        "created_at": "2021-01-12T17:52:46.469000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Why use two methdos:\n\nThe account storage is balanced so we can reliably distribute the data evenly around the network using the trie path.\n\nThe contract storage is imbalanced and there's no known mechanism that I'm aware of that lets us distribute the data evenly around the network if we do it based on `(global_state_root, account_state_root, trie_path)`.  But if we do contract storage the \"naive\" way of `node_hash -\u003e node` the data can easily be distributed evenly around the network.",
        "created_at": "2021-01-12T18:14:54.965000+00:00",
        "attachments": []
    }
]