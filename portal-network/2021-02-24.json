[
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Zooming out the desired gossip network properties (off the top of my head) are:\n\n- New state data is quickly gossiped to the nodes interested in storing it, in a format that is easily verified by DHT nodes (aka a witness)\n- Missing state data (that might be old/cold) can be injected into the network\n- Network mitigates DOS via flooding the network with large quantities of random (but valid) state.\n- Network mitigates any form of amplification of invalid data.",
        "created_at": "2021-02-24T00:09:59.500000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "There's also stuff like:\n\n- New nodes joining the network can \"quickly\" discover data within their radius (even really cold data?)\n- The more popular a piece of data is, the more likely it will be widely replicated.",
        "created_at": "2021-02-24T00:12:55.937000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "How would you discover a node with a huge radius, without the announcement indirection?",
        "created_at": "2021-02-24T01:20:58.737000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I think that there is a point where expanding your radius beyond a certain amount won't help, so there will be some kind of \"emergent\" radius (maybe something like 90th percentile) that it makes no sense to extend your radius beyond that because nobody is likely to request data from you.",
        "created_at": "2021-02-24T01:39:43.068000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "So the intend is not to allow for full radius nodes at this point, more that it seems nice to let this number be dynamic.",
        "created_at": "2021-02-24T01:40:37.350000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Ok, I think I have a workable starting point for gossip.  It's basically identical to the transaction gossip network structure.\n\n- The network is TCP based with long lived connections (libp2p or devp2p)\n- Nodes maintain a `radius` which is announced upon connecting to a new node **and** can be updated during the course of that connection.\n- Nodes manage their peer pool so that:\n    - They first prioritize having peer connections that fully *cover* the part of the keyspace that their radius covers.\n    - They second prioritize having peer connections that cover the remainder of the keyspace.\n    - They *prefer* connections with nodes that have a similar radius to themselves.\n- We use the 3-message pattern of `AdvertiseProofs/GetProofs/Proofs` that is used in the `ETH` gossip protocols\n    - Minimal advertisement that data is available via `AdvertiseProofs`\n    - Request/response messages to retrieve data advertised.\n- Nodes should only advertise data that was interesting to them (If someone advertises X and X wasn't interesting to me, I should not advertise X to my peers).\n\nThis seems to fully cover both our new data ingress needs and patching up data that is found to be missing.\n\nCombined with the POKE mechanics that Sam suggested above, I think this gives us a functional starting point.",
        "created_at": "2021-02-24T17:00:39.438000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "This seems to cover the DOS and amplification vectors since gossip messages dead end on any node that isn't interested in the data.",
        "created_at": "2021-02-24T17:01:43.797000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "A client doesn't prefer partial radius overlap to \"similar size\"?",
        "created_at": "2021-02-24T17:02:02.589000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "The exact semantics of how they balance radius overlap with similarity of size is a little fuzzy at this point.  We want to avoid peer pool churn, but also want to occasionally drop connections with \"worse\" peers for \"better\" peers.",
        "created_at": "2021-02-24T17:05:32.371000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\"better\" might be definable as equal or greater radius overlap and closer match to local radius, but implementations would need to be careful for griefing since a node could advertise a specific radius just to try and cause pool churn by getting you to drop a connection (which inturn is a vector for eclipse attacks)",
        "created_at": "2021-02-24T17:09:23.912000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I would love to see an artistic rendition of the radius concept and the peer choice mechanism.",
        "created_at": "2021-02-24T17:10:39.142000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Out of curiosity, how many dimensions is the \"radius\" over?",
        "created_at": "2021-02-24T17:11:24.333000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "1d, 2d, 3d, etc.?",
        "created_at": "2021-02-24T17:11:30.182000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Is it just a section of a hash (1d, a line)?",
        "created_at": "2021-02-24T17:11:42.618000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Like I handle everything where `hash \u0026 0x000000F000000 != 0`?",
        "created_at": "2021-02-24T17:12:18.682000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "And the more connections a malicious actor has to you via some set of sybil peers, the more knowledge they have about what other connections you would be interested in, so there's a real problem that needs mitigation here.",
        "created_at": "2021-02-24T17:12:43.642000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I think 1-dimension with the line wrapping around on each end.",
        "created_at": "2021-02-24T17:13:33.225000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Given the first priority rule, is there a risk that with a sufficient number of nodes you could end up with partitioning because a bunch of nodes just all peer with each other?",
        "created_at": "2021-02-24T17:14:12.657000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "For example, if each node has 256 peers, and there are 256 nodes for a given slice, they would all end up peered with each other and no one else.",
        "created_at": "2021-02-24T17:14:33.790000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I think the peer preference system will need buckets to prevent that, so you peer with some amount of each category.",
        "created_at": "2021-02-24T17:14:49.915000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I don't think so, because your peers would extend out in both directions on the line, and those peers would have a pool that also extends out in both directions, and so on, so even if all the peers are low radius, they should still form a large interconnected chain of peers (and they should also have some set of peers that cover other parts of the keyspace outside their radius.",
        "created_at": "2021-02-24T17:15:47.066000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Does the system have a mechanism for ensuring that peer radius doesn't end up in this state?",
        "created_at": "2021-02-24T17:16:42.005000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Imagine a network with 500 peers on it, but the system was only divided into 4 slices, where the slices were like 0-2, 1-3, 2-0, 3-1.  You would end up with 125 peers on each slice.  If the max-peers for each node was less than 125, they would partition.",
        "created_at": "2021-02-24T17:18:04.526000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "By dividing up into 8 slices you would partition with less than 62 peers per node, 16 slices with less than 31 peers per node, etc.",
        "created_at": "2021-02-24T17:18:53.764000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "So we definitely *can* ensure what you want, but I think it requires knowing how many nodes are in the network and adjusting the slice count accordingly.",
        "created_at": "2021-02-24T17:19:21.904000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Or do what \u003c@689161464829050960\u003e suggested, which would solve the problem.",
        "created_at": "2021-02-24T17:19:43.131000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I've been wondering about the scenario where:\n\n - Each node has 99 max peers.\n - Each node has a radius of exactly `R`, where `R \u003c\u003c\u003c 2^256`\n - 100 peers exist initially. Their node ids are all \"close\" to some middle point `P`.\n - 100 peers are added later. These node ids are \"far\" from `P`.\n\nWould the initial group of peers ever connect to the second group of peers?",
        "created_at": "2021-02-24T17:25:29.027000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Well, node ids are random, so the premise is already un-natural since it wouldn't occur without all parties on the network being \"weird\"",
        "created_at": "2021-02-24T17:36:19.172000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "So for any reasonable scenario, I think there needs to be some  non-trivial portion of the network that you assume has randomly distributed identifiers.\n\nAnd then there's going to be variability in how long nodes stay on the network.  Some will be always on, many will only be present for some limited amount of time... this naturally creates churn in the available slots each peer has, which should move the *honest* nodes towards the natural equilibrium of being widely interconnected.",
        "created_at": "2021-02-24T17:38:07.327000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Right, but once you have a large enough pool of nodes, you'll end up with pockets of closeness. You could get a segmentation if an ISP goes down for some time, then comes back up.",
        "created_at": "2021-02-24T17:38:20.161000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "How does slice size get determined?  Too small and the network may be missing slices.  Too big and nodes have to store too much.  This changes as number of nodes grow/shrink.",
        "created_at": "2021-02-24T17:38:54.811000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "silce == radius? If so, each node chooses their own radius based on bandwidth/storage available",
        "created_at": "2021-02-24T17:39:32.478000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "What happens if you have a bunch of light nodes with very small radiuses and not enough to cover the whole space?",
        "created_at": "2021-02-24T17:40:02.557000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I guess just accept that the system doesn't work (not enough altruism)?  üò¨",
        "created_at": "2021-02-24T17:40:20.782000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah, exactly.",
        "created_at": "2021-02-24T17:49:02.481000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Hm, would you want to see:\n\n - nodes expand their radii to cover the full space, and evict data aggressively; or\n - nodes don't cover the full state, but evict less aggressively?",
        "created_at": "2021-02-24T17:50:37.283000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I tried generating a graph but it was way too noisy to get any real sense.  I think I know what I need to do to modify my graph generation to make it reasonable.  Gonna try that now",
        "created_at": "2021-02-24T17:58:12.342000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah, there are degenerate cases like this where total data needs exceeds total network capacity and in those situations we would expect significant service degradation",
        "created_at": "2021-02-24T18:00:10.701000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Have you tried `circo` or `twopi`?\n\nhttps://mikemol.github.io/graphviz/dot/2018/02/27/graphviz-dot-basics-pt-2.html",
        "created_at": "2021-02-24T18:24:22.083000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah, I'm playing around with those two.  I'm just finding it complicated to get it to render something that is in any way reasonable.  Seems I either end up with something so massive and cluttered that it just looks like a massive tangled mess, or if I eliminate most of the edges and try to just draw the viewpoint from a few sample nodes, the overall topology no longer gets adequately visualized.",
        "created_at": "2021-02-24T18:36:58.971000+00:00",
        "attachments": null
    },
    {
        "author": "__lithp__",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!364458974906548225\u003e on the call yesterday you talked about potentially making changes to TG/Nethermind to allow them to support bridge nodes. Just spent a little time brainstorming a JSON-RPC interface bridge nodes could use: https://notes.ethereum.org/@lithp/r1NB6M4Gu\n\nLeaving it here if you have any comments / if there's something about how bridge nodes might work that I'm misunderstanding",
        "created_at": "2021-02-24T20:37:26.752000+00:00",
        "attachments": null
    },
    {
        "author": "__lithp__",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "hmm, just published, should work now",
        "created_at": "2021-02-24T20:39:22.813000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "working now",
        "created_at": "2021-02-24T20:39:28.622000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Cursory glance looks like it covers needs, worth trying to actually implement those and see how hard/easy/impossible they are.",
        "created_at": "2021-02-24T20:43:09.056000+00:00",
        "attachments": null
    },
    {
        "author": "__lithp__",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "It seems like `getBlockChanges` and `getItemWitness` are the ones on the critical path, and they should also be the easiest to implement.",
        "created_at": "2021-02-24T20:44:17.166000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Regarding the gossip conversation above.\n\nI realize that radius-based state network gossip has one fundamental difference from the same design as applied to transaction gossip.  In the transaction network, there are nodes that exist at the full radius who are actively interested in seeing everything....  In the state network, the only reason a node would have to be full radius is if they are a benevolent bridge node.  Also, we should expect nodes in the state network to have roughly 1000x smaller radius than they might have in the transaction network because there is roughly 1000x more data in the state network....\n\nSo there's more work to be done to figure out how significantly this will effect gossip because the network might enter a degenerate state if nearly **all** nodes have very small radius values.",
        "created_at": "2021-02-24T20:47:23.945000+00:00",
        "attachments": null
    },
    {
        "author": "jason.carver",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!296630428754771968\u003e to fit with Berlin's 2930, maybe `getBlockChanges()`  could return `[(addr, [slot, ...]), ...]`. See https://eips.ethereum.org/EIPS/eip-2930#definitions",
        "created_at": "2021-02-24T21:00:30.129000+00:00",
        "attachments": null
    },
    {
        "author": "jason.carver",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Not exactly that the two things interact, they are just packaging the same type of data. Identical structure would be nice, rather than having to remember which is which.",
        "created_at": "2021-02-24T21:01:46.285000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Also, I distilled some of the high level \"network needs\" into this document earlier today: https://notes.ethereum.org/Fhh0MKOCRgyDdNcw6a9h6w",
        "created_at": "2021-02-24T21:02:19.191000+00:00",
        "attachments": null
    },
    {
        "author": "__lithp__",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Nice catch, unifying formats makes sense to me üëç",
        "created_at": "2021-02-24T21:03:43.917000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!689161464829050960\u003e I just realized a the POKE approach has a problem.  In my model of the network, nodes receive proofs from gossip, validate them against a state root, pluck out the nodes they care about, and discard the rest of the proof.\n\nSo nodes can't naively give other nodes their data because they won't have an inclusion proof to provide alongside the data...\n\nIn the POKE scenario, it is probably reasonable to assume the client is actually building a proof since they will be walking down into the trie from the state root, meaning that by the time they request a specific piece of state data, they should have accumulated a full proof in their walk.  So a node looking up data, that wants to POKE it back along the search path could be expected to have accumulated a proof for it **BUT** we also know that proofs beyond the 2nd layer of the trie will exceed the UDP packet size since each full node in the trie is 16*32 bytes-\u003e 512 bytes.  So the POKE approach only works if we allow the transmission of the proof to span multiple packets... which is an ugly griefing vectors that aren't trivial to mitigate.... üôÅ",
        "created_at": "2021-02-24T21:22:46.807000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Is the griefing attack sending a proof that is invalid only at the last possible point?",
        "created_at": "2021-02-24T21:25:27.646000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "say it takes 10 packets, only 9 show up,  it's a non-attributable fault....  or the 10th packet shows up and only then can you discover that the data was invalid...",
        "created_at": "2021-02-24T21:27:27.182000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "And that's a problem because POKE connections would be short-lived, unlike gossip connections?",
        "created_at": "2021-02-24T21:28:00.731000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "or they take advantage of an implementation that allows for 10s timeouts for each packet, sends them all 9.5 seconds apart, and does this from 200x malicious nodes under their control, creating a massive number of concurrent processes all waiting on data.... and crashes then node...?",
        "created_at": "2021-02-24T21:28:34.582000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah, POKE would have to go over the DHT via UDP packets",
        "created_at": "2021-02-24T21:28:47.855000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I guess I'm trying to wrap my head around why this isn't a problem for the gossip network, but is for pokes.\n\nThe key differences are (and correct me if I'm wrong):\n\n - POKE would either be a blast of UDP or a short-lived TCP connection, while the gossip network would be long lived connections\n - gossip has a bounded set of peers, where poke needs to accept connections from potentially anyone",
        "created_at": "2021-02-24T21:39:38.747000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I would say that a trickling peer is a problem for gossip much more than it is for POKE",
        "created_at": "2021-02-24T21:40:04.857000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "You can set an overall timeout for POKE (i.e. the entire proof has to arrive within 30s) but you can't do the same for gossip.",
        "created_at": "2021-02-24T21:41:34.253000+00:00",
        "attachments": null
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "You can drop incoming POKE requests (the same as gossip peers) if you have too many POKE requests active.",
        "created_at": "2021-02-24T21:42:36.168000+00:00",
        "attachments": null
    }
]