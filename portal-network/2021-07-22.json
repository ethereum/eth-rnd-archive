[
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Don't worry about clogging things up, that's what this channel is here for",
        "created_at": "2021-07-22T07:22:56.038000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Talked with Barry whitehat yesterday and it sounds like they might have a viable solution for both DOS protection and peer ranking that could be layered onto our planned DHT networks. Status may have already implemented the DOS protection/mitigation in their main client. It's a ZK based gadget that can be used to establish a whitelist of allowed senders (such as using ZK to prove that you control the private key to an EOA with a non zero balance). From there it uses ssss (shamir's secret sharing scheme) and more math magic such that anyone who exceeds the sending rate limits will reveal themselves, allowing them to be added to a sort of ephemeral shared global blacklist.\n\nProbably not did that's in any way immediately pressing but nice to know it exists.",
        "created_at": "2021-07-22T08:18:22.864000+00:00",
        "attachments": null
    },
    {
        "author": "acolytec3",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "https://vac.dev/rln-relay Here's the write-up on their concept",
        "created_at": "2021-07-22T09:20:47.318000+00:00",
        "attachments": null
    },
    {
        "author": "acolytec3",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "And here's a good audio discussion of it and really interesting history on whisper if you like that version. https://www.zeroknowledge.fm/186 The one question I have on this particular approach is I think Status relies on the idea of using a smart contract to register every participant in the network (at least I think Oskar describes that in the podcast) and I'm assuming that won't work for our use case so wondering if there's another way to enforce network membership in the way that spammers would still dox themselves into the blacklist?",
        "created_at": "2021-07-22T09:25:10.904000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Also, I think I figured out a viable storage layout that let's us move from the GetNodeData approach with O(log(n)) access cost to nodes storing contiguous sections of the state trie with O(1) access costs for our state network that is viable under the current merkle trie layout. Will work on a write asap.",
        "created_at": "2021-07-22T12:36:45.267000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Ok, I'm not sure whether this is at all clear, but here is my proposal: https://notes.ethereum.org/h58LZcqqRRuarxx4etOnGQ",
        "created_at": "2021-07-22T13:39:41.876000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!453997683431178243\u003e I'd be curious to get your ðŸ‘€ on this since it is a problem you spent time on.",
        "created_at": "2021-07-22T14:15:06.225000+00:00",
        "attachments": null
    },
    {
        "author": "deme1744",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "So this is about moving away from storing \u0026 providing every trie node over the network and instead just provide the leave nodes and their proofs? I remember a post where it was described on how this solution was more complex because of proofs needing to be updated continuously? Although, honestly I didn't really understand how that was so different with having to walk the trie again for updates also. Any insight here, why the change? (I mean, don't get me wrong, I think it makes sense, for example just for the O(1) network lookups, just curious)",
        "created_at": "2021-07-22T14:15:23.127000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "The change is that we could never figure out how to assign the data in such a way that it was viable due to the imbalanced nature of the contract storage tries.  The *constant* updating of these proofs is something we'll have to look into whether it will be a problem.",
        "created_at": "2021-07-22T14:17:25.744000+00:00",
        "attachments": null
    },
    {
        "author": "deme1744",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Right, I forgot that there was that issue also",
        "created_at": "2021-07-22T14:17:48.427000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I'm still really skeptical of my proposed solution because I know that there are multiple smart people who have tried to tackle this and nobody figured it out with a **lot** of thinking and problem solving...  but I think what I'm proposing has all of the properties we've been trying to get....",
        "created_at": "2021-07-22T14:19:59.768000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "That property being that all of the \"leaf\" data is randomly and evenly mapped across the 32 byte keyspace without needing to know anything about how the data is actually distributed as a whole.",
        "created_at": "2021-07-22T14:21:10.970000+00:00",
        "attachments": null
    },
    {
        "author": "jlokier",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Randomly mapping expands the amount of storage you need on proofs.  The way the storage is mapped looks to me like almost every storage slot will have to be accompanied by a proof the size of its path in the storage trie, because small accounts' storage slots are spread out among different nodes. This is because proof size reduces when related values in the same Merkle subtrees are kept together.",
        "created_at": "2021-07-22T16:00:08.319000+00:00",
        "attachments": null
    },
    {
        "author": "jlokier",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "The problem of imbalanced storage sizes is quite a problem though as you rightly pointed out though. and I suppose the proof sizes are small for small accounts, so perhaps it balances out.",
        "created_at": "2021-07-22T16:04:31.347000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I think that being able to reason about this probably requires doing the work to either properly cumpute/model what this ends up looking like in practice or to actually enumerate the state and populate the storage of some nodes and see what it looks like and how much overhead we are seeing in terms of proof storage vs leaf storage. In general I think your assessment is accurate, that a lot of storage slots will exist in isolation with their accompanying proofs.",
        "created_at": "2021-07-22T17:32:40.279000+00:00",
        "attachments": null
    }
]