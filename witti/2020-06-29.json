[
    {
        "author": "butta",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "https://witti.beaconcha.in/block/10880#pills-votes\n\n\u003c@573787335796326400\u003e those are teku validators right? multiple votes from the same validator",
        "created_at": "2020-06-29T07:23:26.724000+00:00",
        "attachments": null
    },
    {
        "author": "butta",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "found by \u003c@!386151184869752834\u003e ðŸ‘€",
        "created_at": "2020-06-29T07:23:43.251000+00:00",
        "attachments": null
    },
    {
        "author": "benjaminion",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!97774605351010304\u003e What's the issue? That we were including so many attestations (duplicates, and inefficient, but not actually against the protocol)? If so, I think this was fixed in https://github.com/PegaSysEng/teku/pull/2000",
        "created_at": "2020-06-29T07:34:03.557000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!573787335796326400\u003e speaking of that - do you do anything to try to optimize the attestations you add in a block? or just whatever unique that shows up on the attestation channel?",
        "created_at": "2020-06-29T07:39:33.878000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Note that Teku will still include aggregates we have if they include votes from validators that we haven't yet seen in a block. So there may be duplicate votes in that aggregate but some validators we believe are new votes.",
        "created_at": "2020-06-29T07:39:53.038000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Teku basically takes all the attestations it's been told about, filters down to the ones that are valid in the block we're trying to create then starts with the attestation that already has the most validators embedded in it and tries to aggregate as many other attestations into that one (if there's overlap of validators you can't aggregate them).  Then we move on to the unaggregated attestation with the next most validators and try to aggregate with that.  Repeat until we either fill the block with attestations or run out of valid attestations to include.",
        "created_at": "2020-06-29T07:41:40.490000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "But back in those early slots of Witti we were failing to remove attestations we'd already seen included in blocks so we mostly just filled the blocks. ðŸ™‚",
        "created_at": "2020-06-29T07:42:37.836000+00:00",
        "attachments": null
    },
    {
        "author": "alex.g.t",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "using teku to fetch those blocks/attestations yielded no duplicates that i can see",
        "created_at": "2020-06-29T07:43:50.868000+00:00",
        "attachments": null
    },
    {
        "author": "alex.g.t",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "but i guess \u003c@!97774605351010304\u003e is fetching them another way via prysm",
        "created_at": "2020-06-29T07:44:23.529000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "right - yeah, the computatinally hard problem I guess is combining attestations optimally so you can fit as many as possible - but then again, it's not necessarily a \"real\" problem since we can fit quite many in a block",
        "created_at": "2020-06-29T07:53:44.935000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "thanks \u003c@!340345049063882753\u003e",
        "created_at": "2020-06-29T07:53:56.076000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!449019668296892420\u003e Yeah it's an interesting problem - I'm pretty sure the \"start with the biggest and shove it all in\" strategy was taken from Paul's description of what Lighthouse did at DevCon.",
        "created_at": "2020-06-29T08:08:17.743000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "yeah we do some maximum coverage stuff that gets kinda complicated",
        "created_at": "2020-06-29T08:08:47.603000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "i'm not sure we've ever actually seen it doing its magic though, only in testing",
        "created_at": "2020-06-29T08:09:04.605000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "It makes sense to me as a very cheap heuristic that should work out good enough.  Since we throw any individual attestations we happen to pick up from subnets in there as well if you're running enough validators to be subscribed to all subnets you will wind up with a single fully aggregated attestation per committee per block.  But you may wind up doing more work aggregating individual attestations than you absolutely had to.",
        "created_at": "2020-06-29T08:09:09.376000+00:00",
        "attachments": null
    },
    {
        "author": "benjaminion",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!449019668296892420\u003e As it happens, I assigned myself an issue on this last week https://github.com/PegaSysEng/teku/issues/2192. It's not yet our most pressing challenge, though ðŸ˜…",
        "created_at": "2020-06-29T08:17:36.099000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "lol, I know that feeling ðŸ™‚",
        "created_at": "2020-06-29T08:18:03.617000+00:00",
        "attachments": null
    }
]