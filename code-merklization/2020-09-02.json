[
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "thanks Alex, if I find anything else I will comment in the FEM thread",
        "created_at": "2020-09-02T05:14:04.726000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We haven't cross-posted it here, but this the EIP: https://eips.ethereum.org/EIPS/eip-2926\n\nStill need to add the Python code drafted up by \u003c@!273808422753796097\u003e in the \u003c#688075293562503241\u003e chat.",
        "created_at": "2020-09-02T16:25:05.047000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "And another concern raised was if the cost increase for creation is not significant, then some simpler gas cost rule would be beneficial (as opposed to the rather complex looking one).",
        "created_at": "2020-09-02T16:26:18.385000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Cool to see the EIP, just gave it a read-through.  At the risk of bike shedding ü§¢   I'd like to float the idea of removing the RLP from the spec.  It looks like the only places that RLP is used are:\n\n- `RLP([METADATA_VERSION, codeHash, codeLength])`\n- `RLP([firstInstructionOffset, C.code])`\n\nLEB128 looks suitable for `METADATA_VERSION, codeLength, and firstInstructionOffset`.  `codeHash` can just be fixed length bytes.  `C.code` can also just be the raw bytes, allowing us to just serialize these using concatenation:\n\n- `LEB128(METADATA_VERSION) || codeHash || LEB128(codeLength)`\n- `LEB128(firstInstructionOffset) || C.code`",
        "created_at": "2020-09-02T18:46:13.969000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "If there is negative sentiment towards LEB128, `codeLength` could be a fixed 4-byte big endian, and `METADATA_VERSION \u0026 firstInstructionOffset` could just be encoded as single bytes.",
        "created_at": "2020-09-02T18:47:42.172000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "My motivation is to chip away at the use of RLP in the protocol whenever possible.",
        "created_at": "2020-09-02T18:50:00.343000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "(cross posted this in the linked forum discussion for better visibility)",
        "created_at": "2020-09-02T19:20:53.802000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I don't think it is bikeshedding -- at this point in time at least üòâ\n\nWe had mentioned this in the rationale under the RLP section. The proposal is definitely not adamant on RLP, the trie format, and where the version is. In my opinion these are discussion points ‚Äî also please check other stuff in the rationale, all those can be discussed.\n\nAt the time of writing (last week) we were on the side that lets make this 100% following whatever the ‚Äústandard‚Äù is within the state, and as we think this should go in after or at the same time as binarification, it should just follow the rules (both for merkleization and data encoding) specified by binarification.\n\nHowever based on some feedback, I would happy to consider making this self contained and defining both its own kind of merkleization and encoding rules. Perhaps the ‚Äúindex-based alternative‚Äù mentioned in the rationale would also warrant this approach.",
        "created_at": "2020-09-02T20:58:23.563000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "RLP is an anti-feature for anything related to hashing and constructing merkle trees (anti-feature meaning it does not add any real value, but encumbers). It is only useful for creating packets sent over the network, so they can be un-packaged. When we hash, we never expect anything to be unpackaged, because hashing is supposed to be irreversible. So there is no need at all to use RLP with hashing",
        "created_at": "2020-09-02T21:01:00.604000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Right, I'm personally not a fan of RLP. The motivation was as mentioned above to just follow whatever is in the system to be consistent.",
        "created_at": "2020-09-02T21:02:37.725000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "The metadata however, if it contains the version, would need to be serialised in a known way. Probably keeping the first byte as version is good enough.",
        "created_at": "2020-09-02T21:03:10.081000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "the only requirement to the serialisation format for hashing is that it avoid collision when polymorphic types/collections are used. If in this use case, the types are well defined, you can simply use concatenation",
        "created_at": "2020-09-02T21:03:12.096000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "To clarify: it does not matter too much for the hash, but for the witness format. Which we do not discuss here anyway.",
        "created_at": "2020-09-02T21:04:33.281000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "So if you define a fixed-sized serialisation for each basic type, and the tuple has pre-defined number of elements without possiblity of omission, then concatenation should work?",
        "created_at": "2020-09-02T21:04:38.024000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We actually had a simple concatenated version prior to the RLP üò¨",
        "created_at": "2020-09-02T21:06:03.547000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I am also not sure about the whole methodology of building Patricia trees where key == chunk_number, and the value is the chunk. I know this is how transactions are hashed in the blocks, but it is quite weird üôÇ",
        "created_at": "2020-09-02T21:06:07.180000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e I am also not sure about the whole methodology of building Patricia trees where key == chunk_number, and the value is the chunk. I know this is how transactions are hashed in the blocks, but it is quite weird üôÇ\n\u003c@456226577798135808\u003e Check the \"Index-based alternative for the keys in the code trie\" in the rationale. We're going to get that version out next week when \u003c@!259648573401071617\u003e is back. Will incorporate all this feedback into it.",
        "created_at": "2020-09-02T21:07:26.073000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Oh, I see",
        "created_at": "2020-09-02T21:07:51.211000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "thank you, I was still half way through yesterday üôÇ",
        "created_at": "2020-09-02T21:09:30.874000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Perhaps we shouldn't have been eager to release this version (which just describes the logic used in the earlier experiments), because we realised a lot of possible simplification, but decided to include all that under \"rationale\". Sorry for asking y'all to read all this üôÇ",
        "created_at": "2020-09-02T21:10:48.791000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "No, it is great that you released it!",
        "created_at": "2020-09-02T21:11:16.910000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "you should not apologise, it is great work",
        "created_at": "2020-09-02T21:11:39.009000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "My main concern is however the motivation. It seems that benefits are only present if we need witnesses, given code deduplication can not be achieved with this. Well definitely not with the metadata field, but even without that chances are slim for having the ability to deduplicate using chunks. And for the matter I think chances are also slim with more complex subroutine/basicblock based approaches, given the way Solidity code generation works.",
        "created_at": "2020-09-02T21:14:29.356000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yes, the motivation is related to witnesses. I think code deduplication is mostly an \"implementation detail\" üôÇ",
        "created_at": "2020-09-02T21:22:36.475000+00:00",
        "attachments": null
    },
    {
        "author": "axic3354",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Code deduplication is an implementation detail, unless we want to expose the benefits to users by charging based on new chunks introduced.",
        "created_at": "2020-09-02T21:31:11.615000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "as I mentioned before, I am not of the opinion that all possible optimisations need to be passed on to the users in the form of reduced gas cost üôÇ that is a self-strangling strategy IMO",
        "created_at": "2020-09-02T21:32:48.127000+00:00",
        "attachments": null
    }
]