[
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "",
        "created_at": "2020-05-29T13:06:13.812000+00:00",
        "attachments": [
            {
                "type": "",
                "origin_name": "Chunk_utilization_in_the_basic_block_merklization_approach_with_a_minimum_chunk_size_of_32_bytes.png",
                "content": "e6f1de3641e5134cba24a98bee1c5e753883d56adf3d77e3334e2193953bf95b"
            }
        ]
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I wanted to see how much of the code that was being sent in the chunks were actually executed and how much were just due to chunking overhead. It seems chunk utilization is on average around 70%. Here I was using the jumpdest-based approach with a minimum chunk size of 32 bytes (those that are smaller are merged to reach the min size)",
        "created_at": "2020-05-29T13:08:53.831000+00:00",
        "attachments": null
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "This prototype used a hexary trie. Assuming a binary trie reduces proof hashes 3x, the optimal chunker would yield somewhere between ~11-15% improvement (in total code witness size)",
        "created_at": "2020-05-29T13:21:59.355000+00:00",
        "attachments": null
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Oh and I was thinking maybe we could encode a contract's `codeHash` and `codeSize` (and other metadata if necessary) in a special chunk in the code trie",
        "created_at": "2020-05-29T13:28:54.388000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Wait is that saying that an optimal chunker would only save ~11-15% relative to the witness having to contain the entire code?",
        "created_at": "2020-05-29T17:43:42.525000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Or am I misunderstanding?",
        "created_at": "2020-05-29T17:43:46.208000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Aaaah it's this chunking strategy compared to a hypothetical optimal chunking strategy",
        "created_at": "2020-05-29T17:44:33.060000+00:00",
        "attachments": null
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "weighing the option of an additional 10% improvement vs the complexity of any of the proposed alternative strategies that *might* get us that 10%, I continue to lean towards the very boring approach of fixed-size chunking for code merklization.",
        "created_at": "2020-05-29T18:36:44.570000+00:00",
        "attachments": null
    },
    {
        "author": "_drinkcoffee",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Agreed. \u003c@689635476826619924\u003e  is producing an implementation to independently reproduce \u003c@259648573401071617\u003e 's numbers.",
        "created_at": "2020-05-29T22:34:14.626000+00:00",
        "attachments": null
    },
    {
        "author": "_drinkcoffee",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@259648573401071617\u003e , I would expect more hashes needed for 32 byte chunks, compared to a larger chunking size. \u003c@689635476826619924\u003e and I were thinking about producing results for a range of chunk sizes. What are your thoughts on this?",
        "created_at": "2020-05-29T22:36:56.859000+00:00",
        "attachments": null
    }
]