[
    {
        "author": "hmijail",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!296630428754771968\u003e \u003c@!259648573401071617\u003e these are my current results after the fix: (binary tree, 32-byte hashes)\n`32-byte chunks\nrunning total: blocks=101    overhead=110.7%    exec=9092.2K    merklization=19155.2K = 11695.1 K chunks + 7460.1 K hashes    estimated median:14.0\n\n16-byte chunks\nrunning total: blocks=101    overhead=69.7%    exec=9092.2K    merklization=15425.1K = 10566.7 K chunks + 4858.4 K hashes    estimated median:14.0\n\n8-byte chunks\nrunning total: blocks=101    overhead=42.9%    exec=9092.2K    merklization=12995.9K = 9933.4 K chunks + 3062.5 K hashes    estimated median:14.0\n\n\n4-byte chunks\nrunning total: blocks=101    overhead=26.1%    exec=9092.2K    merklization=11464.7K = 9525.0 K chunks + 1939.7 K hashes    estimated median:14.0\n\n2-byte chunks\nrunning total: blocks=101    overhead=15.4%    exec=9092.2K    merklization=10494.8K = 9249.0 K chunks + 1245.8 K hashes    estimated median:14.0\n\n1-byte chunks\nrunning total: blocks=101    overhead=8.6%    exec=9092.2K    merklization=9871.9K = 9092.2 K chunks + 779.8 K hashes    estimated median:14.0`",
        "created_at": "2020-11-06T06:29:46.083000+00:00",
        "attachments": null
    },
    {
        "author": "hmijail",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "if no one reports any other problem ðŸ˜… , I'll fix the ethresear.ch post",
        "created_at": "2020-11-06T06:45:43.098000+00:00",
        "attachments": null
    },
    {
        "author": "_drinkcoffee",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Do the figures allow for needing one byte to indicate the first opcode in the chunk? How is this offset handled for a push32 when the chunk size is less than 32 bytes",
        "created_at": "2020-11-06T08:13:53.490000+00:00",
        "attachments": null
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!461762841641484302\u003e can I ask which segment that is? I'm confused because I'm getting different results (segments-9090000, binary tree, 32-byte hashes). Also the number of hashes are increasing with the chunk size in the results you shared, but intuitively they should decrease (lower tree depth)\n```\n1-byte chunks: running total: blocks=1000      overhead=283.3% exec=57093.0K   merklization=218830.7K = 57093.0 K chunks + 161737.7 K hashes           estimated median:23.0\n16-byte chunks: running total: blocks=1000      overhead=129.3% exec=57093.0K   merklization=130905.6K = 66553.9 K chunks + 64351.8 K hashes            estimated median:23.0\n32-byte chunks: running total: blocks=1000      overhead=116.6% exec=57093.0K   merklization=123666.9K = 73792.6 K chunks + 49874.2 K hashes            estimated median:23.0\n64-byte chunks: running total: blocks=1000      overhead=116.6% exec=57093.0K   merklization=123666.9K = 86801.6 K chunks + 36865.2 K hashes            estimated median:23.0\n```",
        "created_at": "2020-11-06T10:04:28.543000+00:00",
        "attachments": null
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "You might notice something curious in my results: proof size for 32-byte chunks and 64-byte is exactly the same. I thought it's coincidence but I ran it for multiple segments and it was always like that. No idea why, I'll have to dig deeper",
        "created_at": "2020-11-06T10:08:09.310000+00:00",
        "attachments": null
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "sidenote: it's cool to be able to just try all combinations of arity, chunkSize, etc. so easily, thanks for the tool!",
        "created_at": "2020-11-06T10:11:30.350000+00:00",
        "attachments": null
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e How is this offset handled for a push32 when the chunk size is less than 32 bytes\nI've been setting `offset = chunkSize` for the data-only chunks, but in lower chunk sizes these data chunks will be more prevalent and we might be better off with a different trick (like storing offset only for non-data chunks in metadata)",
        "created_at": "2020-11-06T10:21:39.030000+00:00",
        "attachments": null
    },
    {
        "author": "hmijail",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!636680106630316033\u003e this is a calculation for hashes in a simple merkle tree, with no consideration for that kind of metadata, since that should be much smaller and dependent on the final format of the data itself.",
        "created_at": "2020-11-06T11:41:36.529000+00:00",
        "attachments": null
    },
    {
        "author": "hmijail",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!259648573401071617\u003e try importing the module and just using the merklize() function with any random list of chunks, it's really illuminating!",
        "created_at": "2020-11-06T11:56:24.613000+00:00",
        "attachments": null
    },
    {
        "author": "hmijail",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "(deleted a bunch of messages for clarity)",
        "created_at": "2020-11-06T14:30:28.028000+00:00",
        "attachments": null
    },
    {
        "author": "hmijail",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "First of all, the segments file 9090000 is one of only 2 in the range 9M-10M with a median \u003e20. The median for that whole range is \u003c15, so I *guess* that there can be some statistical weirdness. \nTo avoid this kind of problem I've uploaded to the releases section of the tool an index file with the per-block, per-file and global stats for the range.\nHaving said that, I'm not sure how much that matters. Still looking into it.",
        "created_at": "2020-11-06T14:39:38.488000+00:00",
        "attachments": null
    },
    {
        "author": "hmijail",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I don't understand what has changed in the code to justify the difference, but now both files 9000000 and 9090000 seem to have a minimum overhead of ~110% around chunk size 48 (better than 44 or 52). Chunks smaller than 32 do increasingly terrible. I don't get it! ðŸ˜© \nI'll push my changes and try taking a fresher look in the morning.",
        "created_at": "2020-11-06T15:39:09.857000+00:00",
        "attachments": null
    },
    {
        "author": "hmijail",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I have deleted the overheads table in the Ethresear.ch article and added a couple of warnings.",
        "created_at": "2020-11-06T15:42:16.165000+00:00",
        "attachments": null
    },
    {
        "author": "_drinkcoffee",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e \u003c@!636680106630316033\u003e this is a calculation for hashes in a simple merkle tree, with no consideration for that kind of metadata, since that should be much smaller and dependent on the final format of the data itself.\n\u003c@!461762841641484302\u003e For small chunk sizes, the metadata would have a big impact. For example, if the chunk size was 4 bytes, then an extra byte to indicate the start of the first opcode adds 25% overhead.",
        "created_at": "2020-11-06T23:51:13.929000+00:00",
        "attachments": null
    }
]