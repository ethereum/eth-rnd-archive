[
    {
        "author": "_drinkcoffee",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "My vote is meta-data with the code. This will lead to fewer message digests in the proof.",
        "created_at": "2020-06-02T01:09:20.690000+00:00",
        "attachments": null
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!689635476826619924\u003e here's his own description of the approach:\n\u003e [...] we came up with an alternative that might be much cheaper but is admittedly a bit more complicated. The idea is that \"merkle\" proofs follow execution order instead of code offset. More details: The account also contains the root of a second hash tree that is created following all static jumps across basic blocks starting from PC 0, ignoring back edges (each tree node corresponds to such a basic block and its hash is created from its code, its starting PC and the hashes of the target blocks of static jumps). This new structure is actually a forest (it needs to be computed not just starting from PC 0, but there are multiple disconnected components) and needs to be linked from a full binary/merkle tree of the code. Proofs of execution now work as follows: the code of all executed code is included as well as the hashes (in this new structure) of all static jumps that are not taken. For each dynamic jump, we use a proof in the traditional binary/merkle tree of all code/all basic blocks. Since this tree links back into the new structure, we can  go back to the more efficient proofs directly after the jump. The proof size in addition to the actually executed code should be: one hash per static jump not taken plus a full binary merkle proof per dynamic jump.\n\u003e Note that if we disallow dynamic jumps completely, the execution will need its own binary search for each function.\n\u003e \n\u003e Actually the generation of the hash tree in my proposal above can be simplified: instead of a complicated graph search, just hash all static jumps to a pc larger than the current and treat all other static jumps as dynamic jumps",
        "created_at": "2020-06-02T12:27:49.140000+00:00",
        "attachments": null
    }
]