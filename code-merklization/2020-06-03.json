[
    {
        "author": "80raghavendra",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Thanks \u003c@!259648573401071617\u003e. From the text, here is my understanding. Feel free to correct me. Maintain two trees. One is a standard Merkle Tree where leaf nodes correspond to basic blocks. The second tree works on the same leaf nodes as the first one, but it has some kind of smart hashing as compared to standard Merkle hashing. The hash of a leaf basic block here is actually a hash of the basic block code + the starting pc + hashes of the chain of target blocks where execution starts from the leaf basic block under consideration. If the code that is executed is passed in, then the validator can construct this hash. If the validator is also passed in the hashes of the static jumps that are not taken, which are presumably small in number, then he can construct the root of this second Tree modulo dynamic jumps. In the case of dynamic jumps to basic blocks, such basic blocks along with their Merkle Proofs are included. So we need to revert back to the standard Merkle Tree for dynamic jumps. \n\nCompared to the idea where we group the entire execution path into leaf nodes, this idea avoids passing the Merkle Proofs for it, and passes a single hash / per disconnected basic block. The only burden seems to be of maintaining two separate trees and two roots in the account to me. I understand that it is conceptually effective in case there are no dynamic jumps.",
        "created_at": "2020-06-03T02:18:28.340000+00:00",
        "attachments": []
    },
    {
        "author": "80raghavendra",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "One possible extension to the idea is to statically analyse the code and find conservatively the destinations for the dynamic jumps. Being conservative, here we have to deal with an approximation of multiple destinations per dynamic jump. I see that if all the destinations fall into a single execution fragment, then the same construction of the second tree works and we get the same benefits as a static jump. If the destinations fall into different execution fragments then we will have to extend the grouping to somehow accommodate these dynamic jumps as well, then again I think we have a way of getting the same benefits. The main benefit of this extension is that it avoids the first Merkle Tree and the account maintains only one root.\n\nHas this idea been implemented? If so, please point me to the repository. Depending on the viability of resolving dynamic jumps, I am keen to implement this idea and the extension and see how it performs.",
        "created_at": "2020-06-03T02:18:35.586000+00:00",
        "attachments": []
    },
    {
        "author": "_drinkcoffee",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "An update on the function id methodology.\n\nThe result of the methodology: have a Merkle Patricia Trie of functions in a contract. Have the root of the trie stored in account state. The key into the trie is the function signature hash (what I am calling a function id). The leaves of the trie are all of the information needed to reconstruct the code that will execute for the function. That is, the set of (start program counter, length in bytes, code fragment bytes) which make up the code that could execute for the function.\n\nThe methodology: Have code that statically analyses contract code and creates the trie. The analysis would happen for existing contracts as a conversion process and for new contracts at deployment time. \n\nThe implementation is here: https://github.com/PegaSysEng/codewitness\n\nThe current state of the implementation is that I am still refining the code analysis. Once this can successfully analyse a large proportion of the contracts on MainNet, the task will then be to analyse how well it performs compared to the alternative approaches.\nFor 10000 contracts from MainNet, the analysis is currently showing: Probably Solidity: 3958, Definitely Solidity: 582, Should be able to Analyse: 3922, Successfully Analysed: 752\nProbably Solidity: Starts with setting up the memory pointer.\nDefinitely Solidity: The contract included aux data, it could be analysed, and it indicates Solidity.\nShould be able to Analyse: The methodology could detect the list of function ids in the code.\nSuccessfully Analysed: The static code analysis worked, and generated Merkle Patricia Trie leaves for all of the contracts.",
        "created_at": "2020-06-03T04:33:14.408000+00:00",
        "attachments": []
    },
    {
        "author": "_drinkcoffee",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e An update on the function id methodology.\nAfter finding an issue related to EVM stack usage, the number of Successfully Analysed is now 2789!",
        "created_at": "2020-06-03T05:46:50.715000+00:00",
        "attachments": []
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!689635476826619924\u003e no it hasn't been implemented. Although I like the idea of an execution graph quite a bit, the complexity was a deterrent for me personally. Like others I'm also leaning towards the simplest approach with decent savings. Oh and btw if time allows Christian might write up a post explaining his approach in more depth.",
        "created_at": "2020-06-03T11:45:30.846000+00:00",
        "attachments": []
    },
    {
        "author": "80raghavendra",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@259648573401071617\u003e Is it possible to access your fixed size and jump dest chunking implementations?",
        "created_at": "2020-06-03T12:42:45.659000+00:00",
        "attachments": []
    },
    {
        "author": "s1na",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!689635476826619924\u003e Sorry in advance for the mess! here's the branch: github.com/ewasm/biturbo/pull/64\nthe chunking and merkleization logic: https://github.com/ewasm/biturbo/blob/merklize-mainnet-blocks/src/relayer/bytecode.ts\nthe test runner which takes 50 mainnet blocks, merkleizes the codes and produces stats: https://github.com/ewasm/biturbo/blob/merklize-mainnet-blocks/test/relayer.ts#L221\nthe blocks and their prestate: https://github.com/ewasm/biturbo/blob/merklize-mainnet-blocks/test/fixture/blocks-prestate.json",
        "created_at": "2020-06-03T13:47:08.404000+00:00",
        "attachments": []
    }
]