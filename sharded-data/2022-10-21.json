[
    {
        "author": "supernovahs",
        "category": "Cross-layer",
        "parent": "",
        "content": "I have a query. Does EIP 4844 work at each transaction level or at a block level?",
        "created_at": "2022-10-21T17:29:57.181000+00:00",
        "attachments": null
    },
    {
        "author": "supernovahs",
        "category": "Cross-layer",
        "parent": "",
        "content": "If there' s an L2 tx on optimism  and I have the calldata, How to calculate the change in proof size as against the current calldata.",
        "created_at": "2022-10-21T17:31:01.389000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003cprotolambda\u003e The L1 blob pricing is changing per-block and expressed in data-gas per blob, the opcode that refers to the batch datahash by index is per-transaction (to avoid txs having conflicts/dependencies with blobs).",
        "created_at": "2022-10-21T17:35:44.065000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003cprotolambda\u003e A L2 tx gets bundled with other L2 txs, compressed together, and split between potentially multiple data-transactions that use 4844 blob data. It's quite disconnected from the availability layer. L2 tx pricing would use L1 cost averages per L2 tx byte to approximate the cost of the L2 tx.",
        "created_at": "2022-10-21T17:37:39.668000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003cxrchz\u003e how does the packing into blobs get done so there isn't space left over? i.e., the buffering of data up till the 4096*32 byte boundaries",
        "created_at": "2022-10-21T21:49:40.407000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003cprotolambda\u003e \u003e \u003cprotolambda\u003e  We batch txs into block tx lists with metadata, encode those as rlp stream, and compress that stream. The stream is then cut up in pieces based on available space in data transactions. And we do some multi-plexing like thing to handle when txs go missing. The result is that we can fill data txs pretty efficiently, and compress a lot together if we don’t submit too frequently. This is all coming in next upgrade though (currently on testnet), mainnet optimism today doesn’t pack that efficient yet, and can only fit whole txs (although compressed with others) in the L1 data tx. (re @xrchz: how does the packing into blobs get done so there isn't space left over? i.e., the buffering of data up till the 4096*32 byte boundaries)",
        "created_at": "2022-10-21T21:59:01.631000+00:00",
        "attachments": null
    }
]