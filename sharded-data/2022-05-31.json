[
    {
        "author": "yahgwai",
        "category": "Cross-layer",
        "parent": "",
        "content": "What are the methods for increasing data throughput of danksharding?\n\nIf the coding remains 2D, increasing the size of either rows or columns would result in a linear increase in the amount of work done by each validator. Also, plotting this function: https://notes.ethereum.org/@dankrad/minimum-reconstruction-validators shows a linear relationship between area (n^2) and number of validators. So an increase in area would have a linear increase in work done by each validator, and a linear increase in number of validators which is an O(d^2) increase overall. \n\nAnother option would be to add more data as planes in a 3rd dimension (but validators still on sampling in 2 random dimensions) then more data can be added without increasing the amount of work done per validator. The total number of validators required also seems to be linear w.r.t to volume. So the overall increase by adding a 3rd dimension would be a O(d) increase in work done by the system - which is good. However adding a 3rd dimension increases the redundant data ratio from 1:3 to 1:7, meaning that for every piece of data the block producer would need to propogate more data. \n\nAre there other ways to increase data throughput? Ideally an increase in throughput would only correspond to a linear increase in work done by the system, and wouldn't adversly affect other resources like bandwidth.",
        "created_at": "2022-05-31T15:01:33.607000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003cdankrad\u003e This does not seem quite accurate. A linear increase in the data would case an sqrt(n) increase in the side length of the square",
        "created_at": "2022-05-31T18:27:35.602000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003cdankrad\u003e But this is for 4844 so this discussion is off topic here",
        "created_at": "2022-05-31T18:27:59.668000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003cprotolambda\u003e \u003c@555483069038198827\u003e this telegram group is being bridged to the sharded-data discord channel, so it’s not really off topic",
        "created_at": "2022-05-31T18:29:43.511000+00:00",
        "attachments": null
    },
    {
        "author": "ralexstokes",
        "category": "Cross-layer",
        "parent": "",
        "content": "(sorry i fat fingered the audio record in telegram)",
        "created_at": "2022-05-31T18:32:28.191000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003camvlasov\u003e Is there a plan to have stricter implementation requirements for 4844? E.g. some trivial precomputations for miller loop due to fixed G2 point can have another 15% I believe",
        "created_at": "2022-05-31T18:34:44.728000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Cross-layer",
        "parent": "",
        "content": "\u003cdankrad\u003e \u003e \u003cdankrad\u003e  I'm going by the title of the telegram group (re @protolambda: \u003c@555483069038198827\u003e this telegram group is being bridged to the sharded-data discord channel, so it’s not really off topic)",
        "created_at": "2022-05-31T18:35:56.650000+00:00",
        "attachments": null
    },
    {
        "author": "liamhorne3565",
        "category": "Cross-layer",
        "parent": "",
        "content": "Sorry for the confusion. Changed name of Telegram group to clarify.",
        "created_at": "2022-05-31T18:37:04.667000+00:00",
        "attachments": null
    }
]