[
    {
        "author": "pk910",
        "category": "Testing",
        "parent": "",
        "content": "is it possible to find the ip of a validator that easily?\nif i understood correctly a validator might connect to various beacon nodes and is not bound to a specific one - so it'll just switch to another when the beacon node gets ddos'ed and the VC has fallback BNs..  \nthe VC is not visible to the net as the IP is not announced anywhere and should be safe?",
        "created_at": "2022-01-14T00:28:18.388000+00:00",
        "attachments": []
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "It depends on the setup. If VC is on the same machine as BN then the task of finding VC address is reduced to finding BN address that is serving particular validator which isn't that hard considering trivial heuristics like BN subscription to subnets and other more advance heuristics exist.\n\nIf VC is connected to multiple BNs via intranet it's hard to figure out VC address. And as you've mentioned VC connected to multiple BNs is more resistant to DoS attacks as it's always has fallback BNs, and potentially disseminates blocks and attestations through several BNs at the moment (or all of them).",
        "created_at": "2022-01-14T05:13:47.473000+00:00",
        "attachments": []
    },
    {
        "author": "killari.",
        "category": "Testing",
        "parent": "",
        "content": "I don't know the distribution of how many ip's run x% of all the validators. But I would think that you don't need to DOS that many IP's to take majority of validator power out.\n\nAnyway, after a such attack has been conducted, I believe validators would enforce their defenses. So the attack is probably only going to be super effective for a short while. Unfortunately such defenses make it more and more expensive to run validators and we get economies of scale",
        "created_at": "2022-01-14T09:04:27.208000+00:00",
        "attachments": []
    },
    {
        "author": "matt_mm",
        "category": "Testing",
        "parent": "",
        "content": "I have one interesting question. I've manage to run EL+CL. I've used my own `genesis.json` for EL - but the contract storage is 1:1 with `kintsugi`. I've generated CL state file (`genesis.ssz`) using my mnemonic and 70 validators (pubkeys). After chain started (CL genesis time reached) CL was able to attest and submit blocks... How it's possible?",
        "created_at": "2022-01-14T09:25:46.382000+00:00",
        "attachments": []
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "So the contract storage is just the deposit contract, it has 0 deposits by default. The `eth2-testnet-genesis` tool adds the deposits based on information from `mnemonics.yaml`, which you defined and hence have a different validator set from Kintsugi. Your CL chain is just going to be different than the Kintsugi chain, it just happens to have the same EL deposit contract. You can reuse the EL deposit contract across as many CL chain as you like. Even if you choose the same fork version, your genesis state would be different and no kintsugi node *should* peer with you or accept your blocks in the canonical chain.",
        "created_at": "2022-01-14T09:31:34.920000+00:00",
        "attachments": []
    },
    {
        "author": "matt_mm",
        "category": "Testing",
        "parent": "",
        "content": "got it \u003c@!199561711278227457\u003e , one more time - thank you ðŸ™‚",
        "created_at": "2022-01-14T11:36:23.956000+00:00",
        "attachments": []
    },
    {
        "author": "kommstar",
        "category": "Testing",
        "parent": "",
        "content": "I think it is trivial to change you validators to another beacon node, also could you private network sync nodes to allow blocks, attestations and the like to be available from multiple ips at various data centers globally?",
        "created_at": "2022-01-14T14:34:21.177000+00:00",
        "attachments": []
    },
    {
        "author": "killari.",
        "category": "Testing",
        "parent": "",
        "content": "its trivial to find the new ips too, unless you change them constantly",
        "created_at": "2022-01-14T14:35:23.399000+00:00",
        "attachments": []
    },
    {
        "author": "kommstar",
        "category": "Testing",
        "parent": "",
        "content": "At some point though the attacker is going to feel like they are attacking all the nodes",
        "created_at": "2022-01-14T14:35:52.299000+00:00",
        "attachments": []
    },
    {
        "author": "killari.",
        "category": "Testing",
        "parent": "",
        "content": "if you arebig staker, you can invest more in infra ofcourse",
        "created_at": "2022-01-14T14:36:06.703000+00:00",
        "attachments": []
    },
    {
        "author": "kommstar",
        "category": "Testing",
        "parent": "",
        "content": "You could do all that on a relatively small budget",
        "created_at": "2022-01-14T14:36:27.523000+00:00",
        "attachments": []
    },
    {
        "author": "killari.",
        "category": "Testing",
        "parent": "",
        "content": "they attack all, but only one at time",
        "created_at": "2022-01-14T14:36:27.893000+00:00",
        "attachments": []
    },
    {
        "author": "ryanleeschneider",
        "category": "Testing",
        "parent": "",
        "content": "Is the following expected behavior for syncing a new geth/teku pair to kintsugi?\n\n- geth snap syncs up to current block \n- teku independently syncs the beacon chain up to current slot\n- teku then seems to set geths head to the first post-merge block, and then each block is replayed but ignored by geth in sequence\n\nThis causes geth to start spamming missing trie errors, presumably because the txpool and mining contexts aren't aware that the head was rolled back?  For example:\n\n```\nJan 14 18:20:49 ip-10-0-65-185 geth[9213]: INFO [01-14|18:20:49.628] Ignoring already processed block         number=11179   hash=04b387..468d33\nJan 14 18:20:49 ip-10-0-65-185 geth[9213]: INFO [01-14|18:20:49.645] Setting head                             head=04b387..468d33\nJan 14 18:20:49 ip-10-0-65-185 geth[9213]: INFO [01-14|18:20:49.648] Ignoring already processed block         number=11180   hash=356876..340b4d\nJan 14 18:20:49 ip-10-0-65-185 geth[9213]: INFO [01-14|18:20:49.657] Set the chain head                       number=11179   hash=04b387..468d33\nJan 14 18:20:49 ip-10-0-65-185 geth[9213]: ERROR[01-14|18:20:49.657] Failed to reset txpool state             err=\"missing trie node e414ea4fc4c0bd464f1b138f8cee316e89b697496394bac90e90f652f882f4ed (path )\"\nJan 14 18:20:49 ip-10-0-65-185 geth[9213]: ERROR[01-14|18:20:49.657] Failed to create mining context          err=\"missing trie node e414ea4fc4c0bd464f1b138f8cee316e89b697496394bac90e90f652f882f4ed (path )\"\n```",
        "created_at": "2022-01-14T18:21:32.779000+00:00",
        "attachments": []
    },
    {
        "author": "mariusvanderwijden",
        "category": "Testing",
        "parent": "",
        "content": "Right now, yes. We will change geth to only sync up to ttd point",
        "created_at": "2022-01-14T18:25:19.186000+00:00",
        "attachments": []
    },
    {
        "author": "ryanleeschneider",
        "category": "Testing",
        "parent": "",
        "content": "Things seemed to stall out so I restarted both teku and geth and got this error:\n\n```\nJan 14 22:54:02 ip-10-0-65-185 geth[50961]: ERROR[01-14|22:54:02.228] Impossible reorg, please file an issue   oldnum=148,276 oldhash=62f857..0972ca oldblocks=0 newnum=148,276 newhash=62f857..0972ca newblocks=0\n```",
        "created_at": "2022-01-14T22:55:52.203000+00:00",
        "attachments": []
    }
]