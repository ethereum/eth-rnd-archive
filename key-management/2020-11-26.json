[
    {
        "author": "sproul",
        "category": "Consensus Layer",
        "parent": "",
        "content": "In hindsight I think the `allow_partial_import` flag could be clearer. Its intention was to say: allow (but don't require) an implementation to import some subset of `.data`, because some of the entries appear to contain slashable data. Specifically: entries with slashable data can be skipped, regardless of ordering, and if skipped they should be skipped in their entirety. So an interchange like `{data: [good, bad, good, good]}` would import all 3 `good` entries, while omitting the `bad` entry and all parts of it.\n\nMy understanding of web3Signer's implementation was that you were planning to be as liberal as possible in what you accept, in which case you should ignore the `allow_partial_import` flag and just aim to always succeed on import (unless `should_succeed` is false, which is only the case in very few of the tests). A better test formulation would probably be to write `allow_partial_import_for: [Pubkey]`, but I'm loathe to change the format _yet again_.\n\nAnd no worries about the pretty printing, I merged your PR and will update the generator to make sure tests are pretty printed in future üòä",
        "created_at": "2020-11-26T00:20:14.533000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Hey \u003c@602753420033785856\u003e had a question on the EIP , was reading through it and noticed this condition \n```\nRefuse to sign any block with¬†slot \u003c= min(b.slot for b in data.signed_blocks if b.pubkey == proposer_pubkey), except if it is a repeat signing as determined by the¬†signing_root.\n```\nWhat is the reason for this being a min instead of a max ? If you do receive a block with a lower slot than your max slot, shouldnt you also reject it ?",
        "created_at": "2020-11-26T01:05:08.974000+00:00",
        "attachments": null
    },
    {
        "author": "sproul",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Hey, thanks for reaching out. The idea here is to explicitly support signing around clock drift bugs. So if a validator accidentally signs some things in the far future, they aren't _completely_ blocked from signing new messages so long as they don't sign a surround. E.g. if the current epoch is 100, and I sign from 99=\u003e200 (assuming 99 is justified), then I can keep signing 99=\u003e100, 99=\u003e101, 99=\u003e102, until something ahead of 99 justifies, at which point I can't sign x=\u003ey for x\u003e99, y\u003c200.\n\nImplementations are free to use the maximum as a lower bound (and that's what Teku does), but they don't have to (Lighthouse uses the minimum).",
        "created_at": "2020-11-26T01:12:05.317000+00:00",
        "attachments": null
    },
    {
        "author": "sproul",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Oh sorry, just realised you were asking about blocks üòÖ  The idea behind the block rule is similar. The minimum slot block in an import file represents the beginning of \"known history\", and it's assumed we have accurate info between that point and the end of the interchange file, so signing blocks in between (in case of clock drift) is ok.",
        "created_at": "2020-11-26T01:14:42.356000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Ah ok that makes sense , guess this is an easy sanity check in light of possible clock drifts . Double signing proposals will still be caught later on with the assumption of accurate record history. Thanks for clarifying üëç",
        "created_at": "2020-11-26T01:24:33.310000+00:00",
        "attachments": null
    }
]