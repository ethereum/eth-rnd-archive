[
    {
        "author": "kevaundray",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "EVMMAX call in 5 minutes: https://github.com/ethereum/pm/issues/1204",
        "created_at": "2024-12-05T11:56:19.375000+00:00",
        "attachments": null
    },
    {
        "author": "kevaundray",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "EVMMAX Implementers call",
        "created_at": "2024-12-05T13:22:18.936000+00:00",
        "attachments": null
    },
    {
        "author": "lu_pinto",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003c@427903869226582018\u003e quick search yield this interesting benchmark between JNI and FFI: https://github.com/Glavo/java-ffi-benchmark?tab=readme-ov-file\n\nI think the closest to our use case for ADDMODX, ... is the NoopBenchmark (performances are pretty comparable) as we are providing slot indices in EVMMAX memory for computation correct? Not sure if trivial call would be applicable to this case, maybe so since it's only arithmetics.\n\ndoes EVMMAX, currently for ADDMODX and so on, store the result in an EVMMAX memory slot? if that's the case then the most costly would be STOREX and LOADX of the entire lot because you need to convert from foreign memory and currently our EVM stack implementation works with an object allocated on the heap containing a byte array (this is yet another object on the heap). For that, benchmark C String to Java String looks the most interesting but I think we should measure it ourselves because in a byte[] we have direct access to the length and it will be faster than searching for the NULL byte.\n\n\u003c@427903869226582018\u003e  do you have any throughput numbers for Go? What's the throughput you were after. Maybe we could compare\n\nPS: another note is that this project is still in preview, so not stable yet",
        "created_at": "2024-12-05T14:11:01.263000+00:00",
        "attachments": null
    },
    {
        "author": "jwasinger",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e are providing slot indices in EVMMAX memory for computation correct?\n\nYeah, we are providing 7 bytes of input per `ADDMODX`/`SUBMODX`/`MULMODX`.",
        "created_at": "2024-12-05T15:49:57.013000+00:00",
        "attachments": null
    },
    {
        "author": "jwasinger",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e does EVMMAX, currently for ADDMODX and so on, store the result in an EVMMAX memory slot?\n\nYeah, this is correct.",
        "created_at": "2024-12-05T15:50:32.240000+00:00",
        "attachments": null
    },
    {
        "author": "jwasinger",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e do you have any throughput numbers for Go? What's the throughput you were after. Maybe we could compare\n\n\u003c@1252939266871263294\u003e I have Go benchmarks of the arithmetic implementation here: https://github.com/jwasinger/evmmax-arith .\n\n\nI have a set of self-contained benchmarks (executable EVM bytecodes):  these perform many invocations of `ADDMODX`/`SUBMODX`/`MULMODX` such that the overhead of startup is amortized.  I estimate interpreter loop overhead from these.  However, they are not updated for the latest version of EIP 6690.  I can update and post them here.",
        "created_at": "2024-12-05T16:00:34.870000+00:00",
        "attachments": null
    },
    {
        "author": "jwasinger",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "So a good measure of the FFI overhead would be to measure a method that takes a single `uint64` parameter and returns an error flag.  related:  It might be worth it to implement a C-EVMMAX library and have clients like Besu/Nethermind call into this library for speed reasons if the FFI overhead can be made low enough?",
        "created_at": "2024-12-05T16:06:55.409000+00:00",
        "attachments": null
    },
    {
        "author": "jwasinger",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "but \u003c@1252939266871263294\u003e to respond to the benchmarks you posted:  I think anything from the JNI/Panama categories on that graph is pretty good!\n\nOn my m2 macbook pro, Geth ecrecover precompile runs at ~65 mgas / sec which converts to ~16 ns / gas.  The JNI benchmarks you posted show ~3 ns of overhead per invocation, which seems pretty acceptable to me (obv it's not apple-to-apples.  we are not comparing benchmarks from the same machine.  but from a glance it seems like it could be okay)",
        "created_at": "2024-12-05T16:11:59.720000+00:00",
        "attachments": null
    }
]