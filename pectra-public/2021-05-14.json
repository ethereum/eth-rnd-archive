[
    {
        "author": "m.kalinin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "In order to implement dynamic computation of `TRANSITION_TOTAL_DIFFICULTY` we'd need a block that the beacon chain agrees on to become a source of `total_difficulty` and `difficulty` and then do a simple math: `TRANSITION_TOTAL_DIFFICULTY = block.total_difficulty + block.difficulty*X`. A straightforward candidate for the block is the one from `state.eth1_data`. \n\nBut the problem here is that `eth1_data` is 11 hours in the past in the best case hence `difficulty` of blocks after the source block may go either down or up. And we definitely want `X` to be large enough to get `TRANSITION_TOTAL_DIFFICULTY` in the future. Getting it in the past opens a possibility of throwing away a segment of PoW blocks during the transition.\n\nI might not get the property of the difficulty formula correct and if difficulty increases significantly then time between blocks also increases hence less blocks produced. If it goes down then we see PoW blocks more frequent and more blocks produced. And after all the source block difficulty might be the best estimation for us and we just need to set `X` to some high number like `2**14` which is four times larger than the worst case voting period.\n\nAlso, we must be sure that during the recent voting period proposers came to agreement on a block, otherwise, `X` might not be enough if e.g. several voting periods failed in row. The other thing to check is that the block proposers have agreed upon exists on the PoW chain.\n\nThese are my thoughts. Maybe you have other intuition on it?",
        "created_at": "2021-05-14T08:35:56.731000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Agree. It definitely more difficult than running a post-merge testnet. And IMO not every step in transition testnet run could be automated. But we will automate as much as we can as transition process is gonna be tested dozens of times.",
        "created_at": "2021-05-14T08:37:42.965000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "FYI, there is a decision to move Merge Implementers Call to time slot that is 1 hour before Eth2 Implementers call to batch them together and allow for better time management. We will try and see how does this schedule work.\n\nThe next call is planned for **Thursday, June 3, 13:00 UTC**",
        "created_at": "2021-05-14T08:52:18.867000+00:00",
        "attachments": null
    },
    {
        "author": "sma4321",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "is there scope for an eth1-side soft rule that, if an eth1 client is talking to a beacon chain consensus layer within 12 hours of the merge, it will not reorg greater than x blocks? where x is some small number that eth1 sees sometimes but not often? (plus, if it's that close and a client doesn't have a beacon node, it is going to fall away soon anyway)",
        "created_at": "2021-05-14T14:52:48.308000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e And after all the source block difficulty might be the best estimation for us and we just need to set X to some high number like 2**14 which is four times larger than the worst case voting period.",
        "created_at": "2021-05-14T16:32:47.673000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah that's what I had in mind",
        "created_at": "2021-05-14T16:32:56.925000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I would support a soft eth1-side rule that once a block is an eth1data block in the canonical beacon chain, it can't be reorged",
        "created_at": "2021-05-14T16:49:09.171000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "But adding such logic may overcomplicate things",
        "created_at": "2021-05-14T16:49:23.678000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "so perhaps not worth it...",
        "created_at": "2021-05-14T16:49:28.650000+00:00",
        "attachments": null
    },
    {
        "author": ".vbuterin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "`2**14` blocks is a totally fine delay, I actually had the number of 2 days in mind myself ðŸ¤£",
        "created_at": "2021-05-14T16:50:27.972000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I see. `2**14` blocks is 2.5 days if there are `14` seconds per block.\n\nI am also curious about two edge cases. What happens with accuracy of this estimation if\na) hashrate drastically *increases* with increased difficulty and decreased block times\nb) hashrate drastically *decreases* resulting in decreased difficulty and increased block times\n\nMy intuition is that real total difficulty in these two edge cases would be more or less similar to the estimated one. Thus, the estimation should be pretty accurate.",
        "created_at": "2021-05-14T17:57:11.684000+00:00",
        "attachments": null
    }
]