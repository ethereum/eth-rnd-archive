[
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Hey \u003c@755590043632140352\u003e do you which of the prysm nodes are run by prylabs vs EF ?",
        "created_at": "2021-03-24T00:06:09.917000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I am guessing the EF prysm nodes havent been updated",
        "created_at": "2021-03-24T00:06:24.981000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "But i will take a look more at the inconsistent vote",
        "created_at": "2021-03-24T00:06:45.752000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I don't think they are running with the `graffitiwal` graffiti, I'm only counting those as yours. Perhaps \u003c@!199561711278227457\u003e  can confirm, but I suppose they have their own graffiti",
        "created_at": "2021-03-24T00:09:35.953000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!476250636548308992\u003e",
        "created_at": "2021-03-24T00:09:41.418000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Ah k thanks",
        "created_at": "2021-03-24T00:10:00.010000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Will take a look into it then",
        "created_at": "2021-03-24T00:10:07.864000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I don't see anything wrong in your PR, but it looks as though the Powchain service is not started in some nodes. However, a little over 50% of the last 360 votes from prysm nodes where to the right hash, so perhaps this is an issue with deployment itself, perhaps you didn't really update some nodes?",
        "created_at": "2021-03-24T00:15:17.837000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "These are the indexes of the last bad votes just in case:\n```\n$ log -r 180 -t beacon blocks  | grep graffitiwal | grep -v 0xcf6c22ee1 \n96         3075   52807     128     0  0/0     0    graffitiwall:69:77:#009695         0x9b9cf2e7a    0x135a2eb9d\n95         3058   77891     128     0  0/0     0    graffitiwall:31:90:#00ffff         0x93f65ec06    0xfeb92621b\n94         3022   61635     128     0  0/0     0    graffitiwall:53:65:#00c1c1         0x97644071d    0x404b2e346\n93         2996   48133     128     0  0/0     0    graffitiwall:64:60:#00b9b9         0x03c9ce36b    0xa03f4ad73\n92         2974   62680     128     0  0/0     0    graffitiwall:4:76:#788c8c          0x64a47d5b7    0x744ffbdcf\n92         2967   76737     128     0  0/0     0    graffitiwall:45:87:#00c1c1         0x176c8e174    0x4548e24fe\n92         2965   55367     128     0  0/0     0    graffitiwall:11:54:#fdfdfd         0x80137140f    0x1697648d3\n92         2953   55901     128     0  0/0     0    graffitiwall:47:84:#00c1c1         0xf1b08f58e    0x790e670cc\n92         2949   79234     128     0  0/0     0    graffitiwall:60:86:#009695         0xdafa17ff1    0x384814b46\n91         2917   79175     128     0  0/0     0    graffitiwall:35:35:#00ffff         0x68d2e3b0d    0x47d4ab5ca\n91         2914   78654     128     0  0/0     0    graffitiwall:51:38:#00ffff         0xee465d206    0x383ff4429\n90         2905   50469     128     0  0/0     0    graffitiwall:27:29:#628080         0xb46c04734    0x27978fb0b\n90         2904   79577     128     0  0/0     0    graffitiwall:62:49:#00ffff         0x30f301de7    0xedfe4fcad\n90         2899   51621     128     0  0/0     0    graffitiwall:16:83:#477272         0x113b8394e    0x83a8a9e45\n```",
        "created_at": "2021-03-24T00:16:40.548000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "what's impressive is that all blocks are completely full",
        "created_at": "2021-03-24T00:18:12.277000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Ah thats a good point",
        "created_at": "2021-03-24T00:21:03.600000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Let me try clearing our dbs and resyncing",
        "created_at": "2021-03-24T00:21:16.170000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Maybe it will help here",
        "created_at": "2021-03-24T00:21:25.175000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I think there maybe some inefficiencies in aggregation",
        "created_at": "2021-03-24T00:24:06.402000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I have seen a few repeated attestations",
        "created_at": "2021-03-24T00:24:21.667000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "please could you tag me when this is done?",
        "created_at": "2021-03-24T00:25:05.152000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Sure thing , i will tag you here üëç",
        "created_at": "2021-03-24T00:25:24.022000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!755590043632140352\u003e in the process of rolling it out now",
        "created_at": "2021-03-24T06:53:44.085000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "should be done in the next 20 minutes",
        "created_at": "2021-03-24T06:53:54.533000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "also this might be relevant, I have been digging through our logs and it appears that some of our nodes have issues sustaining long lived connections with our goerli nodes",
        "created_at": "2021-03-24T06:58:16.830000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "they frequently are getting disconnected",
        "created_at": "2021-03-24T06:58:32.751000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!755590043632140352\u003e all our prater nodes are now resynced, however I am pretty sure the cause of the bad votes is because our goerli nodes are having issues staying up. Looking into how to stabilize them",
        "created_at": "2021-03-24T07:09:32.409000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "There are a few, but not lots.",
        "created_at": "2021-03-24T07:32:33.570000+00:00",
        "attachments": [
            {
                "type": "text/plain; charset=ascii",
                "origin_name": "message.txt",
                "content": "9efdf255b98d01fb39c7bcc40c1ab20be90eb96774e428c6e5905b7fd4493b18"
            }
        ]
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "From mainnet, I think the issue is more that we have a lot of attestations that show up after the event, so existing aggregates are added to over time and each time an addition comes in the aggregate is re-added to the chain.  But I haven't carried out a proper analysis of the numbers to confirm that.",
        "created_at": "2021-03-24T07:34:03.958000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!144468805697929216\u003e do you have an idea of how well aggregation is being carried out currently in the network currently ?",
        "created_at": "2021-03-24T08:22:13.081000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Not really.  I haven't come up with a suitable process to provide suitable metrics.\n\nSome quick SQL suggests that the attestations are on average 62% full, but that doesn't take in to account duplicates (e.g. attestation 1 has 124/128 indices included, attestation 2 in the following block has 125/128 indices included).  I also suspect that the distribution would be more interesting than the average.",
        "created_at": "2021-03-24T08:50:59.965000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah, here you go.  A lot of lone attestations out there.",
        "created_at": "2021-03-24T08:54:49.240000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "inclusion.png",
                "content": "00ccf59466812b24a1dbb50d6b89a309f84558384624e5e909fdcfb74b23c0b3"
            }
        ]
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah would be interested to see how effective it is in the current conditions, with all blocks being completely full",
        "created_at": "2021-03-24T08:54:53.928000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "ah great ! thanks",
        "created_at": "2021-03-24T08:54:58.572000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Next question would be if those lone attestations _could_ have been aggregated.",
        "created_at": "2021-03-24T08:55:28.635000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "The lone attestations are pretty interesting",
        "created_at": "2021-03-24T08:55:35.741000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I suspect they could have been but might not have reached the aggregator in time( or at all)",
        "created_at": "2021-03-24T08:57:21.495000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "The EF prysm nodes have their graffiti set to be `EF #VM_name`, e.g `EF #vm-eth2-raw-iron-prater-605`. And I'm running the Prysm nodes on `v1.3.4`, is there a more recent image I should update it to?",
        "created_at": "2021-03-24T09:34:31.184000+00:00",
        "attachments": null
    },
    {
        "author": "zahary.",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We've published a new release this morning (v1.0.12) that should fix the voting issue. It has been deployed to all servers on our fleet except one (so some rogue votes are still expected)",
        "created_at": "2021-03-24T11:44:00.701000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Are you all also experiencing similar network traffic? Is that the expected level?\n\nTimeframe of the graph: 12 hours\nClients in the list:\n601: Nimbus\n602: Lighthouse\n603: Prysm\n604: Nimbus\n605: Lighthouse\n\nTeku is on a different host, but also shows similar(~20Mbps) network load.",
        "created_at": "2021-03-24T13:08:43.016000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "Screenshot_2021-03-24_at_6.27.39_PM.png",
                "content": "61179c1ec397f454f026372cf79e736359e202066e6845e5036bccdddc4316aa"
            }
        ]
    },
    {
        "author": "protolambda",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!199561711278227457\u003e Nimbus used to consume more bandwidth than other clients, not sure how much that has changed. Also check the peer count per client, bandwidth usage may be skewed towards clients with more peers.",
        "created_at": "2021-03-24T13:46:59.175000+00:00",
        "attachments": null
    },
    {
        "author": "zahary.",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We've made quite a lot of improvements to our bandwidth usage",
        "created_at": "2021-03-24T13:47:34.237000+00:00",
        "attachments": null
    },
    {
        "author": "zahary.",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "But we are still running with a larger number of peers by default which may be a source of higher bandwidth usage",
        "created_at": "2021-03-24T13:48:30.359000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "So all the clients have between 45-50 peers, I could try reducing it to 25 peers if it seems that we might hit data limits.",
        "created_at": "2021-03-24T14:13:51.763000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Seeing general agreement on `0xd0cd3816def6b6e5171ef8ebe9667e0cf8f8e8adc82c635f8dffa14b32e83b82` for the current voting period, with `0xe89bd506fc0c00947a4228e12f0da21b9e16b511a2b4843be46cd881c4464a7f` as a distant second (plus a bunch of randoms).",
        "created_at": "2021-03-24T15:06:05.531000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I'm running only one prysm node and it's taking 4 times the bandwidth than mainnet",
        "created_at": "2021-03-24T17:41:40.264000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "That's the biggest change resource wise for me",
        "created_at": "2021-03-24T17:42:11.926000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "It's hard to get peers in prater, I'm putting my max at 80 and can't get more than 40 reliably",
        "created_at": "2021-03-24T18:12:29.101000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "There probably aren't that many prater peers out there.",
        "created_at": "2021-03-24T18:42:28.473000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Most of our Teku nodes are advertising the wrong IP currently (there as an IP change and config hasn't been updated to match).  I'll fix that today.  Those advertising the wrong IP have about 50 peers, but those advertising the right IP are maxing out at 74 peers.",
        "created_at": "2021-03-24T21:28:34.685000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "My lighthouse node has 45 and my teku node 65.",
        "created_at": "2021-03-24T21:41:34.414000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Last comment on ETH1 voting periods: they seem to have settled down a fair bit, but still seeing a significant portion of nodes voting for the previous voting period's hash:\n```\n f_voting_period |                         f_eth1_block_hash                          | f_count \n-----------------+--------------------------------------------------------------------+---------\n               0 | \\xe292fb224c989941dee48174ce547de76aabe70db9a6ea32ec6e2e5747df7a90 |      28\n               0 | \\x5ac670562dbf877a45039d65ec3c2e3402a40eda9b1dba931c2376ab7d0927c2 |     499\n               0 | \\xac0daf8c8c6d52485c1932c530569446e48925938d7ea9eca868c984e985cd89 |     898\n               1 | \\x5ac670562dbf877a45039d65ec3c2e3402a40eda9b1dba931c2376ab7d0927c2 |     435\n               1 | \\xcf6c22ee1b8b40468d31373fe8427460e62ca5c51e70b90b79fae090774530cb |    1194\n               2 | \\xcf6c22ee1b8b40468d31373fe8427460e62ca5c51e70b90b79fae090774530cb |     286\n               2 | \\xe89bd506fc0c00947a4228e12f0da21b9e16b511a2b4843be46cd881c4464a7f |    1138\n               3 | \\xe89bd506fc0c00947a4228e12f0da21b9e16b511a2b4843be46cd881c4464a7f |      91\n               3 | \\xd0cd3816def6b6e5171ef8ebe9667e0cf8f8e8adc82c635f8dffa14b32e83b82 |    1677\n               4 | \\xd0cd3816def6b6e5171ef8ebe9667e0cf8f8e8adc82c635f8dffa14b32e83b82 |     339\n               4 | \\x7c9e1b9d9f527bf339f742e4745db3b70ed7802a8a26b45f9c1ef996fbdafbfd |     961\n```",
        "created_at": "2021-03-24T21:45:07.836000+00:00",
        "attachments": null
    },
    {
        "author": "protolambda",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "EF updated prysm and teku nodes, but not yet our Nimbus nodes (as far as I'm aware), tomorrow it should improve",
        "created_at": "2021-03-24T21:46:32.190000+00:00",
        "attachments": null
    }
]