[
    {
        "author": "ralexstokes",
        "category": "general",
        "parent": "",
        "content": "here's a specific proposal to operationalize EIP-7892 (BPO forks) for Fusaka:\n\nhttps://hackmd.io/@ralexstokes/blob-acc-2025\n\nwould love your thoughts",
        "created_at": "2025-03-27T19:45:22.125000+00:00",
        "attachments": null
    },
    {
        "author": "jtraglia",
        "category": "general",
        "parent": "",
        "content": "Oh I like this. A few questions/comments...\n* After an increase, would it be possible to lower the max blob count if we observe a long-lasting issue?\n* Who exactly decides whether or not to delay a scheduled upgrade? What if the majority of large staking entities decide they want to keep the original schedule and refuse to update?\n* How long is the observation period before the deciding the keep the current schedule or define an updated one? If we decide to modify the schedule, I imagine we should give stakers at least a few weeks to upgrade. So the decision must be made within ~1 month of the last increase?\n* Currently, we've defined [`MAX_BLOBS_PER_BLOCK_FULU=12`](https://github.com/ethereum/consensus-specs/blob/dev/specs/fulu/beacon-chain.md#execution). This proposal starts with the Pectra limits. I actually prefer this idea, rather than an immediate increase. Just a reminder that we'll need to delete this config value.\n* And maybe obvious but worth mentioning anyway. After an increase, if there isn't enough blob demand to adequately test the new change, we can/should submit large blob transactions ourselves for some sustained period of time. For example, one week after the increase there could be a one hour test of sustained max blob usage.",
        "created_at": "2025-03-27T20:39:18.955000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "I don't like the numbers that are being thrown for blob counts, however, modulo this I definitely like and support the idea of programatically scheduling the increase. The problem I see is that we won't really know if the increase was succesful, we would only know that in the happy case we can continue increasing. My biggest problem with blob scaling is that the failure mode is what gets hurt with having larger numbers. And programatically increasing will not catch these failure modes unless we, well, fail. Still at least this gives us a way to back up even if we fail at lower counts so it's strictly better than directly bumping count to 48/72. What would be good is to see if something like Holesky can survive 2 weeks of non-finality with 48 blobs not only target but actual average.",
        "created_at": "2025-03-27T20:44:24.317000+00:00",
        "attachments": null
    },
    {
        "author": "ralexstokes",
        "category": "general",
        "parent": "",
        "content": "yeah, not attached to these specific numbers; hope to first build consensus around _some_ type of iterative rollout\n\ni assume there will be many more discussions and data from devnets to help inform the exact shape of the rollout",
        "created_at": "2025-03-27T20:45:36.615000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "We've talked informally within Prysm about a rollout like this and unless I'm mistaken we all agreed it was a good idea. Moreover if we shiped BPOs with Fusaka we should be good for other situations as well. \u003c@308223150725005322\u003e noticed that we should actually tests with BPOs for blob count reduction as well",
        "created_at": "2025-03-27T20:47:04.348000+00:00",
        "attachments": null
    },
    {
        "author": "ralexstokes",
        "category": "general",
        "parent": "",
        "content": "\u003e After an increase, would it be possible to lower the max blob count if we observe a long-lasting issue?\nfollowing something like EIP-7892, we could adjust the config to a lower number at a given point in time\n\nbut it would essentially be a hard coordination point, so something to weigh in mind wrt how aggressive we want the shape of the rollout to look",
        "created_at": "2025-03-27T20:47:32.448000+00:00",
        "attachments": null
    },
    {
        "author": "ralexstokes",
        "category": "general",
        "parent": "",
        "content": "\u003e Who exactly decides whether or not to delay a scheduled upgrade? What if the majority of large staking entities decide they want to keep the original schedule and refuse to update?\neveryone who runs a node -- the model is essentially just like a hard fork\n\nin this case, i would expect a majority of core devs advocating for a change would be well recieved",
        "created_at": "2025-03-27T20:48:24.195000+00:00",
        "attachments": null
    },
    {
        "author": "philngo",
        "category": "general",
        "parent": "",
        "content": "Nice, we were going to release our Fusaka blog post tomorrow to pretty much say the same thing. Fusaka should only be PeerDAS and BPO fork with some debate on the scheduling timelines of each 2x increase. Even better if we include some sort of a plan for how to measure safety/performance with the mainnet data to justify sticking with the scheduling we initially bake in.",
        "created_at": "2025-03-27T21:03:54.437000+00:00",
        "attachments": null
    }
]