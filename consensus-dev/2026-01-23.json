[
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "snappy, like all other similar compressors, use hash functions and a buffer/window to find matches - both the hash function used and the window size affects compression ratio (and speed of compression) - when you give a \"level\" argument to gzip or whatever, you're changing these parameters among other things, but they all result in an equally valid \"compressed\" output - in fact, in the protocol today it's possible to write a client that always creates a \"compressed\" payload that is larger than the SSZ encoding, and if clients don't recompress, that _uncompressed_ message will propagate... this is the worst case we need to account for, always",
        "created_at": "2026-01-23T14:33:46.145000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "there exists an upper bound on how much bigger the \"maximum compressed size is\" in an \"honest\" compressor - it's trivial actually, it's the uncompressed size + a few bytes of header to tell the decompressor that the block that follows is uncompressed... ever honest compressor will switch to \"no compression\" if the compressed size approaches the uncompressed size",
        "created_at": "2026-01-23T14:35:20.564000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "that said, you can craft a \"compressed\" version of a payload that a decompressor can decompress that is much much larger than the uncompressed data, simply by adding lots of headers - afair, somewhere we have a check for this but I don't remember where ðŸ˜‰",
        "created_at": "2026-01-23T14:36:22.768000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "ditto protobuf: when you encode things as protobuf, you can create a payload that is arbitrarily large, but the effective payload that the application sees is just a fraction of this (protobufs are unordered key-value streams where the last key overrides any previous versions of the same data - unknown keys are _ignored_ by protobuf parsers) - if you're doing validation only on \"decoded data\", you're neglecting the wire encoding in your application and opening up for DoS .. I'm kind of hoping every implementation is aware of this and has protection mechanisms against it, the easiest of which is to always _reencode_ the protobuf before sending since re-encoding gets rid of all the junk",
        "created_at": "2026-01-23T14:40:09.594000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "fwiw, this is part of the reason why we didn't want protobuf in the eth2 spec to begin with - it's pretty hard to reason about .. unfortunately, it's still part of libp2p and therefore part of the \"security surface\"",
        "created_at": "2026-01-23T14:41:18.876000+00:00",
        "attachments": null
    },
    {
        "author": "jtraglia",
        "category": "general",
        "parent": "",
        "content": "Consensus devs, please review this PR: https://github.com/ethereum/consensus-specs/pull/4814",
        "created_at": "2026-01-23T21:02:06.636000+00:00",
        "attachments": null
    },
    {
        "author": "jtraglia",
        "category": "general",
        "parent": "",
        "content": "cc \u003c@412614104222531604\u003e this is related to config clean up.",
        "created_at": "2026-01-23T21:02:29.630000+00:00",
        "attachments": null
    }
]