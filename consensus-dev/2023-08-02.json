[
    {
        "author": "m.kalinin",
        "category": "general",
        "parent": "",
        "content": "\u003c@777935002263617576\u003e on the latest call were you mentioning that your optimised version isn't run against test vectors or did I misunderstand you?",
        "created_at": "2023-08-02T14:45:12.862000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "general",
        "parent": "",
        "content": "No, I maybe wasn't clear. I wrote a bit about it https://twitter.com/sauliuseth/status/1684603068900413440 . Grandine currently has a single optimized version. It passes test vectors just like other CL clients. However, none of the clients AFAIK are spec compliant, for example, those implemented DoS protections are not in the spec. Weird attestations are ignored by clients but it should be not ignored according the spec. However, this means that clients implement something between what is specified by specification and what is covered by test vectors. I personally think that it's OK. It's not realistic to make a performant client that is fully spec compliant and client diversity should ensure that slightly different implementations survive the network because those parts that are not covered by test vectors likely are implemented differently (at least some of the implementations would not be vulnerable to the attack). \n\nI raised this question during the call because this defacto situation brings us a huge engineering issue. The problem is that the scope that test vectors cover changes over time. So we have an optimized implementation that passes test vectors now, however some new test vector can be released that invalidates the optimizations we have. Generally, it should not be an issue because the client should be spec compliant so a new test vector should just pass. But in reality, clients are optimized and they are not fully spec-compliant, so they mainly target test vectors. So in order to avoid those huge refactorings we were thinking that we should have two modes - the optimized mode that passes test vectors and the not-so-fast version that also passes test vectors and is more compliant with the spec. However, after thinking a bit more we probably will stick to a single version that tries to be as much spec compliant as possible, but it will execute any weird processing (like weird attestations) with very low priority.",
        "created_at": "2023-08-02T15:13:22.422000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "general",
        "parent": "",
        "content": "This also makes an interesting situation - people who make/release test vectors are actually the ones who define what is the defacto specification.",
        "created_at": "2023-08-02T15:14:57.445000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "general",
        "parent": "",
        "content": "THanks a lot for the clarification! It makes it clear to me now and I misunderstood you during the call",
        "created_at": "2023-08-02T15:26:31.243000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "general",
        "parent": "",
        "content": "Very likely it was an issue with my English ðŸ™‚ But this topic is really interesting in general. I think short term way to improve the situation is to carefully extend specification coverage by extending test vectors.",
        "created_at": "2023-08-02T17:10:45.375000+00:00",
        "attachments": null
    }
]