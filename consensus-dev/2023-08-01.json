[
    {
        "author": "ajsutton",
        "category": "general",
        "parent": "",
        "content": "If I were building a new consensus client these days I'd start by implementing the latest milestone and ignore all the tests for prior milestones. I believe the consensus tests duplicate tests for each milestone even if they aren't testing specific rules.  You can then use checkpoint sync to skip into the latest part of the chain and potentially never bother implementing the older milestones or at least defer them until you want to start supporting archive nodes and historic queries.",
        "created_at": "2023-08-01T00:12:45.195000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "general",
        "parent": "",
        "content": "I'm inclined to agree. Especially as the Bellatrix-start is already the baseline, and it looks like Capella-start might become the baseline in the near-to-medium-term future, at least phase0 and altair aren't that relevant except insofar as it might be useful to be able to run the state transition function. But this is only the beacon chain part of the spec, not the validator, p2p interface, etc, all of which are basically irrelevant pre-Bellatrix, and soon, pre-Capella",
        "created_at": "2023-08-01T00:37:54.065000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "general",
        "parent": "",
        "content": "I would also be tempted to try and get the most recent fork running, mainly so that you can start following the tip of the chain as soon as possible. The detail in CL implementation, IMO, is being efficient/optimised enough to follow the chain on a second-to-second basis. The consensus-specs as they appear are really not efficient enough for production (there's a bunch of O(n^2) stuff in there).\n\nI'd be concerned that starting at phase 0 may result in a client that is optimised for syncing the *history* of the chain, without paying heed to the optimistations necessary for following the *tip* of the chain.\n\nAs an example, some major challenges of CL clients are handling attestation verification load (not just the signature verification load, but the loading the context required to verify them (e.g. shuffling)) and being able to quickly process blocks from the network when those blocks might be built on arbitrary heads. Both of these challenges are either not present or substantially different when syncing the ancient history of the chain.",
        "created_at": "2023-08-01T00:56:00.459000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "Others have covered your question completely. I'd just add that spectests cover the \"easy\" part of building a CL. Following the formal specification, while not trivial, is very simple vs 1) doing it efficiently (as Paul already spent time describing) and 2) doing it securely. \n\nTo say a few words about 2: what p2p library will you use? Do you need to do it from scratch? Do you need to start worrying about DOS protection yourself or will use use an existing library. If it's the former you'll need a large team. Most importantly: it's very different to run a client that can follow the chain with no forks and 99% , than a client that will not break nor miss a single proposal if there are reorgs and jumping contending justified checkpoints with entire chains being suddenly visible after a network split. One way I like to think about this is that if one is selfish it should be relatively simple to build a client that outperforms all major clients by compromising on being resilient in adverse situations. After all, if only a few people run your client then you can't derail the network and eventually forkchoice will make you follow and vote/propose for the right head. \n\nSo spectests are a great great tool that has (at least in my case) saved my butt more than a few times. But it's only in the fact that they test correctness in the fully running binary, with all the caches already hot and all the parallel routines wanting to run into a data race",
        "created_at": "2023-08-01T01:31:36.355000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "general",
        "parent": "",
        "content": "By starting from earlier hardforks you will build your code structure that supports multiple hardforks. Usually, it's not a trivial thing.",
        "created_at": "2023-08-01T13:47:08.701000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "general",
        "parent": "",
        "content": "I'm also not sure that I agree that it's not the right way to optimize for historical chain syncing. It's actually a very convenient thing to benchmark. Often if historical chain syncing is optimized you only need to insert some pre-processing (like slot/epoch pre-processing) for chain-tip performance. At least that's the case if you use tree states.",
        "created_at": "2023-08-01T14:01:19.311000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "general",
        "parent": "",
        "content": "Passing test vectors is only a relatively small part. It really helps for some class of regressions, but on the testnets you more likely will benefit a lot from some comparative performance/behavior analysis with other clients, because on those funky networks, it's often hard to tell if your problem is because your client is broken or because the network barely functions. But that's the step after the test vectors pass.",
        "created_at": "2023-08-01T14:06:57.053000+00:00",
        "attachments": null
    }
]