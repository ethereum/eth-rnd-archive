[
    {
        "author": "tersec",
        "category": "general",
        "parent": "",
        "content": "i.e. for those for whom it's easier to never have to fall back to caring about `MAX_BLOB_PER_BLOCK{_FORKNAME}`, that's a viable implementation choice",
        "created_at": "2025-05-26T00:00:07.661000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "general",
        "parent": "",
        "content": "Anyway, mostly would like to echo",
        "created_at": "2025-05-26T00:06:39.465000+00:00",
        "attachments": null
    },
    {
        "author": "raulvk",
        "category": "general",
        "parent": "",
        "content": "\u003c@654267572107083777\u003e pushed a version with commented backports",
        "created_at": "2025-05-26T00:11:06.780000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "general",
        "parent": "",
        "content": "ok, that resolves that particular point for me",
        "created_at": "2025-05-26T00:17:45.423000+00:00",
        "attachments": null
    },
    {
        "author": "raulvk",
        "category": "general",
        "parent": "",
        "content": "i wasn’t able to find the place where Geth validates the max blob count in the block. is it specified somewhere that the EL performs the check? (looked in the Engine API spec too)\n\nhttps://github.com/search?q=repo%3Aethereum/go-ethereum%20MaxBlobsPerBlock\u0026type=code",
        "created_at": "2025-05-26T00:50:40.071000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "With blob verification @ 6/9 taking 22% of the txpool time (vs every other tx); do we have any eastimates how that will change with PeerDAS and the additional cell verification and higher blob counts?",
        "created_at": "2025-05-26T00:57:24.254000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "c5051ca2d4f5f28c65515b6051bc529f0d24875fa59f765c63b9a5056711f609"
            }
        ]
    },
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "Justin Traglia's previous analysis indicated 15x more compute required per blob, this is the red line in the figure here: https://notes.ethereum.org/@anderselowsson/EIP-7918E So if we scale blobs by 8x it would mean 0.22 * 15 * 8 / (0.22 * 15 * 8 + 0.78)=97% of all compute..?",
        "created_at": "2025-05-26T01:10:23.498000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "I wonder if this has implications for the repricing. Would this not put the price of 1 blob under peerDAS at 750k gas?",
        "created_at": "2025-05-26T01:16:40.550000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "This definetely has implications. If this is the case, you need to price blobs in a sane way to account for compute",
        "created_at": "2025-05-26T01:18:26.993000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "general",
        "parent": "",
        "content": "is it still going to be possible for a staker to (per previous ethresear.ch post from Dencun) earn 97-98% of profit by excluding blobs entirely? At some point the incentive mismatch/misalignment becomes a stability hazard",
        "created_at": "2025-05-26T01:18:37.726000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "general",
        "parent": "",
        "content": "and is it different depending on whether one can do MEV-like searching or more ordinary, say, local Nethermind EL-like searching?",
        "created_at": "2025-05-26T01:19:24.844000+00:00",
        "attachments": null
    },
    {
        "author": ".paulharris",
        "category": "general",
        "parent": "",
        "content": "theres no incentive for blobs at all",
        "created_at": "2025-05-26T01:19:28.714000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "general",
        "parent": "",
        "content": "right, so I'm looking at this as, well, a rational EL might just have an option to ignore blob tx's entirely as much as feasible",
        "created_at": "2025-05-26T01:20:05.090000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "I think all ELs reject blobs from txpool if they don't have at least a 1 gwei priority fee; but is only applied to the 21k min tx gas. So not a lot compared to other txs",
        "created_at": "2025-05-26T01:21:12.299000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "But this an execution cost",
        "created_at": "2025-05-26T01:21:36.412000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "PeerDAS blobs need to be properly priced if they end up taking 97% of the txpool",
        "created_at": "2025-05-26T01:22:03.660000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "But if I am not mistaken, considering that it is 15x more expensive computationally wise, ig now blobs should cost a min of 500k-750k EL gas?",
        "created_at": "2025-05-26T01:23:06.139000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "Cc \u003c@917367578966523934\u003e",
        "created_at": "2025-05-26T01:23:28.276000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "I think that if it was on the hotpath, it should be 750k but txPool is not hotpath so maybe a little discount? (500k?)",
        "created_at": "2025-05-26T01:26:47.749000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "anyway if it is 97% of the txpool 50k is too little",
        "created_at": "2025-05-26T01:29:35.894000+00:00",
        "attachments": null
    },
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "I think this would be rather drastic. I imagine we go for a lower level for Fusaka, i.e., in the range of asymptotic peak at high blob counts for full batching of all cells in DAS (8 columns). This would be perhaps 25k or so. I was considering 2^13=8192 or 2^14=16384 for devnet1, but we can discuss it further",
        "created_at": "2025-05-26T01:30:16.847000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "I mean, if it is 97% this is not drastic",
        "created_at": "2025-05-26T01:30:31.765000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "97% is drastic on the txpool",
        "created_at": "2025-05-26T01:30:39.928000+00:00",
        "attachments": null
    },
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "Ah sorry I was responding to 750k as drastic",
        "created_at": "2025-05-26T01:30:57.857000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "97% is essentially a DDOS",
        "created_at": "2025-05-26T01:30:59.495000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "yes ok - it might be drastic. but the price cannot be 50k then",
        "created_at": "2025-05-26T01:31:19.451000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "750k is kind of just 15x 50k. as i said, txpool is not hotpath so you can kind of give it a discount",
        "created_at": "2025-05-26T01:31:42.263000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "so maybe beetwen 300k-500k?",
        "created_at": "2025-05-26T01:32:07.064000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "I mean you can price them at 50k but then you will not be ablie to go higher than X amount of blobs (where X is probably lower than the promised 48. probably will be a target of 12 max)",
        "created_at": "2025-05-26T01:33:57.518000+00:00",
        "attachments": null
    },
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "400k means the reserve price (minimum) for the blob base fee would be 3x the execution base fee. I think we want to do something rather moderate for Fusaka, in the range 8192-16384. But as mentioned, it is generally an important discussion.",
        "created_at": "2025-05-26T01:38:49.218000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Context the execution base fee is same as just a basic Eth transfer tx; x3 would be about same as an ERC20 transfer",
        "created_at": "2025-05-26T01:41:46.201000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "I think we need to establish the meaning of moderate",
        "created_at": "2025-05-26T01:44:01.114000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "because of one end 400k sounds high. on the other, \"moderate\" is a DDOS vector",
        "created_at": "2025-05-26T01:44:23.680000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "400k is just a number btw. you need to treat it as such. it might sound/look big but the alternative is actually worse. I suggest to look at this with a relative perspective rather than an absolute one",
        "created_at": "2025-05-26T01:45:23.118000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "I agree, this is kind of a mismatch here. because ERC20 transfers do not ddos the pool and I mean, you can go with a low price but then you should limit blobs per block to a more sane amount than 48",
        "created_at": "2025-05-26T01:46:37.080000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "you can also just recognize that after 12 blobs/block, the mempool is the bottleneck instead",
        "created_at": "2025-05-26T01:47:08.518000+00:00",
        "attachments": null
    },
    {
        "author": "terencechain",
        "category": "general",
        "parent": "",
        "content": "Beacon state contains of fork version, this is used for signature domain for example\nFor BPO, we dont want to mess with that, that's basically a hard fork if we change it\nBut it seems to be, by adding fork version to  blob schedule, we introduce a second variance of fork version, which may be confusing\nthere's one type of version that is used for signature signing, another type of version that used for enr",
        "created_at": "2025-05-26T03:09:30.377000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "general",
        "parent": "",
        "content": "1. We can ignore low price txs before verification, 2. we can limit blobs/tx, 3. think about some more efficient proof, like a zk circuit specially to confirm this data is ok?",
        "created_at": "2025-05-26T08:08:05.628000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "general",
        "parent": "",
        "content": "\u003c@316280621783580673\u003e  we also probably use just 1 thread for this, so can be potentially 4x faster",
        "created_at": "2025-05-26T08:09:14.969000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "general",
        "parent": "",
        "content": "to save on networking we can request without wrapper or extend light data passed about txs",
        "created_at": "2025-05-26T08:10:54.626000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "general",
        "parent": "",
        "content": "Consider also rn blob tx pool is almost empty comparing to regular one and is designed for way(?) bigger load. That's why we have devnet heh",
        "created_at": "2025-05-26T08:12:56.610000+00:00",
        "attachments": null
    },
    {
        "author": "raulvk",
        "category": "general",
        "parent": "",
        "content": "I guess that’s the point. BPO hardforks _are_ hardforks. With the only caveat that they don’t require code changes to roll out.",
        "created_at": "2025-05-26T08:15:01.772000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Is using 20% txpool of CPU; even though mostly empty",
        "created_at": "2025-05-26T08:15:52.506000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "Charging the cost as if this was a verification happening on chain doesn't make much sense to me, for two reasons:\n- As it has been said already, it's not in the critical path, so the timing constraints are quite different (the problem isn't \"if the whole block was repeating this operation over and over, execution the block would take time X\", as with underpriced opcodes). \n- block execution isn't parallelized (yet), but this can be",
        "created_at": "2025-05-26T08:19:32.727000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Blob txs are materially different",
        "created_at": "2025-05-26T08:19:33.227000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "general",
        "parent": "",
        "content": "Hm, maybe in case of heavy load we can prioritize txs and throttle verification to spend no more than 50%, ignore the lower ones",
        "created_at": "2025-05-26T08:19:55.331000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "general",
        "parent": "",
        "content": "Verify more lazily",
        "created_at": "2025-05-26T08:20:51.159000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Every validator is repeating the txpool operations; the blobs are soaking up network bandwidth and node cpu regardless of whether part of block validation/production critical path",
        "created_at": "2025-05-26T08:22:26.239000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "SSTORE also isn't part of critcal path",
        "created_at": "2025-05-26T08:22:38.344000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "Most of the price of SSTORE is about state growth, not execution time, no?",
        "created_at": "2025-05-26T08:23:06.047000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "I think it's fine to charge something more in line with the load, it's 100% underpriced (or rather, the compute is essentially not priced at all). Just don't think it should be treated as if the proof verification for each blob was happening sequentially during block execution",
        "created_at": "2025-05-26T08:24:21.568000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "As for the CL part of it, it's also a bit nuanced because there's batching (plus also parallelizable)",
        "created_at": "2025-05-26T08:24:57.415000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "2900 gas when no growth (non zero to non zero); 32 bytes",
        "created_at": "2025-05-26T08:25:00.779000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "I guess most importantly, there's sharding, so it's not actually the case that each proof is verified by each validator",
        "created_at": "2025-05-26T08:26:44.914000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Ok but are we charging for block production; or block verification?\n\nIf its verification; then do all opcodes go to zero when its just zk?\n\nIf its production then full blob proof verification count (as per spec) as they won't otherwise be included",
        "created_at": "2025-05-26T08:26:55.777000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "If the block producer was the one producing the proof, it's at least an option for the compute opcodes, though probably not what will be done/we should do",
        "created_at": "2025-05-26T08:29:05.711000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "By this line of argument, parallelization of verification through access lists is useless, because it doesn't help the builder",
        "created_at": "2025-05-26T08:29:33.747000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "parallelization is an implementation detail option; doesn't change prices?",
        "created_at": "2025-05-26T08:31:44.973000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Unless you mean current accesslists; which no one really uses as price change if you get one value wrong (not used) you need to get 25 right to cover the additional cost of sending it",
        "created_at": "2025-05-26T08:33:03.803000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "To me the point of parallelization is being able to do more execution, e.g. increasing the gas limit. So we're saying that in a world with parallelization, we treat each unit of compute as cheaper",
        "created_at": "2025-05-26T08:33:07.595000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "No I mean parallelization with BALs for example",
        "created_at": "2025-05-26T08:33:30.391000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "If we're not (possibly) doing it to increase the gas limit, idk why else",
        "created_at": "2025-05-26T08:33:47.462000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "We set the fee unit and capacity, then market decides the price per fee unit. Atm we are setting blob fee unit as 1 wei (market then sets price of that)",
        "created_at": "2025-05-26T08:35:55.825000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Which is sorta where problem is; since 1 wei is to support the decimals on a fixed integer; not something that should be use as main, since are no decimals lower than 1 wei",
        "created_at": "2025-05-26T08:37:53.717000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "I agree that we are at the moment not pricing computation related to blobs at all, and it's ok to change that. But treating the computation as if it's happening in the block is just not a very principled approach in my opinion",
        "created_at": "2025-05-26T08:38:24.711000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "So is just free compute as not happening in block? It is a MUST in spec before including blobs in block (even if not critical path)",
        "created_at": "2025-05-26T08:42:06.499000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "",
        "created_at": "2025-05-26T08:42:47.042000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "6b0f1ab78df55a2c61c1ef64d7b91ac7265f4192e2817f8820f6bc5d6c519419"
            }
        ]
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Doesn't specify that its the txpool that should be doing it; so could be interpreted as during block building. Mempool validation makes sense as why hold a tx that can't run",
        "created_at": "2025-05-26T08:43:54.339000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "Where did I say that? I just said I agree we're underpricing that and we should fix it",
        "created_at": "2025-05-26T08:45:05.594000+00:00",
        "attachments": null
    },
    {
        "author": "ansgar.eth",
        "category": "general",
        "parent": "",
        "content": "my understanding is that all EL clients will have a flag to limit txpool blob tx throughput for Fusaka, no? This was mentioned repeatedly as a protection mechanism for weak nodes. Once such a flag exists, there is no issue here?",
        "created_at": "2025-05-26T09:52:42.184000+00:00",
        "attachments": null
    },
    {
        "author": "ansgar.eth",
        "category": "general",
        "parent": "",
        "content": "onchain gas was never meant to acocunt for txpool load, that has been true for all other types of transactions as well. If blob txs indeed are causing such heavy load, then we should revisit the startegy of relying on the public tx pool for their propagation",
        "created_at": "2025-05-26T09:53:53.957000+00:00",
        "attachments": null
    },
    {
        "author": "ansgar.eth",
        "category": "general",
        "parent": "",
        "content": "my understanding is also that ELs are planning to move to sharded tx pools for blob txs in the future anyway, specifically because of bandwidht \u0026 compute loads of these transactions, no?",
        "created_at": "2025-05-26T09:54:48.201000+00:00",
        "attachments": null
    },
    {
        "author": "ansgar.eth",
        "category": "general",
        "parent": "",
        "content": "also, side note, if we really plan to 1000x EL throughput in general (including via zkEVM) over the next ~4 years, the same issue will appear for normal txs",
        "created_at": "2025-05-26T09:55:29.674000+00:00",
        "attachments": null
    },
    {
        "author": "ansgar.eth",
        "category": "general",
        "parent": "",
        "content": "either way, onchain gas is not the instrument to use to limit txpool load. never was, can't be going forward, so not useful to consider for this situation either",
        "created_at": "2025-05-26T09:56:27.844000+00:00",
        "attachments": null
    },
    {
        "author": "ansgar.eth",
        "category": "general",
        "parent": "",
        "content": "the nice thing is, with EIP-7918, even if the public txpool won't support throughput levels of `TARGET_BLOB_PER_BLOCK`, the blob basefee has a reasonable floor, and so can't collapse down to 1 wei.",
        "created_at": "2025-05-26T10:00:49.834000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "I would like to see this distinction made clear in all conversations about gas pricing and limit changes.  Does the given pricing/limit exist to protect block builders, or does it exist to protect everyone who is validating the chain?",
        "created_at": "2025-05-26T10:04:37.986000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "The issue isn't the load perse; the issue is mispricing compared to everything else (expectation with normal txs is they eventually pay their gas; which covers any load)",
        "created_at": "2025-05-26T10:20:43.503000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "This kind of have nothing to do with the builder. Everyone runs the txpool and validate blocks the blob flag has nothing to do with this. If computation is 15x post-fusaka then we should charge 15x for the execution in principle. Parallelization is an impl detail here and we dont consider it fpr pricing",
        "created_at": "2025-05-26T11:12:47.918000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "Referring to this",
        "created_at": "2025-05-26T11:15:37.400000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "\u003e validate blocks\nNot every node validates every proof as part of block-related checks (column validation), because of sharding. For example, verifying 8 columns with  64 blobs takes ~30 ms here https://gist.github.com/jtraglia/698a4f7bd43764db19753f9fa046998e, so \u003c 0.5ms per blob. Even without batching across columns, it's ~72ms total, so just over 1ms per blob, in line with the current BlobKzgProof verification\n\n\u003e Everyone runs the txpool\nFor one thing, txs don't *need* to go through the tx pool. If we can only support a smaller tx pool than the blob throughput we can support, we should imo limit the tx pool, not make txs much more expensive or limit the throughput. And if we want to keep having a large tx pool, we should eventually have sharding there as well. Making blob txs much more expensive while we're in the awkward middle of having sharding on the CL but not on the EL is quite possibly a bad strategic decision imo. \n\n\u003e Parallelization is an impl detail here and we dont consider it fpr pricing\nI don't think that's true. Parallelizable compute is a much more abundant resource than sequential compute, how can that not impact pricing? If blocks were always fully parallelizable, would we not increase the gas limit?",
        "created_at": "2025-05-26T11:48:50.614000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "\u003e For example, verifying 8 columns with  64 blobs takes ~30 ms here https://gist.github.com/jtraglia/698a4f7bd43764db19753f9fa046998e, so \u003c 0.5ms per blob. Even without batching across columns, it's ~72ms total, so just over 1ms per blob\n\nSo longer than it takes to run a full 36M L1 block with 355 txs (64ms)\n\n\u003e , in line with the current BlobKzgProof verification\n\nWhich if you run in a smart contract costs way higher than it does as part of blobs (and is also underpriced)\n\n\u003e If blocks were always fully parallelizable, would we not increase the gas limit?\n\nMixing concepts, that's increasing supply; much like BPO forks are changing supply\n\nIt however is not changing tx; opcode or precompile gas. In fact one of the precompiles in Pectra was repriced as the original pricing was for the parallel time not serial",
        "created_at": "2025-05-26T12:05:55.267000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Let's put some hard comparisons here; is a min price for a blob at the price an L1 user would pay for a single ERC20: Transfer too high? (Or should it cost more to do an L1 ERC20 transfer than post a 128kB blob) If so why?",
        "created_at": "2025-05-26T12:13:25.051000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "Assuming the ERC-20 transfer moves the asset between two accounts that already have some of that token (so no net increase in state size)?",
        "created_at": "2025-05-26T12:49:32.977000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "About the networking component of it specifically, historically all pricing has been congestion pricing. The protocol provides some resources and a price only exists to allocate them if they are scarce.  In that sense, there's no way to say whether an ERC20 transfer *should* cost more or less than 128KBs of data, because they're just different resources (there's of course a bit more to the story there, since calldata is a similar resource. But lots to say there, re: history expiry, blobs being sent on separate subnets, and of course soon sharding)\n\nAbout the computation side of it, in my opinion a principled approach to pricing would only look at the verification cost of blobs that end up in a block, i.e. the CL verification cost, and in particular the cost to the weakest nodes, which are doing the minimum amount of work (verifying 8 columns). It would also take into account the fact that this compute is parallelizable, by making it relatively cheaper than other compute in the block, by some factor to be agreed upon based on the minimum node spec (same factor by which we would be ok with making compute cheaper, in gas terms relative to things like calldata or state growth that don't benefit from parallelization,  after implementing BALs). I don't know what this means concretely, given that compute opcodes vary quite a lot in how they are priced relative to the actual execution time. If we were to do this, I am sure it would mean that blob txs should be charged a lot more than the current flat 21k fee for compute, I don't know how much exactly though and I think it probably wouldn't be the kind of numbers that were said above (750k gas)",
        "created_at": "2025-05-26T12:59:00.360000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "4844's POINT_PRECOMPILE gas price is a fair reference price though it is lower than PeerDAS which does additional will cell verification on top",
        "created_at": "2025-05-26T13:01:51.614000+00:00",
        "attachments": null
    },
    {
        "author": "ethdreamer",
        "category": "general",
        "parent": "",
        "content": "With the concerns raised here, I would maybe opt for explicitly specifying that the BLOB_SCHEDULE controls the blob parameters *after* `ELECTRA_FORK_EPOCH` and *then* requiring that all capacity increases in future full hard-forks be specified/consistent in the blob schedule.",
        "created_at": "2025-05-26T13:02:29.357000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Saying that I think `EIP-7918: Blob base fee bounded by execution cost` is the way to go; as we are mostly just arguing about what that cost is",
        "created_at": "2025-05-26T13:04:51.683000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "Assuming that's correctly priced at 50k gas, as I mentioned before the *CL verification cost* actually stays about the same as one BlobKzgProof verification per blob, due to sharding (+ a bit of help from batch verification which gets better per blob as we have more blobs, but not much). So from that point of view the price  *per blob* should maybe be 50k gas divided by some parallelization factor\n\n(Again this assumes we're not trying to charge for mempool verification, only for block verification, which imo is the right thing to do here)",
        "created_at": "2025-05-26T13:05:14.459000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "I think it would even be fine to charge 50k gas fwiw, at the end of the day the cost *per L2 tx* would only cross 1c at about 80 gwei. Just think it would be quite the mistake (and not just strategically, also in terms of pricing principles) to say that we should charge 15x that",
        "created_at": "2025-05-26T13:07:46.183000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "(Also maybe this whole discussion should have been in execution-dev)",
        "created_at": "2025-05-26T13:08:15.405000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Also needs a cap on blobs per tx; as no tx pools are set up to have a variance between 1 and 72 (128kB to 9MiB) txs; as the heuristics on what txs to keep in pool are based on count; if they can be 9MiB txs that no longer works by itself.\n\nCapping blobs per tx removes the need for some hard compsci and opinionated valuing of tx vs size. Especially since there is no validator incentive attached to blobs directly (100% burn; priority fees only on the 21k)",
        "created_at": "2025-05-26T13:10:20.111000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "I very much support a (even small) cap on blobs per tx, at least in the tx pool, but I'd preferably just do it in protocol",
        "created_at": "2025-05-26T13:14:58.837000+00:00",
        "attachments": null
    },
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "I extended the analysis from the previously shared note up to 1024 blobs to illustrate my point of asymptotically approaching around 25k gas. This is indicated by the grey dashed line. So when we consider DAS—an integral part of consensus—the weaker nodes that sample 8 columns would when batching all columns jointly asymptotically approach around 25k per blob as the blob count rises (brown dashed line). Then we might wish to consider some level of parallelization, but on the other hand, the point evaluation is generally parallelizable as well. I think this is a good starting point for the conversation, which may then settle slightly above or below. Test code itself could be reviewed more closely and it could be run on more machines, etc.",
        "created_at": "2025-05-26T13:46:46.918000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "kgzPlot1024.png",
                "content": "d612e10d599248ea9982b47595a41916f1fab7e07dca8769fc9e1856d7b0709b"
            }
        ]
    },
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "Of course, if we go as high in terms of blob count as indicated here, we would likely do things a bit differently anyway..",
        "created_at": "2025-05-26T13:53:30.678000+00:00",
        "attachments": null
    },
    {
        "author": "terencechain",
        "category": "general",
        "parent": "",
        "content": "I guess that’s the point. BPO hardforks",
        "created_at": "2025-05-26T13:56:03.061000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "\u003e  Then we might wish to consider some level of parallelization, but on the other hand, the point evaluation is generally parallelizable as well\nNot an expert, but my understanding is that the verification of a *single proof* (which would be the relevant thing for pricing of the precompile) is already optimized as much as it can be more or less \u003c@427491045308235776\u003e \u003c@862973562243252234\u003e. Main thing being doing a single multi-pairing instead of two separate pairings",
        "created_at": "2025-05-26T14:00:03.239000+00:00",
        "attachments": null
    },
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "I meant in terms of those txs generally being parallelizable",
        "created_at": "2025-05-26T14:03:03.144000+00:00",
        "attachments": null
    },
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "Not depending on other txs in the same block",
        "created_at": "2025-05-26T14:03:48.724000+00:00",
        "attachments": null
    },
    {
        "author": "kevaundray",
        "category": "general",
        "parent": "",
        "content": "Yeah this is correct -- we have a PR that squeezes out about 10-20% more, but I don't see it getting better than that",
        "created_at": "2025-05-26T14:04:04.144000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "A call to the precompile could happen at any time, it's not declared upfront",
        "created_at": "2025-05-26T14:05:16.246000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "really nice figure! I think it should help to ground the discussion",
        "created_at": "2025-05-26T14:06:27.272000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "\u003c@427491045308235776\u003e is the single proof verification parallelized already?",
        "created_at": "2025-05-26T14:21:24.951000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "If yes you cannot parralelize it further",
        "created_at": "2025-05-26T14:21:40.343000+00:00",
        "attachments": null
    },
    {
        "author": "kevaundray",
        "category": "general",
        "parent": "",
        "content": "To clarify, \u003c@520034910149410861\u003e was talking about the point eval precompile which is ran on a single thread",
        "created_at": "2025-05-26T14:27:05.340000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "yeah i am trying to figure if single proof computation's internals can be parallelized/are parallelized",
        "created_at": "2025-05-26T14:27:36.352000+00:00",
        "attachments": null
    },
    {
        "author": "kevaundray",
        "category": "general",
        "parent": "",
        "content": "Ah, I don't think you gain much from trying to parallize it internally. Most of the the internals consists of pairings and independent scalar multiplications",
        "created_at": "2025-05-26T14:30:35.156000+00:00",
        "attachments": null
    },
    {
        "author": "kevaundray",
        "category": "general",
        "parent": "",
        "content": "You _can_ gain something if you verify them(proofs) in a batch",
        "created_at": "2025-05-26T14:31:51.894000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "anyway I see it this way:\n\n- post-Fusaka blobs are definetely more expensive than pre-Fusaka's.\n    - they are a DDOS for mempool\n    - very expensive for hotpath as well.\n\nsaid that, if we were to price them with regards to the mempool they would be 750k gas/blob, with regards to hot path, it is still expensive with regards to Erigon at least because it would be 5-10x slower than an average tx",
        "created_at": "2025-05-26T14:32:15.812000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "so you need to price them in a range beetwen 50k-750k regardless. parallelism can help but we do not price things based on parallelism generally speaking. this would be an exception, not saying I am against, just it is an exception",
        "created_at": "2025-05-26T14:33:43.555000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "anyway you definetely should not price something like this at 50k imo. this is not a matter of being \"drastic\", it is a matter of not DDOSing the chain. we could also add limit to txpool and force the blob limit to something lower than the protocol but that seems suboptimal",
        "created_at": "2025-05-26T14:36:06.444000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "i guess if you want to price them with parallelism in mind, you can do something like `750k/8 cores=93.7k or 100k to round it`",
        "created_at": "2025-05-26T14:41:20.847000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "I do think we should price with parallelism in mind. I don't think we should price based on mempool verification. If we are concerned about the amount of load on the mempool, it would imo make a lot more sense to require mempool txs to be more expensive in terms of gas price or blob gas price (set an appropriate minimum price only for the mempool), or to have strict mempool limits (or at least allow nodes to locally set them), and let blob demand beyond that go out of the mempool if needed. Basically, use one of the two levers we have, price or quantity, but *only in the mempool*. Why should we charge txs for mempool verification when they might not even go through the mempool?",
        "created_at": "2025-05-26T14:57:32.072000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "Of course it's possible/likely that the consequence of that would be that the blob mempool is used a lot less and becomes mostly a vehicle for censored txs that are willing to pay a premium for inclusion, should there be any. We should consider whether we want this, versus (for example) biting the bullet for now if possible (depending on mempool performance in practice), and then improving the situation later with some form of EL sharding (there's at least some quite minimal things we can go there).",
        "created_at": "2025-05-26T15:01:33.816000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "I think it's possible that this is entirely ok, and maybe even desirable, but it's something to consider quite carefully because it might be something that's sticky. However in my opinion it is likely better, and surely more principled, than the alternative, which is to make all txs much more expensive regardless of whether they use the mempool or not. 750k would mean that at 20 blobs target we use 15M gas on average just for blobs",
        "created_at": "2025-05-26T15:03:33.100000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "I think this may be better to do in protocol otherwise some weird market dynamics may appear in L2s submitting privately to builders.",
        "created_at": "2025-05-26T16:08:32.414000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "although I suspect this very much screws up rollups like Arbitrum that have actual execution in their batch posting",
        "created_at": "2025-05-26T16:09:29.932000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "100k gas then?",
        "created_at": "2025-05-26T16:16:11.680000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "I think it is not an horrible price. 5x a normal eth transfer and napkin-math-wise it makes sense",
        "created_at": "2025-05-26T16:17:17.731000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "btw I am not saying it should 750k gas. However, i think it is the number you should reason with",
        "created_at": "2025-05-26T16:17:54.770000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "\u003e I think this may be better to do in protocol otherwise some weird market dynamics may appear in L2s submitting privately to builders.\nImo it's not obvious that this is worse than charging everyone a very high price for mempool load that they may or may not be causing, but very much depending on how high the price actually is. \n\n\u003e although I suspect this very much screws up rollups like Arbitrum that have actual execution in their batch posting\nWdym? Why would charging computation per blob be better or worse depending on whether you do your own computation with blob txs? \n\n\u003e 100k gas then?\nI think that's on the side of probably fine for both computational load considerations and in terms of not overcharging.",
        "created_at": "2025-05-26T16:48:27.372000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "general",
        "parent": "",
        "content": "About the computational load btw, the repricing EIP proposes 21k for the point evaluation precompile. Though that's in the context of many opcodes being dropped by more than that.",
        "created_at": "2025-05-26T16:49:25.506000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "then we just go to 50k like we originally intended too :)",
        "created_at": "2025-05-26T17:02:43.643000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "we just went back in circles lol",
        "created_at": "2025-05-26T17:02:52.112000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "not about computation per blob, but about restricting blob count per transaction to a low number. Some rollups post batches to an EOA, others post batches to a contract and the batch tx itself has computation. So amortizing many blobs in the same tx comes at a saving, while for rollups that only need the blob KZG-commitment on-chain without any execution on the batch it's almost the same to post N-tx with 1 blob or 1 tx with N blobs. \n\nAnyway it's not much of a difference and I do agree that this should be restricted, specially if we do increase the blob count to the astronomical numbers people are talking about.",
        "created_at": "2025-05-26T17:50:39.796000+00:00",
        "attachments": null
    },
    {
        "author": ".paulharris",
        "category": "general",
        "parent": "",
        "content": "its part of what they were talkig about for the EL schedule, but it may be how the gas scales up? i dont know how it all fits on the EL Side",
        "created_at": "2025-05-26T21:27:24.685000+00:00",
        "attachments": null
    }
]