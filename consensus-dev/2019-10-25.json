[
    {
        "author": "eth2_bridge_bot",
        "category": "general",
        "parent": "",
        "content": "**protolambda**\nYes it does fjl, which is why I'm giving handel-like tree heuristics on gossipsub a try. But translating it to an epidemic model is hard, as you need to drop smaller aggregations from the network in favor of bigger ones. Life times, global stepwise lifetimes, clusters, messages prioritization/speed were all discussed. But none of them seems to be a clear winner. Testing may help. And otherwise we have to research more structured but privacy-iffy approaches like Handel",
        "created_at": "2019-10-25T05:53:41.347000+00:00",
        "attachments": null
    },
    {
        "author": ".fjl",
        "category": "general",
        "parent": "",
        "content": "what I like about cost function + epidemic broadcast is that it can be combined with a dandelion-like anonymity layer.",
        "created_at": "2019-10-25T10:40:57.549000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "general",
        "parent": "",
        "content": "\u003c@600991921275535360\u003e made a basic model for aggregation via gossip, there was a naive cost functions for making an aggregate (aggregation is started from the heaviest attestation in the pool). And that model showed that in general gossip based aggregation strategies are promising. It's been evaluated in a mesh composed from the committee participants.\n\nWe don't have this option in a real system; we could try to build such kind of mesh but it doesn't seem to be viable. First, we need to decide on the mesh that could be used for gossip based aggregation. What options to we have here? Shard subnets, beacon_attestation, what else?\n\nAnother thing worth noting is that it's not necessary to build full aggregates before sending them to a proposer. We need to decide on a threshold enough to not blow up beacon_attestation topic with enormous amount of traffic. It will take less time for proposer to finish the aggregation than a for a mesh to do that.\n\nAll that needs further investigation and more rigorous modelling, of course.",
        "created_at": "2019-10-25T16:20:46.693000+00:00",
        "attachments": null
    },
    {
        "author": "eth2_bridge_bot",
        "category": "general",
        "parent": "",
        "content": "**protolambda**\nWorking on modelling. Primary concern is network load; producing aggregates on the fly for better information/bandwidth ratio is nice, but to drop the sources that form these aggregates before they reach the whole network (which would allow for local aggregation with less traffic) is difficult. Seeing the sources and aggregate outputs, and dropping the sources is difficult. Or you start optimistically dropping (i.e. drop small ones without seeing the bigger aggregate), but you get in a very gray liveness problem area.",
        "created_at": "2019-10-25T16:41:48.833000+00:00",
        "attachments": null
    },
    {
        "author": "eth2_bridge_bot",
        "category": "general",
        "parent": "",
        "content": "**protolambda**\nFor collecting the best aggregation we could mix some optimistic global net for big aggregates, and dynamically lower the threshold based on inclusion. And increase the reward for shards if they don't get included enough, so that proposers/others take the extra step to look for attestations outside of the optimistic global net.\nThis is just one idea, can be done differently too. Thought here is that inclusion just seems more solved than the epidemic aggregation part (with low network traffic).",
        "created_at": "2019-10-25T16:46:35.141000+00:00",
        "attachments": null
    }
]