[
    {
        "author": "m.kalinin",
        "category": "general",
        "parent": "",
        "content": "\u003e  Also note that in the current EIP records are not re-used after 1 year of withdrawable_epoch, so their effects will take a while to be noticed\n\nThis and the fact that reuse indexes doesn't aid network load reduction make me doubtful about this feature helping to alleviate risks related to validator set growth. \u003c@755590043632140352\u003e where do you think it can be helpful except for reducing state size and hashing? How much of a hashing cost it can save us?",
        "created_at": "2023-08-04T04:18:41.917000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "I think it bounds immediately the state: if I remember correctly Kraken alone exited about 40K vals starting in April, since the churn is the same for exits and deposits I think this change bounds the state at whatever size it will have by April for a long time. That gives us time to ship Max EB. We don't need to convince everyone to coalesce their validators, we just need to get a few large operators on board.",
        "created_at": "2023-08-04T05:28:32.975000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "general",
        "parent": "",
        "content": "I don't understand this logic.  Although the churn may be the same for exits and deposits, the actual number of deposits has far outweighed the number of exits.\n\nI agree that it will reduce the state growth for a while, but don't see that it creates any sort of realistic bound.",
        "created_at": "2023-08-04T06:31:01.379000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "I think it kills state growth until we can ship the real solution which is Max EB, but I am assuming that we will find enough operators to join validators",
        "created_at": "2023-08-04T10:04:46.133000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "general",
        "parent": "",
        "content": "At current rates it would kill state growth for ~2 months, tops.\n\nI agree with Ben that it's a big assumption larger operators will look to consolidate validators.  Certainly the last time I looked, it increased risk without providing any significant benefits.  I'd hate to see something as intrusive as MaxEB start to move forward without enough buy-in to know it's even worthwhile.",
        "created_at": "2023-08-04T10:09:32.331000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "I'm a little surprised that it's just 2 months, I thought Kraken alone was like 40k by April and that we accumulated a bit more through May. But yeah I do agree that without any support for MaxEB there's no much point on moving it forward, but that's something we have been doing for everything, no matter how trivial a change. Take a look at the optional boolean I wanted to add to the engine API.",
        "created_at": "2023-08-04T10:13:29.569000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "general",
        "parent": "",
        "content": "In my view state growth isn't the main issue related to validator set growth. I do see value in reusing indexes and I think it can be and should be shipped regardless. But the problem MaxEB tends to solve is overall network load which is far more severe to me than the state growth",
        "created_at": "2023-08-04T11:57:43.376000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "regarding index reuse, I'm not even convinced it's necessary - ie once a validator has exited, we can represent it in the state by a single hash (since it won't change any more) - that's a simple way to provide relief",
        "created_at": "2023-08-04T12:15:21.720000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "this is very much implementation dependant",
        "created_at": "2023-08-04T12:16:05.733000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "it may work for Nimbus and not for Lighthouse",
        "created_at": "2023-08-04T12:16:14.364000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "well, it's an engineering problem really - index reuse _also_ requires massive refactoring of all clients (to rework whatever caching mechanisms they have in place to make things work at all today)",
        "created_at": "2023-08-04T12:17:07.245000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "fair",
        "created_at": "2023-08-04T12:17:32.246000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "(lol, I had this brilliant idea just now, so it might have some .. er .. flaws ðŸ™‚ )",
        "created_at": "2023-08-04T12:19:08.827000+00:00",
        "attachments": null
    },
    {
        "author": "popef",
        "category": "general",
        "parent": "",
        "content": "1. Why do you think 8s is a deadline? I think the spec just expects you to send the aggregate between 8s and 12s marks.\n2. Do I understand correctly that the aggregates that Prysm aggregators will send to the gossipsub channel tentatively include only the attestations that arrive before 6.5s?",
        "created_at": "2023-08-04T16:42:50.881000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "1. The honest validator spec states \n```\nIf the validator is selected to aggregate (is_aggregator), then they broadcast their best aggregate as a SignedAggregateAndProof to the global aggregate channel (beacon_aggregate_and_proof) 2 / INTERVALS_PER_SLOT of the way through the slot-that is, SECONDS_PER_SLOT * 2 / INTERVALS_PER_SLOT seconds after the start of slot.\n```",
        "created_at": "2023-08-04T16:51:10.605000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "2. This is correct in the current defaults, which gives time to the slowest reasonable machines in the worst possible conditions to submit a full aggregate before the 8 seconds mark. We have made substantial improvements in this area and will most probably up this up to 7.5 seconds in a future release.",
        "created_at": "2023-08-04T16:52:46.705000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "for reference, a small NUC homestaking can use 7.9 seconds easily (which is what I run at home)",
        "created_at": "2023-08-04T16:53:19.379000+00:00",
        "attachments": null
    },
    {
        "author": "popef",
        "category": "general",
        "parent": "",
        "content": "That doesn't sound strict, right?",
        "created_at": "2023-08-04T16:56:29.600000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "My reading of this spec is that at 8 seconds you need to submit the best aggregation you have. There have been arguments about  this before, and the reason I take this reading is that something similar happens for proposals\n```\nA validator is expected to propose a SignedBeaconBlock at the beginning of any slot during which is_proposer(state, validator_index) returns True.\n```\nWhich is also a loose statement and some take it as not mandatory to submit as soon as the slot starts.",
        "created_at": "2023-08-04T16:58:43.144000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "An aggregate coming at 11.9 seconds is useless and if clients start doing that we set ourselves for split views. So probably better strict phrasing and clarification on this particular issue may be worth it. What do you think \u003c@291925846556540928\u003e",
        "created_at": "2023-08-04T16:59:31.243000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "general",
        "parent": "",
        "content": "We weren't great about MUST/SHOULD/MAY language in specificity of these specs",
        "created_at": "2023-08-04T17:40:05.345000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "general",
        "parent": "",
        "content": "The intention of the spec is for a validator to send *at* the specified times such that messages arrive before and in time for action of the next time segment",
        "created_at": "2023-08-04T17:40:36.659000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "general",
        "parent": "",
        "content": "there is obviously play in acceptable behavior (especially even just considering time sync issues which the p2p has something like a 400ms allowance baked in), but I do agree that given a particular local aggregation strategy that there will be pre-8s time cutoffs and considerations",
        "created_at": "2023-08-04T17:41:40.676000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "general",
        "parent": "",
        "content": "does prysm aggregate individuals as they come in? or wait until this 6.5 boundary to do all the work? Would an interative aggregation allow for a less harsh time cut-off here? that is, you could be tacking on singles to your best-aggregate up until marginally before 8s",
        "created_at": "2023-08-04T17:42:35.628000+00:00",
        "attachments": null
    },
    {
        "author": "popef",
        "category": "general",
        "parent": "",
        "content": "I just created a PR to make the spec more concrete https://github.com/ethereum/consensus-specs/pull/3472\n\nI don't have a strong opinion on how the spec should be and I don't think we should work too much on the client side, but I think it will be easier for the researchers (especially network researchers) to work on if we have a concrete spec since they will have the concrete protocol in mind.",
        "created_at": "2023-08-04T18:06:56.050000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "Other clients do an aggregate-as-it comes I believe. At least I know Lighthouse does. I benchmarked this approach and it ended up being a little worse given our strategy (but not that worse) because it sometimes used CPU at critical time around the 4 seconds where other locks are being held. Since essentially all attestations are seen way before the 6.5 seconds marks anyway, it is currently very safe to just aggregate in parallel everything you see at that point in time. We have a lot of slack to move that limit safely to 7.5 seconds without affecting even the largest of operators. But since we didn't meassure any degrading on our aggregates, we kept the current conservative default.",
        "created_at": "2023-08-04T18:25:42.568000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "On nimbus's side we consider the aggregation to take negligible time - at the end of the day, we perform similar operations to _validate_ the aggregate - creating it doesn't really take any significant amount of time",
        "created_at": "2023-08-04T18:26:55.211000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "ie we get single attestations and .. add them together, much like you have to add pubkeys to validate the aggregate later on - it's really quite fast",
        "created_at": "2023-08-04T18:27:41.631000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "the major downside I can think of in terms of prysms strategy is that it causes an increase in bandwidth and cpu usage for everyone, but the end result in term sof block production is more or less the same: prysm will send out a lower-quality aggregate that everyone starts spreading, then comes a better aggregate later which the network also has to process (as opposed to dropping it due to the \"covered-aggregate-ignore-rule\"",
        "created_at": "2023-08-04T18:30:47.125000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "one thing I've considered is an \"early-aggregate\" rule: if your client has observed all votes of the committee, we could send the aggregate earlier than 8s and not waste any more time/bandwidth on it",
        "created_at": "2023-08-04T18:32:27.559000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "but what I _really_ think is important is that clients implement the \"attest-when-block-arrives\" rule - this is clear in the spec: it says that clients should send the attestation at once - before we make any other strictness increases in the spec, we should maybe start following that one in all clients",
        "created_at": "2023-08-04T18:33:41.454000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "regarding aggregates in particular, I also think that the spec de facto doesn't require sending on the 8s mark: the time between 8 and 12 seconds is all dedicated to aggregation work - it's a really long and useless window in general - or rather, it's useful because we can squeeze in other work in this window such as pruning databases, cleaning fork choice and pre-heating epoch transitions for the next slot, but other than that, it's a really quiet period",
        "created_at": "2023-08-04T18:36:10.352000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "to make this more concrete, aggregating 512 signatures taks 0.6 ms on my laptop with blst - even if you're in all committees, this really isn't a bottelneck",
        "created_at": "2023-08-04T19:07:48.914000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "it takes longer to verify that one aggregate! ie about 1 ms..",
        "created_at": "2023-08-04T19:09:45.732000+00:00",
        "attachments": null
    }
]