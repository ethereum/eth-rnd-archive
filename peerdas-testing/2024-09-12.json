[
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "I've done another round of interop testing with 4 clients. The network started forking after 706 slots and lost finalization after 1186.  I think the trigger might be insufficient resources on a single machine (slow proof computation), but it's good to see some clients able to recover. I haven't looked at the logs yet, but I've attached them on this page if anyone has time\nhttps://hackmd.io/@jimmygchen/r1m3moJ60",
        "created_at": "2024-09-12T00:11:52.142000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "Yeah I think that makes sense.",
        "created_at": "2024-09-12T00:12:51.050000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "ignore this one, I'm going to try it again, kurtosis dump didn't capture full lighthouse logs ü§¶‚Äç‚ôÇÔ∏è",
        "created_at": "2024-09-12T01:18:21.187000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "^ i can give it a shot against kubernetes, we wont see resource issues that easily then üôÇ",
        "created_at": "2024-09-12T07:44:34.538000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "there is a bug i have just found in lodestar to in storing and fetching when storing \u003c128 columns that i am hunting down (and hence will give incorrect results)",
        "created_at": "2024-09-12T08:09:57.363000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "could be the reason for lodestar forking off",
        "created_at": "2024-09-12T08:10:21.840000+00:00",
        "attachments": null
    },
    {
        "author": "nashatyrev",
        "category": "Testing",
        "parent": "",
        "content": "Could you please shortly explain what are the benefits of having `CUSTODY_REQUIREMENT \u003c SAMPLES_PER_SLOT` now? \nFrom my perspective it doesn't affect the network traffic: a peer would still subscribe to `SAMPLES_PER_SLOT` subnets. It would just decrease storage requirements",
        "created_at": "2024-09-12T09:42:07.090000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "It will allow clients who can, the ability to sample for data columns rather than subscribe to them. ex: custody = 4 and samples = 16 , you wouldn't need to subscribe to 12 other subnets. Without peer sampling as in the spec right now, there is no difference with custody=16.  But peer sampling will be added back in the near future as an option from what I understand. \n\nAlso primary reason to have custody requirement is for a stable backbone rather then to assert data availability of a block",
        "created_at": "2024-09-12T09:50:05.576000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Not a resource limit issue üòÑ\n\nnetwork spontaneously combusts after like 11 epochs indeed",
        "created_at": "2024-09-12T09:56:13.485000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "3f7cf7545e198da8fdbc35eae5f10984b9b3912601acc7ae074e4e6532b9b030"
            }
        ]
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "this is on kubernetes, with a relatively low chance of any resource limits being hit (Atleast as defined in the kurtosis config here https://hackmd.io/@jimmygchen/r1m3moJ60)",
        "created_at": "2024-09-12T09:57:13.500000+00:00",
        "attachments": [
            {
                "type": "application/zip",
                "origin_name": "my-testnet--637d2693a36b4fe982cfb162de8d9c98.zip",
                "content": "1ea950d21af03dacfae3038f689091b041a16245563275853107d51174d0e0ec"
            }
        ]
    },
    {
        "author": "nashatyrev",
        "category": "Testing",
        "parent": "",
        "content": "But there are obvious downsides of this solution: weaker backbones, harder to find peers for sync. Do we really need to adopt this prior to future changes?",
        "created_at": "2024-09-12T10:06:08.785000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "Any future change will always involve us in reducing the number of subnets participated in. Ex: 16 -\u003e 4 . This just allows us to do so without a hardfork",
        "created_at": "2024-09-12T10:08:53.730000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "The alternative is that if we want to finally run with peer sampling, we will need a hardfork to do it because you can't just drop down the number of custodied subnets without being penalized",
        "created_at": "2024-09-12T10:09:49.913000+00:00",
        "attachments": null
    },
    {
        "author": "manunlp",
        "category": "Testing",
        "parent": "",
        "content": "For missed blocks about Prysm full node, we have an issue I'm currently working on:\nAs soon as we have a fork, then the prysm full node will take a very long time to process incoming blocks.\nIt will always lag and so will be unable to propose blocks. (It does not happen with Prysm super nodes.)",
        "created_at": "2024-09-12T10:27:17.401000+00:00",
        "attachments": null
    },
    {
        "author": "nashatyrev",
        "category": "Testing",
        "parent": "",
        "content": "So does it makes sense until the future soft-fork make the  `custody_subnet_count \u003e= SAMPLES_PER_SLOT`  the recommended (or obligatory) option for all clients?",
        "created_at": "2024-09-12T10:37:57.470000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "if you do this, you would need a hardfork to make `custody_subnet_count \u003c SAMPLES_PER_SLOT`",
        "created_at": "2024-09-12T10:40:28.635000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "its not optional if older nodes start penalizing you and banning you for having a lower custody count",
        "created_at": "2024-09-12T10:41:12.938000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "fixed the bug, but i also see another issue prsym seems to be requesting columns not custodied",
        "created_at": "2024-09-12T10:43:05.891000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "44502ee58fe813ae913e8cbf52ef89a4ce961db5911d0f9f5ebdcf6aef7c5a3d"
            }
        ]
    },
    {
        "author": "manunlp",
        "category": "Testing",
        "parent": "",
        "content": "Do you have an image/kurtosis config so I can reproduce locally?",
        "created_at": "2024-09-12T10:59:38.242000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "just to double verify: columns for nodeId `cdbee32dc3c50e9711d22be5565c7e44ff6108af663b2dc5abd2df573d2fa83f` should be `0,2,4,107`",
        "created_at": "2024-09-12T11:08:23.399000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "```yaml\nparticipants:\n# Supernodes\n  - cl_type: lodestar\n    cl_image: ethpandaops/lodestar:peerDAS\n    cl_extra_params: [--persistNetworkIdentity, --supernode]\n# # Non supernodes\n  - cl_type: prysm\n    cl_image: ethpandaops/prysm-beacon-chain:peerDAS\n  - cl_type: lodestar\n    cl_image: ethpandaops/lodestar:peerDAS\n    cl_extra_params: [--persistNetworkIdentity]\n```",
        "created_at": "2024-09-12T11:10:01.936000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "running in non minimal settings",
        "created_at": "2024-09-12T11:10:15.131000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "i don't remember where we had the columns test in testsuite from where i picked initial testvector when subnets used to be 32 and not 128",
        "created_at": "2024-09-12T11:12:12.400000+00:00",
        "attachments": null
    },
    {
        "author": "manunlp",
        "category": "Testing",
        "parent": "",
        "content": "Thanks, will take a look.",
        "created_at": "2024-09-12T11:13:45.896000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "it seems to be from sampling requests: \n```\ntime=\"2024-09-12 10:53:09.90\" level=debug msg=\"Sampled columns from peer with some errors\" peerID=16Uiu2HAkwuNxYrkZJhUHzhuEd6kHrbcQAREQkd8z7EdkRU7bCh26 prefix=sync requestedColumns=[89] retrievedColumns=[] root=0x965b8f570b57dd10efb957fb7ea7d4e02f98b18e247d4a1899147f8517569054\n```",
        "created_at": "2024-09-12T11:20:23.164000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "may be there is a mismatch on columns compute?",
        "created_at": "2024-09-12T11:20:40.080000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@199561711278227457\u003e do we have a testcase for this (column compute)? i remember the old testcase when we have 32 subnets but that doesn't seem to be working as we now have 128 subnets so that value has changed",
        "created_at": "2024-09-12T11:21:35.606000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "you mean a spectest? i havent looked at that for peerdas, so unsure :/",
        "created_at": "2024-09-12T11:22:52.738000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "yes",
        "created_at": "2024-09-12T11:23:05.129000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "https://media.githubusercontent.com/media/ethereum/consensus-spec-tests/master/tests/mainnet/eip7594/networking/get_custody_columns/pyspec_tests/",
        "created_at": "2024-09-12T11:24:30.725000+00:00",
        "attachments": null
    },
    {
        "author": "manunlp",
        "category": "Testing",
        "parent": "",
        "content": "Yeah probably either Prysm or Lodestar is wrong. I'll check on my side.",
        "created_at": "2024-09-12T11:26:36.577000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "ok i seem to be passing this spec test: https://media.githubusercontent.com/media/ethereum/consensus-spec-tests/master/tests/mainnet/eip7594/networking/get_custody_columns/pyspec_tests/get_custody_columns__1/meta.yaml",
        "created_at": "2024-09-12T11:30:34.589000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "i feel all clients should see to it that they pass all networking fixture tests by EF, because it is the only gateway for us to know which topics to subscribe to as a full node and what columns to custody.",
        "created_at": "2024-09-12T11:41:36.224000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "ie all of this https://github.com/ethereum/consensus-spec-tests/tree/v1.5.0-alpha.5/tests/mainnet/eip7594/networking/get_custody_columns/pyspec_tests",
        "created_at": "2024-09-12T11:42:21.541000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "\u003e do we have a testcase for this (column compute)? i remember the old testcase when we have 32 subnets but that doesn't seem to be working as we now have 128 subnets so that value has changed\ni think this should be fine? \u003c@792822129019584562\u003e",
        "created_at": "2024-09-12T11:45:00.036000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "yes",
        "created_at": "2024-09-12T11:46:49.299000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "https://github.com/ethereum/consensus-specs/blob/dev/specs/_features/eip7594/das-core.md#get_custody_columns additionally can everyone please confirm that this `hash` should be sha256 and nothing else",
        "created_at": "2024-09-12T11:47:08.052000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "because i tried using keccak once, and i still passed 5/9 test cases",
        "created_at": "2024-09-12T11:47:31.878000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "this is the uint256",
        "created_at": "2024-09-12T11:47:37.974000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "yeah but the inner hash",
        "created_at": "2024-09-12T11:47:49.098000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "ok yeah",
        "created_at": "2024-09-12T11:47:57.204000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "hash in above is sha256 only",
        "created_at": "2024-09-12T11:49:33.320000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "okay",
        "created_at": "2024-09-12T11:49:40.583000+00:00",
        "attachments": null
    },
    {
        "author": "nashatyrev",
        "category": "Testing",
        "parent": "",
        "content": "Nope I didn't mean adding this requirement to spec and neither penalize for violation. Just agree between clients on defaulting `custody_subnet_count` to be at least `SAMPLES_PER_SLOT`",
        "created_at": "2024-09-12T12:33:21.007000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "That happened pretty quickly! will take a look at the logs, thanks!\nMy current test took 50 epochs before nodes started forking, again with the 6 blobs block - need some investigation, because even if computation takes too long and the block is orphaned, the node should be able to recover / re-org to the other chain",
        "created_at": "2024-09-12T13:08:41.950000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Interestingly, it seems to be a function of resources üòÑ\nI did another test with larger nodes and higherl limits (8cpu + 8gb ram per CL). It lasted till epoch 15 before crashing and burning. I can get you the logs for this run too if you like",
        "created_at": "2024-09-12T13:34:46.714000+00:00",
        "attachments": null
    },
    {
        "author": "jtraglia",
        "category": "Testing",
        "parent": "",
        "content": "Do full nodes only use a single core?",
        "created_at": "2024-09-12T13:42:32.232000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "hey hey, can you add a super nimbus to the party? use `pdsync` branch üôÇ",
        "created_at": "2024-09-12T13:42:33.730000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "```\n  - cl_type: nimbus\n    cl_image: ethpandaops/nimbus-eth2:pdsync\n    cl_extra_params: \n      - --subscribe-all-subnets=true\n      - --sync-light-client=no\n```",
        "created_at": "2024-09-12T13:43:08.840000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "whenever it's good",
        "created_at": "2024-09-12T13:43:25.613000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "I added \n```\n    cl_max_mem: 8192\n    cl_max_cpu: 8000\n```\nto all of them, including full nodes üòÑ\nso they should be using ~8cpu cores",
        "created_at": "2024-09-12T13:44:47.100000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "sure, can do üòÑ",
        "created_at": "2024-09-12T13:44:59.646000+00:00",
        "attachments": null
    },
    {
        "author": "jtraglia",
        "category": "Testing",
        "parent": "",
        "content": "Probably unrelated, but when investigating perf issues the other week, I had to completely reset kurtisis (kill/delete all containers) for it to properly set the core count. It was strange, I‚Äôll forward you some messages from then.",
        "created_at": "2024-09-12T13:49:04.226000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "this happens with me occassionally",
        "created_at": "2024-09-12T13:50:35.601000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "i have to delete all volumes and containers",
        "created_at": "2024-09-12T13:50:54.174000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "everything",
        "created_at": "2024-09-12T13:50:55.656000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "ah interesting, i can make an issue to look into this - also i'll try to validate that its indeed doing so on kubernetes (since we dont use it so often)",
        "created_at": "2024-09-12T13:51:06.368000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "do you mean just for devnet-2 ? Or in general",
        "created_at": "2024-09-12T13:51:19.325000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Testing",
        "parent": "",
        "content": "One issue with doing this imo is that we would eventually not want to have every validator be subscribed to 1/8 of all subnets, but by making this the default behavior we won't actually be confident that the network works well with a weaker backbone",
        "created_at": "2024-09-12T14:09:30.100000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Testing",
        "parent": "",
        "content": "Do you think there would be issues with 4 custody subnets only? After all, 1 out of 32 is the same ratio we use for attestation subnets. And additionally validator custody would mean that many nodes are subscribed to at least 8 and up to all subnets",
        "created_at": "2024-09-12T14:10:47.208000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "works! üòÑ\n\n\nI'll leave it running, but would assume it breaks down after ~15-20 epochs like the previous ones",
        "created_at": "2024-09-12T14:11:30.576000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "5c5494df00323dd55c5696a0afa301782770aa559903b12a12772dbe50b59f1f"
            }
        ]
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "Let‚Äôs see",
        "created_at": "2024-09-12T14:20:54.760000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "yeah it broke üòÑ\nbut at epoch 6 this time.",
        "created_at": "2024-09-12T14:43:37.250000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "Oops",
        "created_at": "2024-09-12T15:16:09.706000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Logs in case you need it. But assuming the other logs from earlier would showcase the same root issue.",
        "created_at": "2024-09-12T15:31:54.066000+00:00",
        "attachments": [
            {
                "type": "application/zip",
                "origin_name": "my-testnet--01fdb140154e41639197b81acc381f2f.zip",
                "content": "8f36dd210d0709d00ff7a72d132682adbd1d2a972e438d2ec1447ec2da6775f7"
            }
        ]
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "Okay",
        "created_at": "2024-09-12T15:35:15.104000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "I‚Äôll try to repro this once",
        "created_at": "2024-09-12T15:35:42.933000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "Can you send the entire config?",
        "created_at": "2024-09-12T15:35:55.280000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "```\nparticipants:\n# Super nodes\n  - cl_type: prysm\n    cl_image: ethpandaops/prysm-beacon-chain:peerDAS\n    cl_max_mem: 8192\n    cl_max_cpu: 8000\n    cl_extra_params:\n      - --subscribe-all-subnets\n      - --minimum-peers-per-subnet=1\n  - cl_type: lighthouse\n    cl_image: ethpandaops/lighthouse:unstable\n    cl_max_mem: 8192\n    cl_max_cpu: 8000\n    cl_extra_params:\n      - --subscribe-all-data-column-subnets\n  - cl_type: teku\n    cl_image: ethpandaops/teku:nashatyrev-das\n    cl_max_mem: 8192\n    cl_max_cpu: 8000\n    cl_extra_params:\n      - --p2p-subscribe-all-custody-subnets-enabled\n  - cl_type: nimbus\n    cl_image: ethpandaops/nimbus-eth2:pdsync\n    cl_extra_params:\n      - --subscribe-all-subnets=true\n      - --sync-light-client=no\n# Full nodes\n  - cl_type: prysm\n    cl_image: ethpandaops/prysm-beacon-chain:peerDAS\n    cl_max_mem: 8192\n    cl_max_cpu: 8000\n    cl_extra_params:\n      - --minimum-peers-per-subnet=1\n      - --data-columns-withhold-count=64\n  - cl_type: lighthouse\n    cl_image: ethpandaops/lighthouse:unstable\n    cl_max_mem: 8192\n    cl_max_cpu: 8000\nnetwork_params:\n  eip7594_fork_epoch: 0\n  eip7594_fork_version: \"0x50000038\"\n  genesis_delay: 300\nsnooper_enabled: true\nglobal_log_level: debug\nadditional_services:\n  - dora\n  - goomy_blob\n  - prometheus_grafana\ndora_params:\n  image: ethpandaops/dora:peerdas-as-deneb\nethereum_metrics_exporter_enabled: true\n```",
        "created_at": "2024-09-12T15:37:56.146000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "i don't see lodestar here ü§î",
        "created_at": "2024-09-12T16:15:11.257000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "didnt you say there was a bug in lodestar?",
        "created_at": "2024-09-12T16:35:45.014000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "yea it got resolved",
        "created_at": "2024-09-12T16:56:04.289000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "I have simulated exactly this yesterday with 1 super and 1 full lodestar into the party and without teku, and I‚Äôve seen no forks for atleast 25 epochs, need a closer look really at this point",
        "created_at": "2024-09-12T17:49:32.547000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "That‚Äôs how long I ran it on my machine",
        "created_at": "2024-09-12T17:49:52.988000+00:00",
        "attachments": null
    },
    {
        "author": "dmitriishmatko",
        "category": "Testing",
        "parent": "",
        "content": "Teku passes",
        "created_at": "2024-09-12T21:35:01.875000+00:00",
        "attachments": null
    },
    {
        "author": "dmitriishmatko",
        "category": "Testing",
        "parent": "",
        "content": "Is there any way to get vcs tag of the image? it looks like an old Teku for me",
        "created_at": "2024-09-12T21:49:33.023000+00:00",
        "attachments": null
    },
    {
        "author": "dmitriishmatko",
        "category": "Testing",
        "parent": "",
        "content": "or full log, we log version at the very beginning",
        "created_at": "2024-09-12T21:49:50.829000+00:00",
        "attachments": null
    },
    {
        "author": "dmitriishmatko",
        "category": "Testing",
        "parent": "",
        "content": "or looks like I left some more traces of blobSidecars",
        "created_at": "2024-09-12T21:54:14.031000+00:00",
        "attachments": null
    },
    {
        "author": "dmitriishmatko",
        "category": "Testing",
        "parent": "",
        "content": "```\n2024-09-12T09:15:03.151492918Z time=\"2024-09-12 09:15:03.15\" level=debug msg=\"Received block\" blockSlot=300 graffiti=\"6-geth-teku TKd08e832dGEe9467eec\" prefix=sync proposerIndex=341 sinceSlotStartTime=148.529924ms validationTime=2.762084ms\n```\nohh, I found a way, it's latest Teku, all good, we still have some blobSidecar traces, working to get rid of them",
        "created_at": "2024-09-12T22:59:30.513000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "do you see them getting allocated enough resources? without changing the defualt `cl_min_cpu`, they might still all get tightly scheduled",
        "created_at": "2024-09-12T23:55:30.977000+00:00",
        "attachments": null
    }
]