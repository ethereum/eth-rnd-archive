[
    {
        "author": "manunlp",
        "category": "Testing",
        "parent": "",
        "content": "Kurtosis question:\n\nTo test the initial sync, I would like to:\n1. Run a devnet with, let's say, 2 EL/BN/VC\n2. Wait for a few epochs\n3. Launch a third EL/BN (VC is not mandatory)\n\nFor `1.`, I tried with this configuration file:\n```yaml\nparticipants:\n  - el_type: geth\n    cl_type: teku\n    cl_image: ethpandaops/teku:nashatyrev-das\n    vc_image: ethpandaops/teku:nashatyrev-das\n    count: 2\nnetwork_params:\n  eip7594_fork_epoch: 0\n  eip7594_fork_version: \"0x50000038\"\nsnooper_enabled: true\nadditional_services:\n  - dora\n  - goomy_blob\n  - apache\n```\n\nThen for `3.`, I tried:\n- Either to change `count: 2` to `count: 3`\n- Either to add new participant, leading to this configuration file:\n\n\n```yaml\nparticipants:\n  - el_type: geth\n    cl_type: teku\n    cl_image: ethpandaops/teku:nashatyrev-das\n    vc_image: ethpandaops/teku:nashatyrev-das\n    count: 2\n  - el_type: geth\n    cl_type: teku\n    cl_image: ethpandaops/teku:nashatyrev-das\n    vc_image: ethpandaops/teku:nashatyrev-das\n    count: 1\nnetwork_params:\n  eip7594_fork_epoch: 0\n  eip7594_fork_version: \"0x50000038\"\nsnooper_enabled: true\nadditional_services:\n  - dora\n  - goomy_blob\n  - apache\n```\n\nIn both cases, it seems all the nodes are rebooting.\n\n==\u003e Is there a way to add a EL/BN pair \"on the flight\"?",
        "created_at": "2024-07-12T07:50:25.313000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "I'd advise to start up with all participants you want to ever run. And after you created the enclave, you quickly shut down one, to get out of sync. I would also give this participants little number of validators, so that it wouldn't affect the general health of the network.\n\nSo something like this \n```yaml\nparticipants:\n  - el_type: geth\n    cl_type: teku\n    cl_image: ethpandaops/teku:nashatyrev-das\n    count: 3\n  - el_type: geth\n    cl_type: teku\n    cl_image: ethpandaops/teku:nashatyrev-das\n    validator_count: 10\nnetwork_params:\n  eip7594_fork_epoch: 0\n  eip7594_fork_version: \"0x50000038\"\nsnooper_enabled: true\nadditional_services:\n  - dora\n  - goomy_blob\n```\nthen run this, and proceed with `kurtosis service stop \u003cenclave-name\u003e cl-4-teku-geth` then you can wait a bit, and then you can start the service back up with `kurtosis service start \u003cenclave name\u003e cl-4-teku-geth`",
        "created_at": "2024-07-12T08:27:32.959000+00:00",
        "attachments": null
    },
    {
        "author": "manunlp",
        "category": "Testing",
        "parent": "",
        "content": "Thanks, will try that!",
        "created_at": "2024-07-12T08:29:10.459000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "what would be a practical number (considering devnets), for parallel requests per loop, if i were to run a data colum by root request loop?",
        "created_at": "2024-07-12T10:42:07.277000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "My current proposed fix is to only attempt once per peer, if the peer fails to return the column, then we will try the another peer custodying the same column, until we run out of peers, and then the sampling is considered failed. \nAnd then we increase the sample size and try again (IncrementalDAS).",
        "created_at": "2024-07-12T12:05:57.213000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "how long do you wait between starting to cycle from the start of the peers?",
        "created_at": "2024-07-12T12:10:51.113000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "on a very small network we gonna keep DOSing each other if we don't wait a bit between failures",
        "created_at": "2024-07-12T12:11:03.092000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "oh i think i didn't answer your question - I think the number of parallel requests would depend on the rate limits of clients, you'd want to do it as fast as you can without getting rate limited / disconnected.",
        "created_at": "2024-07-12T12:11:30.376000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "i assumed you were asking about how many times we try, because that's was the issue with Lighthouse earlier - we were retrying the same request on the same peer.",
        "created_at": "2024-07-12T12:12:02.301000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "Okay makes sense now",
        "created_at": "2024-07-12T12:13:48.354000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "i think once that PR is merged, we'd only send the same request to the same peer at most once, so unlikely going to DOS under the same scenario.\nI'm also proposing to enable outbound rate limit by default, so we don't send too many requests in a short period.",
        "created_at": "2024-07-12T12:13:53.573000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "Agree on this",
        "created_at": "2024-07-12T12:15:52.403000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "What is LHâ€™s rate limit in that case?",
        "created_at": "2024-07-12T12:16:36.983000+00:00",
        "attachments": null
    },
    {
        "author": "agnxsh",
        "category": "Testing",
        "parent": "",
        "content": "For request by root of data columns",
        "created_at": "2024-07-12T12:16:59.030000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "Right now it's 512 columns per 10 seconds (peer peer), ~~but that's going to limit our sync speed to 32 slots per 10 seconds, we may need to tweak it later.~~",
        "created_at": "2024-07-12T12:20:56.687000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "Thats very harsh limiting imo",
        "created_at": "2024-07-12T12:26:33.741000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "are these columns gonna need to be backfilled till blob expiry?",
        "created_at": "2024-07-12T12:28:30.868000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "yeah actually that was incorrect. This is per peer limit so it's not too bad.",
        "created_at": "2024-07-12T12:32:19.568000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Testing",
        "parent": "",
        "content": "yep custody columns will need to be downloaded until blob expiry. but thats a separate rate limiting bucket (for by range method)",
        "created_at": "2024-07-12T12:33:33.020000+00:00",
        "attachments": null
    }
]