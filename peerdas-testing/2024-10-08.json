[
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "But I’m still a bit curious why this broke the network. \nAll other clients should have just rejected the block and only nimbus would have forked away.",
        "created_at": "2024-10-08T08:14:20.668000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "and if this is an issue, wouldn’t any malicious clients be able to break the network if other clients are also susceptible to this “attack”?",
        "created_at": "2024-10-08T08:14:58.441000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "changing the spec makes sense tho non the less",
        "created_at": "2024-10-08T08:15:08.150000+00:00",
        "attachments": null
    },
    {
        "author": "dapplion",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@199561711278227457\u003e \u003c@412614104222531604\u003e  raising again the petition to have a log aggregator running on peerdas devnet. greping through individual nodes is a very time consuming process compared to how agile the LH team debugs things in live devnets that we have our own infra with a log aggregator",
        "created_at": "2024-10-08T09:03:37.508000+00:00",
        "attachments": null
    },
    {
        "author": "manunlp",
        "category": "Testing",
        "parent": "",
        "content": "Yeah I agree. However,  on devnet 2, all columns were invalid. Here, depending on your peers, you may see column `n` valid, while an other peer (connected to nimbus) may see column `n` invalid.\n\nI see 5 cases:\n\n**1. Super node**\n1.a. All 128 columns are valid ==\u003e OK\n1.b. \u003e 64 columns are valid ==\u003e Reconstruct ==\u003e OK\n1.c. \u003c 64 columns are valid ==\u003e Ignore the block, then, eventually rejoin the main fork if the main fork succeeded to get/reconstruct all 128 (valid) columns ==\u003e OK\n\n**2. Full node**\n2.a All 8 columns are valid ==\u003e OK\n2.b Some columns are invalid ==\u003e Ignore the block, then, eventually rejoin the main fork if the main fork succeeded to get/reconstruct all 8 (valid) columns ==\u003e OK\n\nIn all cases, the network should recover IMO.",
        "created_at": "2024-10-08T09:19:12.776000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "Looks like all prysm nodes are up to head, and proposing blocks, and mostly everyone else has fallen behind.",
        "created_at": "2024-10-08T10:20:35.573000+00:00",
        "attachments": null
    },
    {
        "author": "manunlp",
        "category": "Testing",
        "parent": "",
        "content": "Yeah Prysm got through the storm for this devnet, with some Teku and Lodestar nodes",
        "created_at": "2024-10-08T10:26:51.466000+00:00",
        "attachments": null
    },
    {
        "author": "dmitriishmatko",
        "category": "Testing",
        "parent": "",
        "content": "Teku checks this on req/resp and lengths too",
        "created_at": "2024-10-08T15:51:05.801000+00:00",
        "attachments": null
    },
    {
        "author": "dmitriishmatko",
        "category": "Testing",
        "parent": "",
        "content": "shouldn't we call new https://github.com/ethereum/consensus-specs/blob/dev/specs/_features/eip7594/p2p-interface.md#verify_data_column_sidecar on reqresp too? We will not request index greater than number of columns but other checks looks good",
        "created_at": "2024-10-08T15:53:04.951000+00:00",
        "attachments": null
    },
    {
        "author": "jtraglia",
        "category": "Testing",
        "parent": "",
        "content": "Yes, we should update the spec so that's more clear.",
        "created_at": "2024-10-08T17:05:02.072000+00:00",
        "attachments": null
    }
]