[
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "the simple solution here is that the server withholds the response until the client is within the quota - this avoids the need for complex error handling, decreases chatter, avoids roundtrip timing issues and is trivial to implement - the rate limiting happens \"naturally\" at the pace that the server is willing to serve and the client can judge how good a server is by the only metric that mattters: how many valid blocks is this server giving me, per second",
        "created_at": "2021-10-20T07:13:24.282000+00:00",
        "attachments": []
    },
    {
        "author": "nashatyrev",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Right, this looks like the latter option I suggested (with parallelism of max `1`). And it looks more simple and natural to me as well.\nHowever that may work weird for ping/metadata requests. I.e. if a node assumes max rate for ping as 1 request per 10 seconds, what should it do when receiving an extra ping request? From the other side should we rate limit ping/metadata requests at all?",
        "created_at": "2021-10-20T10:02:40.998000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "well, one thing to keep in mind is that ping requests are pretty useless from a utility point of view - they basically announce that you have nothing important to say - re parallelism, there's a case to allow multiple parallel requests up to some limit, but again, this \"naturally\" solves itself if you limit processing to one at a time per node - this allows clients to pipeline requests and avoid roundtrip latency, but by serving them one by one according to a budget, if they send too many, some will simply time out - and if you have too many requests timing out, you eventually disconnect (regardless if they're pings or block requests)",
        "created_at": "2021-10-20T10:58:01.034000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "from a performance point of view, you might make 1-2 block requests in parallel - one in-flight and one queued - beyond that, you don't have much to gain as a client - if you're doing more, you're _likely_ buggy or spammy",
        "created_at": "2021-10-20T11:00:15.583000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "the beauty of this solution is that you don't need to add anything to the spec, and you don't have to worry about compatibility over time with different versions of clients",
        "created_at": "2021-10-20T11:01:54.501000+00:00",
        "attachments": []
    },
    {
        "author": "nashatyrev",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Totally agree except I believe it should still be added to the spec for all clients to be in line with this rate limiting scheme. \nCurrently spec doesn't prevent you from sending as many parallel requests as you like. \nI'm also tending to limit max parallel requests (to `2`) on per message type basis.",
        "created_at": "2021-10-20T11:10:13.047000+00:00",
        "attachments": []
    }
]