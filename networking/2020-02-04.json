[
    {
        "author": "agemanning",
        "category": "Consensus Layer",
        "parent": "",
        "content": "I believe originally we include the payload length prefix so that a receiver can read that exact number of bytes then drop the stream. If the length is too large we can drop the stream immediately. As it's stated in the spec, this isn't really necessary as a receiver can try and read `MAX_CHUNK_SIZE` then drop the stream once we hit that size. \nsnappy compression prefixes the uncompressed length, so if I'm not mistaken, \u003c@!203220829473996800\u003e B suggestion is to simply remove the `encoding-dependent-header` for snappy compression. \nI don't see any issues with doing this. It saves us a bit of redundant information and allows streaming compression. We're happy to go down this path, although would like to hear others thoughts also",
        "created_at": "2020-02-04T03:09:04.037000+00:00",
        "attachments": null
    },
    {
        "author": "protolambda",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Right, removing it for snappy is one thing to discuss. But I still like it for the actual ssz data length, as that enables streaming decoding of the chunk. Argument also being that writing it is much cheaper, as in most cases it can be calculated by just checking a few list lengths in the value. And we get that `ssz_snappy == compress(prefix ++ ssz contents)`, and still `ssz == prefix ++ ssz contents`. So everyone who already has ssz should be able to support snappy super easily",
        "created_at": "2020-02-04T09:14:39.048000+00:00",
        "attachments": null
    },
    {
        "author": "protolambda",
        "category": "Consensus Layer",
        "parent": "",
        "content": "If we're ok with reading the bytes of a chunk fully, then encoding the length of the compressor output is good. But Snappy has two modes: block and frame (streaming) mode. With the latter we could stream chunk contents, right into ssz decoder, right into the destination memory structure. With the modification that we need the ssz length to be known in advance",
        "created_at": "2020-02-04T09:40:00.613000+00:00",
        "attachments": null
    },
    {
        "author": "protolambda",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Or we stick to fully reading the compressor output bytes, decoding it as a block, then decoding the output buffer into memory. But that means we'll have 3 copies (assuming snappy is not too effective) of the chunk in memory. Fine if we don't plan for big chunks, but not pretty",
        "created_at": "2020-02-04T09:43:05.380000+00:00",
        "attachments": null
    },
    {
        "author": "zahary.",
        "category": "Consensus Layer",
        "parent": "",
        "content": "The reason we used the wording \"encoding dependent header\" was exactly due to snappy, which can provide the length of the SSZ payload. This makes the use of length prefix at the chunk level optional and thus encoding-dependent. \n\nRight now, we just need to specify what would be the concrete snappy scheme being used (block vs frames).",
        "created_at": "2020-02-04T09:55:23.262000+00:00",
        "attachments": null
    },
    {
        "author": "protolambda",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Right, but if we use frames we can't know the full eth2 rpc chunk chunk length in advance, so no streaming if we changed status quo to use frames",
        "created_at": "2020-02-04T09:59:00.535000+00:00",
        "attachments": null
    },
    {
        "author": "zahary.",
        "category": "Consensus Layer",
        "parent": "",
        "content": "The snappy framing format is described like this:\n\nhttps://github.com/google/snappy/blob/master/framing_format.txt\n\u003e 1. General structure\n\u003e \n\u003e The file consists solely of chunks, lying back-to-back with no padding in between. Each chunk consists first a single byte of chunk identifier, then a three-byte little-endian length of the chunk in bytes (from 0 to 16777215, inclusive), and then the data if any. The four bytes of chunk header is not counted in the data length.\n\u003e \n\u003e The different chunk types are listed below. The first chunk must always be the stream identifier chunk (see section 4.1, below). The stream ends when the file ends -- there is no explicit end-of-file marker.\n\nSo, each frame has a built-in length prefix and we can perhaps extend the format with a single bit indicating that a particular frame is the last one. You would find the end of the chunk by consuming all snappy frames",
        "created_at": "2020-02-04T10:18:29.153000+00:00",
        "attachments": null
    },
    {
        "author": "protolambda",
        "category": "Consensus Layer",
        "parent": "",
        "content": "That's not exactly what I was asking/looking for, but thanks \u003c@!553610810652622860\u003e",
        "created_at": "2020-02-04T13:23:00.472000+00:00",
        "attachments": null
    },
    {
        "author": "protolambda",
        "category": "Consensus Layer",
        "parent": "",
        "content": "I made this PR with a proposal, and an updated networking doc that explains it best: https://github.com/ethereum/eth2.0-specs/pull/1606\nPlease review if you are interested in compression on RPC. Regular uncompressed ssz rpc is still the same.",
        "created_at": "2020-02-04T13:24:02.672000+00:00",
        "attachments": null
    }
]