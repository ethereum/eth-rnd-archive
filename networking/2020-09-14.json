[
    {
        "author": "ajsutton",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Found a client responding to block by range requests with:\n```[Code -128] Rate limited: wait 400.633933ms```\nAny idea what client that is and is it worth standardising a rate limited error code and message format so that calling clients can actually respect the request to back off?",
        "created_at": "2020-09-14T03:12:55.432000+00:00",
        "attachments": []
    },
    {
        "author": "agemanning",
        "category": "Consensus Layer",
        "parent": "",
        "content": "That is us. It will likely be due to the new changes we've added that prevents large step-sizes being used. \nI think this may be of interest: https://github.com/ethereum/eth2.0-specs/pull/1577",
        "created_at": "2020-09-14T03:20:22.064000+00:00",
        "attachments": []
    },
    {
        "author": "ajsutton",
        "category": "Consensus Layer",
        "parent": "",
        "content": "It was with a step size of 1, but I was very likely hammering it for all it was worth at the time, so not particularly surprised to be rate limited.",
        "created_at": "2020-09-14T03:22:40.987000+00:00",
        "attachments": []
    },
    {
        "author": "ajsutton",
        "category": "Consensus Layer",
        "parent": "",
        "content": "I think it's the first time I've seen a rate limiting error code rather than just being disconnected though which seems quite helpful.",
        "created_at": "2020-09-14T03:23:05.182000+00:00",
        "attachments": []
    },
    {
        "author": "ajsutton",
        "category": "Consensus Layer",
        "parent": "",
        "content": "I had not see that PR before...",
        "created_at": "2020-09-14T03:23:26.499000+00:00",
        "attachments": []
    },
    {
        "author": "agemanning",
        "category": "Consensus Layer",
        "parent": "",
        "content": "Yeah. I used the same response code you guys used for this reason ðŸ™‚",
        "created_at": "2020-09-14T03:23:27.812000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "well, it remains the case that it seems a lot more complex to send error code and handle the error on the client side, rather than just rate limiting on server side (by responding slower) and disconnecting spammers - when you give them an error message, that's nice debugging (now) but also wastes resources on spammers",
        "created_at": "2020-09-14T08:07:40.387000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "if sending a large step size is wrong, it would make more sense to put a limit on it in the spec",
        "created_at": "2020-09-14T08:08:16.084000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "(or remove stepsize altogether)",
        "created_at": "2020-09-14T08:08:26.684000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "if a rate limiting error is added to the spec, then it's expected that clients will implement back-offs - that in consequence means that hammering small requests until rate limited is an expected behaviour",
        "created_at": "2020-09-14T08:10:08.899000+00:00",
        "attachments": []
    },
    {
        "author": "nishant0",
        "category": "Consensus Layer",
        "parent": "",
        "content": "\u003e if sending a large step size is wrong, it would make more sense to put a limit on it in the spec\nIt makes sense to add a limit for step sizes to the spec, I dont see any reason to have unbounded step sizes. I think the majority of clients request with step sizes of 1 anyway.",
        "created_at": "2020-09-14T08:10:57.381000+00:00",
        "attachments": []
    },
    {
        "author": "agemanning",
        "category": "Consensus Layer",
        "parent": "",
        "content": "\u003c@!449019668296892420\u003e - We started going down the path of handling requests per peer and slowing down responses, but it got rather complex pretty quick. \nWe need to handle all the requests per peer and manage responses based on our resources. If a peer asks many requests, eventually we will slow them down and will hit the RPC timeout. There are things to deal with queuing the requests and making sure each response is within the timeout. If we did a FIFO or LIFO queue, some responses will inevitabely timeout. \nThen the requesting peer will see timeouts and likely penalize it. \n\nA requesting peer seeing a timeout could then deduce either it has been rate limited, or it the responding peer has over connected to peers and just simply doesnt having the resources to handle all its requests. \n\nHaving an RPC rate limit message distinguishes the two and allows clients to implement back-offs if they want to. It seemed to be the simpler/less complex solution to me.",
        "created_at": "2020-09-14T08:58:24.994000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "well, if your peer doesn't have the resources to process rpc requests, disconnecting seems.. correct?",
        "created_at": "2020-09-14T09:02:42.773000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "ie hitting a 10s timeout on an rpc request means exactly that: way too much to do (not just a little bit, but really a lot)",
        "created_at": "2020-09-14T09:04:33.586000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "at that point, why do you want to keep the resource hogs around and waste even more resources on them to send them rate limiting messages?",
        "created_at": "2020-09-14T09:05:17.495000+00:00",
        "attachments": []
    },
    {
        "author": "agemanning",
        "category": "Consensus Layer",
        "parent": "",
        "content": "In smaller networks, lighthouse can sync very fast and hit standard rate limits. If a single node has many peers it distributes the requests. But if it has a single peer (let's say) it's does many requests quite rapidly from one peer. \nRather than being disconnected from that peer, I'd prefer it send me a rate limit, such that I slow down my requests/sync and continue on the chain whilst I find more peers.",
        "created_at": "2020-09-14T09:09:44.888000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "I'm not sure I follow though - you want to put a cap on the number of stuffs a remote client requests from you, and I guess you want some fairness perhaps, so there are two limits, a global and a local one - if a peer is exceeding either budget, you can tell in how many  time units it will be within limit again and wait for that long before processing their request - what am I missing?",
        "created_at": "2020-09-14T09:26:29.733000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus Layer",
        "parent": "",
        "content": "in our initial discussions, this was also why we didn't put a strict requirement on responding fully to a request - if they make an outrageous request, you can simply create a smaller response - as much as you're willing to spend on that peer and request at that time",
        "created_at": "2020-09-14T09:31:16.174000+00:00",
        "attachments": []
    },
    {
        "author": "djrtwo",
        "category": "Consensus Layer",
        "parent": "",
        "content": "I'm working on tracking our progress on the discv5.1. Can teams please respond with current status on 5.1 implementations?\nhttps://github.com/ethereum/eth2.0-specs/issues/2059",
        "created_at": "2020-09-14T23:03:48.033000+00:00",
        "attachments": []
    }
]