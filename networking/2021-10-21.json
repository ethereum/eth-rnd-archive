[
    {
        "author": "nashatyrev",
        "category": "Consensus Layer",
        "parent": "",
        "content": "\u003c@!449019668296892420\u003e just created a spec PR: https://github.com/ethereum/consensus-specs/pull/2690",
        "created_at": "2021-10-21T09:47:24.829000+00:00",
        "attachments": []
    },
    {
        "author": "jlokier",
        "category": "Consensus Layer",
        "parent": "",
        "content": "From a performance and throughput optimisation point of view for things such as downloading blocks, I think it's mistake to (a) limit parallel request depth to 2, and (b) have the server process only one request at a time.  Both of these limits pessimize throughput in some circumstances.",
        "created_at": "2021-10-21T14:11:53.157000+00:00",
        "attachments": []
    },
    {
        "author": "jlokier",
        "category": "Consensus Layer",
        "parent": "",
        "content": "For (a) I believe the number 2 is based on the assumption that it's enough to prevent gaps in processing and data flow. But when the network bandwidth-delay product is high compared with the server's processing time, there will be gaps in the data flow with 2 in flight requests that would be closed with 3 in flight requests (and 4, 5, etc depending on the product). That means throughput will be higher with more than 2 requests in flight, even from a server that processes only 1 request at a time.",
        "created_at": "2021-10-21T14:13:10.185000+00:00",
        "attachments": []
    },
    {
        "author": "jlokier",
        "category": "Consensus Layer",
        "parent": "",
        "content": "For (b), the simplest case for processing more than one request in parallel, if you have them, is probably local database access. Local storage is like a mini network with its own round trip time. For maximum throughput the storage system needs multiple requests in flight, and one of the ways to ensure that is by servicing multiple requests in parallel when you have them,",
        "created_at": "2021-10-21T14:16:12.749000+00:00",
        "attachments": []
    },
    {
        "author": "jlokier",
        "category": "Consensus Layer",
        "parent": "",
        "content": "RLPx allows concurrent requests, and this is the reason some RLPx sub-protocols have a `requestId` field, because responses can be sent in a different order than corresponding requests.  Even sub-protocol versions without `requestId` (`eth/65` and below) allow out of order responses, but the client must do a bit more work to figure out how to handle those replies, if it chooses to send multiple requests in flight.\n\nExample from https://eips.ethereum.org/EIPS/eip-2481\n\u003e Letâ€™s consider a client making many simultaneous requests for `GetBlockHeaders` to one of its peers. By nature it can not be guaranteed that the expected responses arrive in the same order as they were sent.",
        "created_at": "2021-10-21T14:39:26.133000+00:00",
        "attachments": []
    },
    {
        "author": "nashatyrev",
        "category": "Consensus Layer",
        "parent": "",
        "content": "\u003c@!807285075736789062\u003e that totally makes sense! Constant `2` was kind of starting point. Actually there is no much difference between `2` and e.g. `64` requests for a slow server to put into its queue. May be you could copy your ideas to the PR above?",
        "created_at": "2021-10-21T14:50:53.402000+00:00",
        "attachments": []
    }
]