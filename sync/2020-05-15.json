[
    {
        "author": "0xraino",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "This is a half-baked idea but instead of calculating a schedule in advance, couldn't a server node choose a rate of leaf serving and allow the client node to construct the branches from the leaves? This way we naturally take longer on the denser parts of the trie.",
        "created_at": "2020-05-15T14:57:39.807000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@213672586264051717\u003e yes, but I think we lose the implicit swarming behavior we're shooting for in that sort of scheme since the various clients would go out of sync and end up requesting disparate data.",
        "created_at": "2020-05-15T15:08:03.114000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Here's a repost of something I just wrote up internally that I thought might be worth sharing:\n\nregarding Jason's comment here: `https://github.com/ethereum/trinity/issues/854#issuecomment-628934284`  (I really hate the snippets that discord automatically includes)\n\nWith a global view of the state trie you can partition it into roughly even chunks like [alexey's diagrams here](https://ethresear.ch/uploads/default/original/2X/6/623b715d0c15a89296778365ab45ccd84f4fc78e.png)\n\nWith respect to the goals of Merry-Go-Round sync: If we require a global view of the state to determine the schedule then there is some set of trade-offs that implies and they currently make me uncomfortable because I think they imply hard problems.  Specifically, I think that this is primarily centered around a \"Coordination Problem\" where those without a global view cannot distinguish between a legitimate schedule and a bullshit one.\n\nWhich is why I'm very motivated to find what I'm thinking of as a \"naive\" approach.  My initial thoughts were to use randomness to poke our way into the tree randomly and \"probabalistically\" cover the tree iteratively.  Jason points out that the imbalanced nature of the heavy subtries likely makes the randomness approach innefective since contracts like ?IDEX? are incredibly huge and thus the likelyhood of actually landing within that tree is actually less likely than a keccak hash collision.\n\nSo...... it is starting to look like there may be a trade-off in play unless a clever solution can be found.  Either accept the complexity of coordination costs or find a new approach to \"balance the imbalanced tree\".",
        "created_at": "2020-05-15T15:08:43.473000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I *think* that it is still possible that the random approach can work, if the tolerances for how broadly a client can traverse across the tree are sufficiently broad.  This would mean that for a client to sync ?IDEX? then it would still require that the random pointer into the trie land within range of the contract enough times to fully sync the subtrie.  I think there is research needed to determine if this is viable or not.",
        "created_at": "2020-05-15T15:13:07.696000+00:00",
        "attachments": []
    },
    {
        "author": "0xraino",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!364458974906548225\u003e I guess what I was trying to say is that any leaf-density -\u003e duration calculation done in the large implies that there's an average rate we want to go iterate through the leaf nodes in the small.\n\nWhat I'm suggesting is we have a network-wide parameter which is how many of the leaves of the current state trie we want to swarm around (`leaf interval`) in, say a duration of `d` blocks (`d` also a network-wide parameter). We start at the first node in the state that wasn't in the previous duration's `interval`. All nodes in consensus should agree on this so we keep in lockstep, swarming around these `interval`s.",
        "created_at": "2020-05-15T15:25:45.463000+00:00",
        "attachments": []
    },
    {
        "author": "0xraino",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I realize I made it sound like the interval was a node-specific parameter before ðŸ˜›",
        "created_at": "2020-05-15T15:26:20.067000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!213672586264051717\u003e that strategy requires consensus on \"what did we all do in the previous interval\", which is not a trivial problem",
        "created_at": "2020-05-15T15:27:48.817000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "imagine nodes joining in the middle of it, how do they know what everybody was doing before?",
        "created_at": "2020-05-15T15:28:10.475000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "that is the reason I favour pre-ordained schedule - it eliminates this kind of need for consensus on the past actions",
        "created_at": "2020-05-15T15:29:43.997000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "it is possible that MGR becomes wildly successful, and we can employ miners to regulate the consensus on MGR schedule, then, of course, design space widens, but at the cost of putting sync in the hands of miners",
        "created_at": "2020-05-15T15:31:46.411000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "at the moment, we have consensus on the block hashes, and state, etc., therefore these are the things we could use to \"seed\" the schedule",
        "created_at": "2020-05-15T15:33:21.733000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "if you however remove the requirement of synchrony of action, then it deteriorates into one of the existing request-response sync algorithms we have now",
        "created_at": "2020-05-15T15:34:42.075000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@456226577798135808\u003e do you have any thoughts or intuition on the randomness approach.  Specifically as to whether heavy sub-tries would pose a problem.  Suppose that for any pointer in the trie the default is to serve up to 10% of the keyspace in either direction (20% of the trie available at any given time, and synch epoch of something like 10 minutes worth of blocks).   I'm not sure what the actual probability function is, but I think those numbers result in roughly 100% coverage within ~10 epochs.  I'm just not sure whether my intuition holds for really heavy subtries and how problematic they would be  (no realy question, just looking for people to poke holes in the idea).",
        "created_at": "2020-05-15T15:39:56.046000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "my intuition makes me sceptical on the randomness approach, because, as you know the \"measure\" of state_key =\u003e density is not very uniform. And unless you can come up with an easy way to transform this measure into a more uniform measure, the randomness approach will leave large parts of the state unexplored. But I do not think such transformation of measure can be compactly described - it basically depends on the \"shape\" of the state itself",
        "created_at": "2020-05-15T15:43:32.946000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "it would work if the state was very compressible and could be generated by some very small function, but I doubt this is the case",
        "created_at": "2020-05-15T15:44:36.882000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "and if it is the case, that small function might be incomputable (it is similar to computing Kolmogorov's complexity, which is incomputable in general)",
        "created_at": "2020-05-15T15:45:16.513000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "but I might be wrong - perhaps one can enumerate all large contracts, assign weights to them, and produce the measure transformation from that",
        "created_at": "2020-05-15T15:48:08.074000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "and perhaps such transformation can fit in couple of Mb string",
        "created_at": "2020-05-15T15:48:30.437000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "that string needs to be computed by someone who has the entire state though",
        "created_at": "2020-05-15T15:49:06.761000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "which kind of goes against one of your desires",
        "created_at": "2020-05-15T15:49:17.880000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "what we do in our approach is we explicitly maintain the measure state_key =\u003e density, so we can always use it to slice up the state uniformly",
        "created_at": "2020-05-15T15:50:29.111000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "you can use something like that too, but perhaps with a lower precision - low enough that the description of the measure becomes small enough to share around",
        "created_at": "2020-05-15T15:52:03.430000+00:00",
        "attachments": []
    }
]