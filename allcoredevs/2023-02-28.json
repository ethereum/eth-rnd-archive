[
    {
        "author": "_shemnon",
        "category": "general",
        "parent": "",
        "content": "All the data for the bloom is already present in the receipt.  If a client still wants blooms it can just regenerate them.  I think this was the case of overfitting a potential optimization into the protocol that never really panned out.  External protocols like TheGraph do a much better job of it.",
        "created_at": "2023-02-28T05:40:48.134000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "I am running into an issue with Lodestar where over time I trend towards only having Prysm peers.  While I'm working with the Lodestar team to address the issue, I think that it would be valuable for *all* clients to give some guarantees of client diversity within their peer set.",
        "created_at": "2023-02-28T07:08:25.972000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "At the least, it seems like it would be prudent to ensure that there is always at least 1 peer slot for each of the well known clients so you can avoid being partitioned from the network if there is a consensus or gossip bug in a client.",
        "created_at": "2023-02-28T07:08:37.362000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "With my current situation, if there is a Prysm consensus bug I will end up being partitioned away from the network because I will only see Prysm clients.",
        "created_at": "2023-02-28T07:09:10.445000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "The same should happen on the execution layer, it feels like we should prefer to have at least one connection to each of Erigon, Nethermind, Besu, and Geth, rather than only being connected to a bunch of Geth nodes.",
        "created_at": "2023-02-28T07:09:50.128000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "In other news, whoever is eclipsing me: Well done!",
        "created_at": "2023-02-28T07:12:25.712000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "For very large nodes that may be possible, albeit highly gameable. But for small stakers it's just plain impossible. You want to connect to aggregators in your subnet, say you have 50 peers and 2 validators. You need 2 agregators . Out of your 50 you will have less than 7 aggregators, given the current network distribution, finding aggregators and keeping a few lodestar peers is just plain hard. Remember that also many are being run behind a nat that do not take incoming connections",
        "created_at": "2023-02-28T10:48:11.016000+00:00",
        "attachments": null
    },
    {
        "author": "pietjepuk",
        "category": "general",
        "parent": "",
        "content": "Only for a short while no? I assume you're gonna be downscoring peers that don't provide useful information/invalid blocks, so eventually you'll be connecting to non-Prysm nodes again.",
        "created_at": "2023-02-28T11:11:05.469000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "If I'm only connected to Prysm peers at the time of the fork, I believe I'll be stuck partitioned away from all other peers?",
        "created_at": "2023-02-28T11:51:26.898000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "My node will have no way of *finding* a non-Prysm peer unless it goes back to the bootnodes (without a restart)?",
        "created_at": "2023-02-28T11:51:42.393000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "For clarity, I am not talking about stakers here, I'm talking about anyone running a node.  I generally care much less about stakers than users.",
        "created_at": "2023-02-28T11:52:30.842000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "If I have 50 peers (default for most clients I believe), and 46 of them follow the \"normal\" rules but 4 of them are reserved for each of the consensus clients, you think that would cause problems?",
        "created_at": "2023-02-28T11:52:55.706000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "Worst case scenario, I have 46 stable peers and 4 \"empty slots\" waiting to find a peer of a particular client to fill it.",
        "created_at": "2023-02-28T11:53:15.662000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "If you really think the drop from 50 to 46 is problematic, we can just do 50 \"any\" peers + 4 \"specific\" peers (0-1 Lodestar, 0-1 Teku, 0-1 Lighthouse, 0-1 Prysm).",
        "created_at": "2023-02-28T11:53:58.606000+00:00",
        "attachments": null
    },
    {
        "author": "pietjepuk",
        "category": "general",
        "parent": "",
        "content": "Not really familiar with node discovery options. I assume libp2p has more than relying on bootnodes or nodes you're already connected to",
        "created_at": "2023-02-28T11:54:54.715000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "Hmm, I always assumed that was exactly all there was to it.  I am definitely curious if there is more to it than that!",
        "created_at": "2023-02-28T11:55:17.280000+00:00",
        "attachments": null
    },
    {
        "author": "ryanleeschneider",
        "category": "general",
        "parent": "",
        "content": "What about if the bootnodes make an effort to hand out a set of initial nodes evenly as evenly distributed by client type as possible in each discovery request?  This wouldnâ€™t solve your specific issue if it happens over time but would help with overall peering diversity.",
        "created_at": "2023-02-28T14:45:13.858000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "I'm concerned that there is a large number of possible ways you could end up with a peering bug that caused a client to prefer peers of a certain other client, resulting in eventually ending up only peered to that certain other client.",
        "created_at": "2023-02-28T15:01:06.319000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "I suspect this is the issue I'm running into with Lighthouse, but I can easily imagine a number of different ways any client could fall into this pattern, and I suspect most users would never notice.",
        "created_at": "2023-02-28T15:01:35.014000+00:00",
        "attachments": null
    },
    {
        "author": "mariusvanderwijden",
        "category": "general",
        "parent": "",
        "content": "Thats what the sync tests should test on hive",
        "created_at": "2023-02-28T15:02:14.963000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "we don't detect what kind of client is on the other end of the pipe - neither do we intend to do so, nor would it be feasible to do so reliably - the only thing we care about is that it conforms to the protocol rules - _of course_, if some client is being penalised more than others we investigate the root causes (ie whether it's us or them, with respect to protocol rules), but the solution lies not in trying to force clients of a particular kind but rather ensure that each and every client is on the same page with any explicit rules, and to write down any implicit rules which most clients follow but some are struggling with, no matter which one it is",
        "created_at": "2023-02-28T15:16:18.004000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "Isn't there a user agent supplied as part of the handshake?",
        "created_at": "2023-02-28T15:16:49.190000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "there is, and it can be set to anything you like (\"micahs client\")",
        "created_at": "2023-02-28T15:17:11.595000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "Note: My proposal here isn't to defend against adversarial eclipse type attacks.",
        "created_at": "2023-02-28T15:17:25.305000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "Just to optimistically try to ensure (as a backstop to what you mentioned above) that every client has *some* peer diversity.",
        "created_at": "2023-02-28T15:17:45.216000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "it's not a gatekeeper - more of a debugging help - and no, we won't take action based on its content, no matter how well intended - it's up to each client to fix their shit",
        "created_at": "2023-02-28T15:17:56.682000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "(with everyone's help, ie we regularly conduct pairwise debugging sessions with any client that we're struggling with, to improve both / any)",
        "created_at": "2023-02-28T15:18:42.040000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "I'm curious what bad thing you think would happen by adding this additional protection?  Is it just the extra complexity you dislike, or is there something more?",
        "created_at": "2023-02-28T15:19:49.003000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "My thinking here is to go for \"defense in depth\".  This certainly won't solve all problems, but it may catch some edge cases/outliers that manage to make it through testing unnoticed, and keep the network semi-healthy until those issues can be fixed.",
        "created_at": "2023-02-28T15:20:29.121000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "it adds complexity without providing much value - clients that fail stay connected do so for a reason: they're not conforming to the protocol, are spamming or buggy, by and large - we can't reliably tell whether the other end is benign or not in doing so, and by giving preference based on user agent would open up for malicious actors to perform the actions that led up to the poor client being prioritised",
        "created_at": "2023-02-28T15:22:14.848000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "That last point is pretty compelling.",
        "created_at": "2023-02-28T15:22:59.977000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "this is \"security hole in depth\", if anything - \"send this user agent, and you get to break the protocol rules\"",
        "created_at": "2023-02-28T15:23:01.149000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "Under the assumption that there exists a user agent that has a reserved slot but a very small number of actual clients on the network (e.g., Lodestar).  Yeah, that is a good argument!",
        "created_at": "2023-02-28T15:24:00.259000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "that said, it's fun to watch the different policies clients have: https://metrics.status.im/d/pgeNfj2Wz23/nimbus-fleet-testnets?orgId=1\u0026var-instance=metal-05.he-eu-hel1.nimbus.mainnet\u0026var-container=beacon-node-mainnet-unstable-01\u0026from=now-2d\u0026to=now\u0026refresh=5m\u0026viewPanel=76 - ie lighthouse for example lets people in and then kicks them, while others only let in new connection if there's a free spot already etc (so after a restart, there's usually lots of lh:s connected, and slowly over time it balances out)",
        "created_at": "2023-02-28T15:25:11.038000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "general",
        "parent": "",
        "content": "^ this is a view into the nimbus pre-production / integration test metrics - ie these are some pretty heavily overloaded nodes which we use to stress the client - it's not necessarily representative of mainnet or anything else",
        "created_at": "2023-02-28T15:27:46.061000+00:00",
        "attachments": null
    },
    {
        "author": ".fjl",
        "category": "general",
        "parent": "",
        "content": "When discv5 is used for peer discovery (the spec mandates it!), then *all* existing nodes can be found through the DHT. You're definitely not limited to the peers known directly to bootnodes or your immediate peers. In theory, the peer set you end up with should come out as a random draw from all live nodes. At least that's what discv5 is designed to provide.",
        "created_at": "2023-02-28T22:18:47.004000+00:00",
        "attachments": null
    },
    {
        "author": ".fjl",
        "category": "general",
        "parent": "",
        "content": "Seems like that is not working in lodestar though.",
        "created_at": "2023-02-28T22:19:21.222000+00:00",
        "attachments": null
    }
]