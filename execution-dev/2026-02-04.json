[
  {
    "author": "zsfelfoldi",
    "category": "general",
    "parent": "",
    "content": "Here are the slides. Please note that this is still just a preview version, I intend to work on it a bit more and present is at EthBoulder.",
    "created_at": "2026-02-04T09:32:04.639000+00:00",
    "attachments": [
      {
        "type": "application/pdf",
        "origin_name": "eip-7745b_presentation_preview.pdf",
        "content": "5021e0f2d35954ac8c4715fd8be312abbb4b7f55a3906abc06e03e2aaa6847a9"
      }
    ]
  },
  {
    "author": "zsfelfoldi",
    "category": "general",
    "parent": "",
    "content": "I am still working on the details of how historic \"big tables\" could be handled, I'm probably going to need some help with the ZK tech. I have the basic idea of how table merging proofs could work but I still miss the practical knowledge. I already checked out the tutorial example of Succinct <@427491045308235776> sent to me and it's cool and I get the basic idea but I still need to learn the technical details (also some Rust knowledge would not hurt either).",
    "created_at": "2026-02-04T09:45:52.085000+00:00",
    "attachments": []
  },
  {
    "author": "zsfelfoldi",
    "category": "general",
    "parent": "",
    "content": "Anyways, it's good that it now comes off as more approachable, I worked really hard to finally achieve that ðŸ™‚ On one hand it feels a bold approach to rely on ZK proofs for historic tables but it really allows a much simpler design for consensus. Also, according to my initial calculations, query proof size can easily be 10-20x better than with the original 7745 design.",
    "created_at": "2026-02-04T09:51:48.617000+00:00",
    "attachments": []
  },
  {
    "author": "zsfelfoldi",
    "category": "general",
    "parent": "",
    "content": "Right now I am working on the more or less efficient implementation (including big historic tables) so we can get some performance numbers but I expect it to be really fast. After all each lookup has a logarithmic cost in terms of table size, and the number of required tables is also more or less logarithmic in terms of chain history length, so both the lookup database read cost and the proof size are log squared, which is pretty good. Also the processing of different tables can be parallelized.",
    "created_at": "2026-02-04T09:58:04.331000+00:00",
    "attachments": []
  }
]