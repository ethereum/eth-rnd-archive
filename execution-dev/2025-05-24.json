[
    {
        "author": "anderselowsson",
        "category": "general",
        "parent": "",
        "content": "Wouldn't hurt to get them involved. A reasonable goal for an FPE is to decrease variability in compute time under parallelization for various block compositions. This allows for higher throughput in the average case but not for the worst case. You can think of it as keeping the gas limit fixed and keeping the worst case as is, but making parallelizable txs cheaper such that more of them fit in a block. \n\nBy the way, it's not clear to me what a cap on tx size is needed in such a design but I have not studied the issue closely.",
        "created_at": "2025-05-24T11:25:41.663000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "Computation is cheap on ETH L1",
        "created_at": "2025-05-24T13:48:43.317000+00:00",
        "attachments": null
    },
    {
        "author": "funnygiulio",
        "category": "general",
        "parent": "",
        "content": "SSTORE is probably the biggest gas guzzler and it is not really that close. I remember when people optimized the shit out of SCs during the NFT/DeFi craze and the biggest gain was to figure out how to squeeze SSTOREs",
        "created_at": "2025-05-24T13:49:15.518000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "440sec L1 trace on Nethermind; 4.4sec is running byte code (1%); of that 81.6% is loading data. Evm execution is mostly bored with only 0.184% of the 440sec running CPU intensive byte code",
        "created_at": "2025-05-24T15:17:28.027000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "6b2c626ba9820a81a3d6749a21d34cff3ca51b26adecf13481f136e1d212fd9f"
            }
        ]
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "BLA with data dropping the data loads",
        "created_at": "2025-05-24T15:24:30.576000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "Yeah the problem is that if you parallelize and pipeline to all cores this creates contention for someone that is serving 1K simultaneous requests and ends up hurting more than helping. I know first hand that people working with RPC indexing services did raise an issue about this",
        "created_at": "2025-05-24T15:24:47.753000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Gaslimit would need to be pretty high to have a serious impact",
        "created_at": "2025-05-24T15:27:26.068000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Not denying that rpc can be quite heavy; and normally runs with a much higher tx gaslimit",
        "created_at": "2025-05-24T15:28:36.211000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "200MGas svg construction and decompression for onchain NFTs for example ðŸ˜‰",
        "created_at": "2025-05-24T15:29:22.511000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Practically will probably be limited by bandwidth before CPU (obvs more to EL side than just running byte code; roots etc; but can also do that in parallel with some forms of BAL)",
        "created_at": "2025-05-24T15:32:18.607000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "Yeah I do agree with that. We seem to be so far away from having execution being anything of a bottleneck on L1 that these things may be irrelevant from now. But I can't fail to see the double standards though. If execution is irrelevant then parallelization is an early optimization that complicates consensus for no reason. If it's actually needed then the RPC provider point comes back to be valid",
        "created_at": "2025-05-24T15:38:18.900000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Well I am running pretty fast storage, so will be slower on common or garden validator; and then roots is also storage, everything slow is basically storage\n\nBAL either most storage is already done or can be parallelized (and NVMe can do *at least 16* requests in parallel); and then cuts down latency, meaning can attest faster etc",
        "created_at": "2025-05-24T15:44:58.391000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "There are blocks that take a longer time; so is more about improving worst cases; as generally we apply limits based on the worst case; not common case; but common case will be what rpc providers will be experiencing mostly for blocks",
        "created_at": "2025-05-24T15:45:58.780000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "So the consensus change is to reduce the huge variance between common case and worst case",
        "created_at": "2025-05-24T15:50:11.331000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "general",
        "parent": "",
        "content": "Ah that's a good argument. Anything that reduces the variance between worse and typical is welcome",
        "created_at": "2025-05-24T15:53:52.904000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Especially since we use worst case to define the limits the protocol should be running at",
        "created_at": "2025-05-24T15:55:26.736000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "general",
        "parent": "",
        "content": "You had me up until this line.  ðŸ˜„  I agree that we should get the worst case down closer to typical case, but once we start talking about cranking up *typical* requirements (which is opened up by reducing worst case requirements) we need to start talking about who will be impacted, how much, etc. I think this is what \u003c@755590043632140352\u003e is getting at.",
        "created_at": "2025-05-24T16:04:19.344000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "I don't mean adjusting requirements; I mean improving what you can safely do with those requirements",
        "created_at": "2025-05-24T16:09:30.068000+00:00",
        "attachments": null
    },
    {
        "author": "ben_a_adams",
        "category": "general",
        "parent": "",
        "content": "Which is why worse case defines things; as is the bound on what is safe (within given requirements). So is about reducing the impact of the worst case",
        "created_at": "2025-05-24T16:10:38.361000+00:00",
        "attachments": null
    }
]