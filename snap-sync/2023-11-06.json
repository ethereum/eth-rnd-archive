[
    {
        "author": "holiman",
        "category": "Execution Layer",
        "parent": "",
        "content": "Here's what \u003c@206016661470314496\u003e had to day about it\n\n\u003e \n\u003e When downloading headers, the only way to do it concurrently is to download fixed chunks, and then piece the chunks together\n\u003e Everything else we can just fill whatever's delivered based on the headr chain\n\u003e but with the header chain. itself, there;s nothing to map it onto\n\u003e so dynamic chunks would make the code very complex\n\u003e we retrieve 512 headers in one chunk\n\u003e We could retrieve less, but then sync gets slow due to latency issues \n\u003e I don't remember how large a header is, but the idea is to try and have 256-512KB network packets\n\u003e downloader docs say a header is 500B\n\u003e so 500 headers == 256KB\n\u003e that seems like a sane default\n\u003e bodies and receipts are large, so capping them makes sense\n\u003e capping headers (at 192) makes a lot less sense imo",
        "created_at": "2023-11-06T14:42:02.182000+00:00",
        "attachments": null
    },
    {
        "author": "suburbandad_",
        "category": "Execution Layer",
        "parent": "",
        "content": "seems reasonable, and it is not a big deal to bump Besu's max header limit.  It was just a surprising requirement.  I will check with folks who know more about the alternative consensus mechanism header sizes to see if the 192 limit is something to do with ibft or qbft.  \n\nhowever, for the sake of testing the PR, I bumped to 512 and it seems to progress.  Geth is dropping besu with a handshake issue, but hopefully will have good results soon.",
        "created_at": "2023-11-06T17:03:22.337000+00:00",
        "attachments": null
    }
]