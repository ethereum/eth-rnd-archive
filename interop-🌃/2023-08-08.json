[
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Feature request for CL teams:\nWould it be possible to make the blob storage datadir configurable with a flag? (Similar to ancients datadir in Geth). The default can ofc be in the same location as the other CL DBs, but having it configurable would help a lot with archiving blobs (You can have an external HDD that stores all the blobs reducing the cost greatly).",
        "created_at": "2023-08-08T08:19:01.430000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Testing",
        "parent": "",
        "content": "We really need to address blob storage (and movement).  The idea of an external blob server should be looked at.  It shouldn't require too much work for the CL teams, in that there would be an API definition for storing and retrieving blobs and a flag with an endpoint.",
        "created_at": "2023-08-08T08:26:59.604000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "Testing",
        "parent": "",
        "content": "Sounds like a separate program for the data availability layer, I guess the main reason we do not discuss it a lot is that it may add to the deployment complexity in the future.",
        "created_at": "2023-08-08T08:37:05.781000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "I think this is the case for us currently, you should be able to provide a separate data directory\ncc \u003c@833706406699073536\u003e",
        "created_at": "2023-08-08T08:37:27.137000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "Testing",
        "parent": "",
        "content": "current Nimbus stores blobs just as part of its main (SQLite) database. This would change the deployment/settings approach. It's of course possible, but nontrivial, especially to support both",
        "created_at": "2023-08-08T08:41:49.094000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "Testing",
        "parent": "",
        "content": "my understanding was the purpose of the short blob lifetime was to keep this constrained as a problem",
        "created_at": "2023-08-08T08:42:23.284000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "We might not need unanimous support of the feature from all clients, but atleast two or three would definitely help the problem of having to archive blobs for the long term. We could of course leave the issue completely to indexers, but i think supporting long term archiving natively has its value as well.",
        "created_at": "2023-08-08T08:45:25.760000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Testing",
        "parent": "",
        "content": "It would be optional, but for some home stakers having a remote blob store may be the only viable option.",
        "created_at": "2023-08-08T08:50:26.586000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "Testing",
        "parent": "",
        "content": "long-term archiving could still happen, the same way as long-term block archiving can (and mostly does, for most CLs by default, I think) happen now: just allocate more storage resources. all of this -- both \u003c@199561711278227457\u003e and \u003c@144468805697929216\u003e 's proposals -- seems like basically storage hierarchy optimization being pushed into CLs and/or the beacon API",
        "created_at": "2023-08-08T08:55:04.453000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "Testing",
        "parent": "",
        "content": "I'm not sure it's a requirement that running an archiving indexer be cheap",
        "created_at": "2023-08-08T08:55:50.312000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "Whole idea is to be able to tell a CL that I want high intensity data on nvme $$$, and low intensity data on HDD $.",
        "created_at": "2023-08-08T08:56:57.560000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "We just thought the most obvious way to achieve this is by directory separation of blob data.",
        "created_at": "2023-08-08T08:57:20.703000+00:00",
        "attachments": null
    },
    {
        "author": "tersec",
        "category": "Testing",
        "parent": "",
        "content": "I suspect the most straightforward way for Nimbus to do this would be to split out blob storage into a separate database which could be stored elsewhere. It's doable either way, just a question of in an overall systems sense the lowest-complexity/risk to put a reasonable functionality (blob archiving support) would be",
        "created_at": "2023-08-08T09:01:07.521000+00:00",
        "attachments": null
    },
    {
        "author": "stefanbratanov",
        "category": "Testing",
        "parent": "",
        "content": "It would require some work from Teku since LevelDB and RocksDB have one folder per DB structure, but we have discussed this and will keep it in mind.",
        "created_at": "2023-08-08T10:15:29.904000+00:00",
        "attachments": null
    },
    {
        "author": "dapplion",
        "category": "Testing",
        "parent": "",
        "content": "Please for next run use Lodestar LTS (v1.10.0) with `--deterministicLongLivedAttnets` and `--network.useWorker`",
        "created_at": "2023-08-08T11:40:06.169000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "In case other client teams have preferences for what we want to change on the next 2M validator test, please do let us know. It should be up today by ~5PM. We're sticking exactly to mainnet distributions this time around, so the distribution found here: https://clientdiversity.org/#distribution",
        "created_at": "2023-08-08T11:47:47.242000+00:00",
        "attachments": null
    },
    {
        "author": "realbigsean",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@199561711278227457\u003e  we have a `--blobs-dir` flag in LH",
        "created_at": "2023-08-08T14:05:15.708000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "We just had genesis on the 2.1M testnet following exact mainnet distributions. \n\nIP addresses can be found here: https://github.com/ethpandaops/holesky-test/blob/master/ansible/inventories/devnet-2m-test/inventory.ini\n\nNetwork configs: https://github.com/ethpandaops/holesky-test/tree/master/network-configs/devnet-2m-test\ngenesis.ssz: https://2m-val-test.fra1.cdn.digitaloceanspaces.com/genesis.ssz",
        "created_at": "2023-08-08T15:24:44.335000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "TLDR:\n```\nEpoch 3:\n  Proposals: 25/32 (78.12%)\n  Attestations: 928136/2100000 (44.20%)\n    Source timely: 867459/2100000 (41.31%)\n    Target correct: 928136/2100000 (44.20%)\n    Target timely: 928136/2100000 (44.20%)\n    Head correct: 897751/2100000 (42.75%)\n    Head timely: 714156/2100000 (34.01%)\n  Sync committees: 10148/12800 (79.28%)\n```\nEven with mainnet distributions and with prysm/lighthouse making up well over majority of the network, we see bad attestation performance and a potentially non-finalizing network as a result",
        "created_at": "2023-08-08T15:35:20.738000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Testing",
        "parent": "",
        "content": "Any thoughts on bringing the number of validators down substantially, perhaps 1.4MM validators, to see how that network would perform?  It would still give us a lot of headroom over mainnet, and a decent amount of time to address whatever scaling limits we find ourselves bumping up against.",
        "created_at": "2023-08-08T15:57:55.385000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "This was basically a last ditch effort to gather data from a more mainnet centric setup. \n\nBased on early results for today I'd even advocate for Holesky being smaller than 1.4M, but I will definitely be testing that before Thursday.",
        "created_at": "2023-08-08T15:59:19.663000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Testing",
        "parent": "",
        "content": "1.4MM active validators is a bit more than double the size of mainnet, which seems a reasonable place to aim for.  If we can't do it, then fair enough, perhaps dial down to 1MM, but I think we need a testnet that sits near the edge of what works today so that a) we avoid changes that will have a significantly detrimental impact on performance and b) we have a testbed for optimizations that will allow the network to scale up.\n\n(Or we add to the protocol a quick-and-dirty hard limit of 1MM active validators so that we don't break mainnet, until we can address whatever the underlying issues may be.)",
        "created_at": "2023-08-08T16:04:40.013000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Yeah, I have way more faith in the 1.4M size - if that one fails as well then we would definitely need to consider this an emergency. I'll also keep the number of nodes the same, reducing keys/host. That should also paint a more realistic picture.",
        "created_at": "2023-08-08T16:08:55.119000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "The 2.1M testnet will stay up till tomorrow morning, please do collect any debug data you want by then",
        "created_at": "2023-08-08T17:02:52.770000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "_none_ of the CL data is high intensity _really_ - all blocks and blobs can be stored on a slow drive, we barely read it at all",
        "created_at": "2023-08-08T17:09:25.286000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "you can run nimbus on a 10yo hdd without problem",
        "created_at": "2023-08-08T17:09:39.380000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "it's only the EL data that needs a high-perf drive to update its massive state db",
        "created_at": "2023-08-08T17:10:07.854000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "also, https://nimbus.guide/era-store.html - we haven't yet decide whether to make blobs part of the era files or separate archives, there are advantages to both / either",
        "created_at": "2023-08-08T17:11:12.415000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "Validator performance for 2.1M test  (ignore that it says goerli) https://grafana.observability.ethpandaops.io/d/participation_rates_dashboard/validator-participation-rates-dashboards?orgId=1\u0026var-network=goerli\u0026from=now-2m\u0026to=now\u0026refresh=10s",
        "created_at": "2023-08-08T19:07:58.551000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "```  ranges: |\n    0-365000: lighthouse-geth\n    365000-495000: lighthouse-besu\n    495000-705000: lighthouse-nethermind\n    705000-1170000: prysm-geth\n    1170000-1340000: prysm-besu\n    1340000-1605000: prysm-nethermind\n    1605000-1615000: lodestar-geth\n    1615000-1620000: lodestar-nethermind\n    1620000-1625000: lodestar-besu\n    1625000-1680000: nimbus-geth\n    1680000-1700000: nimbus-besu\n    1700000-1730000: nimbus-nethermind\n    1730000-1920000: teku-geth\n    1920000-1990000: teku-besu\n    1990000-2100000: teku-nethermind```",
        "created_at": "2023-08-08T19:08:24.044000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "not able to login with github",
        "created_at": "2023-08-08T19:59:57.174000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "It is user/pass",
        "created_at": "2023-08-08T20:00:12.430000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "Dm me if you canâ€™t find your password/username",
        "created_at": "2023-08-08T20:04:23.454000+00:00",
        "attachments": null
    }
]