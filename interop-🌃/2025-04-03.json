[
    {
        "author": "ethpandaops-devnet-pulse-v2",
        "category": "Testing",
        "parent": "",
        "content": "",
        "created_at": "2025-04-03T01:00:00.437000+00:00",
        "attachments": null
    },
    {
        "author": "serenita_io",
        "category": "Testing",
        "parent": "",
        "content": "I added some more metrics on Hoodi attestation performance to my gist, see https://ptb.discord.com/channels/595666850260713488/1355135232399114240/1357248844487856150",
        "created_at": "2025-04-03T07:03:04.658000+00:00",
        "attachments": null
    },
    {
        "author": "samcm",
        "category": "Testing",
        "parent": "",
        "content": "In case it's relevant for ACD and locking in a Pectra mainnet date, I've also been doing some extra analysis in an attempt to track down the missing `head timely %`  that we're seeing on Hoodi. To recap, this metric tracks if the attestation's head vote is correct AND it gets included in the very next block. Shoutout ethdo üôè \n\nA recent Hoodi epoch:\n```\nEpoch: 3786\nParticipation: 87%\nHead Correct: 87%\nHead Timely: 74%\n```\n\nSo where are we losing **13%**?\n- Erigon's validator keys are struggling to produce timely attestations. 50k keys. **LESS 5%.**\n- Similar situation for 50% of Besu's validator keys. 25k keys. **LESS 2.5%**\n- Nimbus-produced blocks struggle to include `inclusion_distance=1` attestations. 50k keys.** LESS 5%. **\nTotal: **12.5%**.\n\nAre we happy with this explanation? I've seen some epochs with a disconnect of around 20% -- is this within the margin of error given the above? Are any CL teams still working on their packing/aggregation?",
        "created_at": "2025-04-03T08:32:05.729000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "head-timely.png",
                "content": "b85cc5fbab618b5a6a8d6168f288cffb2634259e10388a2fc2e73605f64a0366"
            }
        ]
    },
    {
        "author": "tersec",
        "category": "Testing",
        "parent": "",
        "content": "Nimbus is still working on this",
        "created_at": "2025-04-03T08:34:38.678000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Testing",
        "parent": "",
        "content": "Great analysis.  It's worth pointing out that Erigon and Besu are execution layer clients and should be uninvolved with generating attestations themselves, so there is still a question of why these entities are struggling to produce timely head votes.  Is it an issue with their infrastructure, their consensus client choice, or something else?",
        "created_at": "2025-04-03T08:50:10.713000+00:00",
        "attachments": null
    },
    {
        "author": "samcm",
        "category": "Testing",
        "parent": "",
        "content": "Besu's problems are just infrastructure, not sure about Erigon",
        "created_at": "2025-04-03T08:52:17.644000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Erigon runs caplin as the beacon and lighthouse as vc last i heard? cc \u003c@1146819096831340584\u003e \u003c@976542403269910539\u003e",
        "created_at": "2025-04-03T08:52:38.241000+00:00",
        "attachments": null
    },
    {
        "author": "somnergy",
        "category": "Testing",
        "parent": "",
        "content": "No, that's not the case. Erigon had a problem with producing blocks with execution requests, since then we have been running LH as a beacon, LH VC as a validator, and Erigon as an EL in Hoodi. It still squaks \"erigoncaplin c1hoodi\" - which is incorrect cc \u003c@583892532644151312\u003e",
        "created_at": "2025-04-03T09:04:31.758000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Ah nice! So its really unclear why it underperforms then... cc \u003c@84562395988508672\u003e . Was this change done at some specific time?",
        "created_at": "2025-04-03T09:07:50.446000+00:00",
        "attachments": null
    },
    {
        "author": "somnergy",
        "category": "Testing",
        "parent": "",
        "content": "It seems like there is a bottleneck at the VC, since we have been running 37K keys on one VC, but none of the instances are strained of resources. LH VC was always running, LH BN was started around 3 hours into pectra fork on hoodi - so round 5PM UTC on 26 Mar\n\nMoreover around 13K keys for Erigon were not imported, since handover from EF Devops",
        "created_at": "2025-04-03T09:10:42.738000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "It could just be that its too many validators's in one vc, besu has the same issues with 25k validators on one vc.",
        "created_at": "2025-04-03T09:12:34.816000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "any reason you guys run so many keys on a single vc? If you don't wanna run multiple vc's maybe consider doing some consolidations",
        "created_at": "2025-04-03T09:20:09.192000+00:00",
        "attachments": null
    },
    {
        "author": "somnergy",
        "category": "Testing",
        "parent": "",
        "content": "No technical reason. We might consider consolidations and running multiple vc's with other CLs - cc \u003c@583892532644151312\u003e",
        "created_at": "2025-04-03T09:21:17.716000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "We generally advice to run around 5k keys per validator.",
        "created_at": "2025-04-03T09:21:52.181000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Consolidations would definitely take weeks at this scale, so for now splitting across multiple vcs is likely the way to go",
        "created_at": "2025-04-03T09:26:03.899000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Testing",
        "parent": "",
        "content": "(Or use Vouch with Dirk, which will happily handle 50K keys on a single instance.)",
        "created_at": "2025-04-03T09:27:32.137000+00:00",
        "attachments": null
    },
    {
        "author": "nflaig",
        "category": "Testing",
        "parent": "",
        "content": "Thanks for confirming this, that's what I meant on Monday in the testing call but was unsure if it's maybe just Lodestar doing something wrong but seems network wide issue, we have a internal metric for this but it hasn't really changed in the last 7 days but we should quickly see it there if things get better",
        "created_at": "2025-04-03T09:33:10.371000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "e34aae8c15fa19dd6a5f42307fd03e2b6ecaa136d89f7ac92afa7b97e1219a7c"
            }
        ]
    },
    {
        "author": "somnergy",
        "category": "Testing",
        "parent": "",
        "content": "I suppose this would be the problem on mainnet too for the first few weeks (even months). And the big fishes would be aware of it, so we will have a swarm of consolidation requests 2hours before the fork",
        "created_at": "2025-04-03T10:17:18.594000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "I dont think anyone runs close to 25k validators on one host, its usually too much of a risk and when you have so much capital you can run more sane numbers like ~100-1000 per host üòÑ",
        "created_at": "2025-04-03T10:20:22.763000+00:00",
        "attachments": null
    },
    {
        "author": "somnergy",
        "category": "Testing",
        "parent": "",
        "content": "This is a tests replay for all the 100% filled queue tests that we had (with assertoor and testnets). I suppose consolidation requests would be the most economically lucrative thing this hard fork. There would be maybe 1000x surge in bad consolidation requests, and in general, maybe 20x more consolidation request transactions in the pool with huge tips.\nOne maybe happy to pay $50 in priority fee, if they can get their overall cloud infrastructure costs cut in a third. And they will because they are charged in time, and the earlier it is, better it is. This may create a sybil situation on the first day of the fork - high fees!",
        "created_at": "2025-04-03T10:20:26.118000+00:00",
        "attachments": null
    },
    {
        "author": "somnergy",
        "category": "Testing",
        "parent": "",
        "content": "Even if you are running 10, and you now want to consolidate into 3, that's 7 requests! If there are 100,000 validators who wish for this, it's 70,000. Or am I being paranoid?",
        "created_at": "2025-04-03T10:21:49.815000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Yeah that is fair! There likely is going to be a rush of consolidations soon after the fork and another one once larger validators have had a chance to observe risks and do their testing.",
        "created_at": "2025-04-03T10:26:45.551000+00:00",
        "attachments": null
    },
    {
        "author": "siladu",
        "category": "Testing",
        "parent": "",
        "content": "I'd say it's a mix of infra and client choice. \nbesu-teku seems to run 25K keys fine on the same infra, besu-lighthouse is struggling.",
        "created_at": "2025-04-03T10:42:14.025000+00:00",
        "attachments": null
    },
    {
        "author": "mm.999",
        "category": "Testing",
        "parent": "",
        "content": "You have access to the EL/BN machine: stop erigon and restart it with a new graffiti which doesn‚Äôt have the ‚Äúcaplin‚Äù substring.",
        "created_at": "2025-04-03T11:01:41.237000+00:00",
        "attachments": null
    },
    {
        "author": "mm.999",
        "category": "Testing",
        "parent": "",
        "content": "Absolutely. \n\nStill 37K keys managed by our VC instance makes lighthouse use slightly less than 1GB RAM and vCPUs are never under stress. So I don‚Äôt (currently) understand why the statement  ‚Äúwe have too many keys on our VC instance‚Äù can be said. Emotionally I can agree with that, yet there‚Äôs currently nothing which is an evidence of that.",
        "created_at": "2025-04-03T11:05:51.817000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "its very likely because there's a large number of actions that need to be performed in a short duration (when a slot comes in and so on). So it may not track as high load all the time since its very spiky.",
        "created_at": "2025-04-03T11:08:27.963000+00:00",
        "attachments": null
    },
    {
        "author": "siladu",
        "category": "Testing",
        "parent": "",
        "content": "Indeed we don‚Äôt see obvious issues in our monitoring and lighthouse appears to use less resources than teku but still performs worse",
        "created_at": "2025-04-03T11:10:37.453000+00:00",
        "attachments": null
    },
    {
        "author": "mm.999",
        "category": "Testing",
        "parent": "",
        "content": "OK. We have 50K keys available to us and I have a very simple question: should all 50K keys be activated? If yes, then multiple VCs will be run (next week).",
        "created_at": "2025-04-03T11:11:29.447000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "yes.",
        "created_at": "2025-04-03T11:11:52.568000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "we handed over 50k keys, so all 50k should be active.",
        "created_at": "2025-04-03T11:11:58.802000+00:00",
        "attachments": null
    },
    {
        "author": "mm.999",
        "category": "Testing",
        "parent": "",
        "content": "OK. So next week we‚Äôll split our keys over multiple VCs. Besides I think that we‚Äôd better have multiple EL/CL machines too (1:1 relationship between VCs and EL/CLs). What do you think?",
        "created_at": "2025-04-03T11:17:21.267000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Yeah i think a lot of client teams (and our setup) involves more clients. The idea is that the testnet setup helps client teams analyze issues before they roll out to mainnet, so a lot of them run beta builds or soak releases before they go out and hence catch bugs.",
        "created_at": "2025-04-03T11:18:47.696000+00:00",
        "attachments": null
    },
    {
        "author": "pk910",
        "category": "Testing",
        "parent": "",
        "content": "I've been curious why the deposit queue clears up waaay slower than dora predicts on holesky..\nTurned out, there is not only the churn limit (128 ETH/ epoch),  but also a limit on the number of requests processed per epoch..\nThe later is set to `16` (`MAX_PENDING_DEPOSITS_PER_EPOCH`), which is fine when assuming 32 ETH deposits, but gets crazy when doing 1 ETH top up deposits.\nwe effectively have a churn limit of 16 ETH per epoch when there are only 1 ETH top up deposits.\n\nIs that behavior intended?  It will take a looong time to clear up the deposit queue on holesky, as we did top up deposits (1 ETH each), for ~700k validators üòÖ",
        "created_at": "2025-04-03T11:59:04.774000+00:00",
        "attachments": null
    },
    {
        "author": "mariusvanderwijden",
        "category": "Testing",
        "parent": "",
        "content": "cc \u003c@539495253418180618\u003e \u003c@592004585506340885\u003e",
        "created_at": "2025-04-03T12:13:59.366000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "(this value is the same on mainnet and holesky, i.e: `MAX_PENDING_DEPOSITS_PER_EPOCH: 16`)",
        "created_at": "2025-04-03T12:15:37.628000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "interesting, but we are not expecting a ton of 1 ETH deposits on mainnet. so it should be fine, this mechanism introduced to limit a number of sig verifications per epoch to reduce DoS vector when someone submits 256 1 ETH deposits with new pubkey each",
        "created_at": "2025-04-03T13:37:15.866000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "it could be a griefing vector though? Since I can use capital to fill up the deposit queue and exits would get processed faster than the deposits (the exit queue has no such pending limitation afaik). i.e, 16ETH goes in, 128ETH goes out",
        "created_at": "2025-04-03T13:47:31.532000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "I guess the aim of such an attack is kinda lacking meaning, eventually the in/out churn will reduce and be equal.",
        "created_at": "2025-04-03T13:49:18.415000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "imbalance between exit and activation queues shouldn't be a problem. not sure if any attacker will be willing to run this griefing scenario for significant amount of time considering ETH require to cover deposits and transaction costs",
        "created_at": "2025-04-03T14:18:38.258000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "The other attack scenario would be gatekeeping, instead of submitting a deposit of 32 ETH one can submit 32 x 1 ETH deposits and prevent anyone else to be activated during two epochs. It can be done in a single transaction to reduce the cost. But gatekeeping doesn't seem to be anyhow profitable at the size of the mainnet validator set\n\nI think we can increase this parameter at the next HF if we observe anything like that on a mainnet",
        "created_at": "2025-04-03T14:21:25.463000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "wdyt?",
        "created_at": "2025-04-03T14:44:14.357000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "Yeah possibly bumping it in fulu makes sense imo",
        "created_at": "2025-04-03T14:44:55.184000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Yeah i think if we notice stuff we can bump values at the next HF",
        "created_at": "2025-04-03T14:44:56.432000+00:00",
        "attachments": null
    }
]