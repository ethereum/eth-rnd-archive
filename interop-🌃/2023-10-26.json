[
    {
        "author": "ryanleeschneider",
        "category": "Testing",
        "parent": "",
        "content": "It makes some intuitive sense to me that 6 would be more common than 5, but I wonder why four is so prevalent?",
        "created_at": "2023-10-26T01:05:41.321000+00:00",
        "attachments": null
    },
    {
        "author": "savid",
        "category": "Testing",
        "parent": "",
        "content": "possibly question for pari/barnabas but looks like the last 12 hours is more evenly distributed, with 5 blob sidecars still lacking",
        "created_at": "2023-10-26T03:09:52.899000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "c0a8f2d3495792540956539721e2b6075b29e87aa4c261a073e5163cead25d39"
            }
        ]
    },
    {
        "author": "savid",
        "category": "Testing",
        "parent": "",
        "content": "this is last 12 hours of this graph",
        "created_at": "2023-10-26T03:11:08.092000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "476843b9efe0293e19f4f81c1a7abce6ec738c4b584dcc866147ca6584a4bc42"
            }
        ]
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "We tuned the spammer tools to target 3, to showcase a network that always functions at the target blob rate. We used two separate tools though, goomy and txfuzz. It could be that the sum of the two trying to estimate things overshot by one blob very often ðŸ™‚",
        "created_at": "2023-10-26T06:13:37.487000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "We switched to all out spamming last evening. So 6 being extremely prevalent now makes sense ðŸ˜„",
        "created_at": "2023-10-26T06:14:20.749000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Blobber seems to have knocked prysm-geth-5 out \u003c@476250636548308992\u003e (assuming you're the only one awake rn). \n\n`ssh devops@104.248.237.199`\nhttps://dora.dencun-devnet-10.ethpandaops.io/slots/filtered?f\u0026f.missing=1\u0026f.orphaned=1\u0026f.pname=prysm-geth-5\u0026c=50\u0026s=1\n\nit got knocked out almost a few hours after the blobber was started. It could be that the blobber can knock out the other prysms too, but the proposal/gossip path might not be optimal for it.",
        "created_at": "2023-10-26T07:50:12.624000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "I updated my google sheet with more data",
        "created_at": "2023-10-26T08:43:30.356000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "d1398b6fc36fa6346e21418147e59ecc8d2f2b2069610b5c16dd9dee17354e63"
            }
        ]
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@199561711278227457\u003e are the blocks full normal txs wise as well or just blobs being spammed?",
        "created_at": "2023-10-26T08:47:31.478000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "tx fuzz is running too, so blocks should be quit full. i indeed see some blocks with 200+ tx and some with like 7.",
        "created_at": "2023-10-26T08:50:08.073000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@654267572107083777\u003e nimbus-besu-1 seems floppy too: https://dora.dencun-devnet-10.ethpandaops.io/slots/filtered?f=\u0026f.pname=nimbus-besu-1\u0026f.missing=1\u0026f.orphaned=1\u0026c=50",
        "created_at": "2023-10-26T08:51:19.486000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "I am off my machine right now but can look in about an hour",
        "created_at": "2023-10-26T08:53:19.394000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "seems like nimbus besu recovered?",
        "created_at": "2023-10-26T10:39:25.630000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "seeing similar percentile arrival times on lighthouse",
        "created_at": "2023-10-26T12:07:28.615000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "We've been stressing devnet-10 for the last 12+h. This included more blobs and blobber sending invalid blobs. Results of the analysis can be found here: https://notes.ethereum.org/@ethpandaops/devnet-10-stress-analsysis\n\nas always, reach out with questions and lemme know if you can't access the raw data/grafana dashboards. \n\ncc \u003c@491624610215886849\u003e \u003c@504202741933932544\u003e \u003c@792822129019584562\u003e to co-relate with your own latency observations",
        "created_at": "2023-10-26T12:45:40.483000+00:00",
        "attachments": null
    },
    {
        "author": "terence0083",
        "category": "Testing",
        "parent": "",
        "content": "Looking at this now!",
        "created_at": "2023-10-26T13:04:40.559000+00:00",
        "attachments": null
    },
    {
        "author": "terence0083",
        "category": "Testing",
        "parent": "",
        "content": "Ok I believe we have identified the bug. Fixes coming in the next few hours",
        "created_at": "2023-10-26T13:07:50.545000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "thanks!",
        "created_at": "2023-10-26T13:07:54.533000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@792822129019584562\u003e re your estimation for mainnet given the arrival times on devnet-9/10, the current mainnet arrival times are a function of builders waiting for as long as possible before responding with the blinded payload and the disseminating the block. So theoretically, we should be able to have higher blobs inspite of the added latency if we assume that builders would respond quickly (at the expense of missing out mev oppurtunities)",
        "created_at": "2023-10-26T13:11:10.354000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "so following are the points of dicussion mainly (since teku, lighthouse and pari/devops' analaysis doesn't diverge from the latency data seen by lodestar, so lodestar big block performance is not an issue here and ideally should also not be because lodestar latency data was \"seen\" data prior any processing)\n\n1. Why are the blobs not being transmitted concurrently. This is one of the main arguments  was that blobs will be transmitted concurrently with mainnet blocks. But this data doesn't validate that since block p95 latency seems small as compared block 6 blobs p95 reception latency. So where is the gap - is it inter node bandwidth ( \u003c@199561711278227457\u003e ?) or because clients aren't transmitting concurrently ? Note that most of the nodes in devnet 10 are non mev and hence mev-effect is not yet introduced.\n\n2. mainnet blocks don't come early, and as per lodestar latency analysis roughly around 3.5 sec for p95. Now this could also be because of mev wait till proposal since as the blocks are of same full nature as mainnet (again \u003c@199561711278227457\u003e ? ðŸ˜† ). As per my observed data, mev produce block resolve around 1-2 sec into the slot, and how blobs will change that timeline might totally be dependent on mev infra. And add to it the latency that could come on mev blobs publishing. so yes \u003c@491624610215886849\u003e that is going to be a significant factor.\n\n3. May be the resolution is to *optimistically* import the block while blobs are still reaching and is a valid solution. lodestar currently doesn't do that, not sure how many CLs do \u003c@491624610215886849\u003e \u003c@504202741933932544\u003e ?",
        "created_at": "2023-10-26T13:52:01.559000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "each node has 1G symmetric network, so assume perfect bandwidth situation ðŸ˜„",
        "created_at": "2023-10-26T13:53:44.749000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "re 1, I don't think lighthouse starts transmitting the blobs until the block is flood published to all available peers first. So we don't have concurrency in publish afaik",
        "created_at": "2023-10-26T13:54:20.534000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "why wouldn't you full publish them together",
        "created_at": "2023-10-26T13:55:10.417000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "thats what lodestar does ... (or what underlying nodejs engine should do)",
        "created_at": "2023-10-26T13:55:39.536000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "when you say you publish them together, do you mean you call gossipsub publish in multiple async tasks for each message (block and every blob)",
        "created_at": "2023-10-26T13:59:02.887000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "yes",
        "created_at": "2023-10-26T13:59:16.864000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "till the data is converted into bytes, nodejs will do that sequentially (even if we create multiple async promises and await all together) but after handoff to the nodejs network layer the actual transmission should be all concurrent",
        "created_at": "2023-10-26T14:00:16.663000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "i haven't looked into how viable doing this in current rust-libp2p is. Will get back to you on this",
        "created_at": "2023-10-26T14:02:15.357000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "re (3), we don't do optimistic import. But we cannot attest on  optimistically available data so not sure how that would be helpful",
        "created_at": "2023-10-26T14:05:22.886000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "CL clients (maybe not all of them) can't revert a block from the fork choice which is an obstacle for optimistic import",
        "created_at": "2023-10-26T14:06:36.195000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "basically idea is while you run block through validations and may be some signficant time in validation by EL, the blobs can still come in without affecting the overall import times",
        "created_at": "2023-10-26T14:08:45.225000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "and one can still give proposer boost on the seen time of the block if all the blobs become available till one can fully validate the block",
        "created_at": "2023-10-26T14:09:17.625000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "ohh you mean optimistic validation. Lighthouse does try to complete all block validations (including EL verification) while waiting for the blobs",
        "created_at": "2023-10-26T14:11:59.578000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "IMHO, a more optimal strategy would be to send a block and each blob concurrently to different peers as the first step, the faster the network will see all blobs the lower dissemination time should be. Tho, one of those blobs could be sent to a slow peer thus a better first step might be sending a block and each blob to different couples of random peers",
        "created_at": "2023-10-26T14:19:00.385000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "The big block mainnet case suggests another strategy: send block and all blobs to one peer, then send this bundle to another one and so on",
        "created_at": "2023-10-26T14:22:36.084000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@425567222635954178\u003e's simulation of an ideal pubsub suggests that these two strategies are more or less equal",
        "created_at": "2023-10-26T14:27:47.792000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "that's my intuition as well. Spread out the block + blobs to the network as quickly as possible. We don't have this functionality in rust-libp2 because we haven't had the need for publishing multiple large messages in a short duration",
        "created_at": "2023-10-26T14:30:51.449000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "do you have a link to that analysis btw? can't find it now",
        "created_at": "2023-10-26T14:37:11.398000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@425567222635954178\u003e could you please share the link",
        "created_at": "2023-10-26T14:39:46.732000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "",
        "created_at": "2023-10-26T14:47:41.522000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "image.png",
                "content": "93012e1caa3b9921f07d3fa8f6cf025bacb5b1bb75b0f7b45f773be7d7b185ab"
            }
        ]
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "not big differences",
        "created_at": "2023-10-26T14:48:16.504000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@491624610215886849\u003e tell me one thing, which delay you use to decide to give proposer boost? block or all data?",
        "created_at": "2023-10-26T14:51:48.044000+00:00",
        "attachments": null
    },
    {
        "author": "nashatyrev",
        "category": "Testing",
        "parent": "",
        "content": "Here is some analysis but it's not too rigorous: https://hackmd.io/WvGbtlgrT22RJpJ5sV248Q#Ideal-pubsub-algorithm-Case-2-decoupled-message\nThere is no reasoning about the case we are discussing, but in that model publishing block+blobs sequentially should yield better latency in my mind",
        "created_at": "2023-10-26T14:52:24.257000+00:00",
        "attachments": null
    },
    {
        "author": "pawandhananjay",
        "category": "Testing",
        "parent": "",
        "content": "block + blobs have to be available for proposer boost iirc. \u003c@607055410104500244\u003e ?",
        "created_at": "2023-10-26T14:52:57.099000+00:00",
        "attachments": null
    },
    {
        "author": "realbigsean",
        "category": "Testing",
        "parent": "",
        "content": "yes all data",
        "created_at": "2023-10-26T14:53:18.837000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@792822129019584562\u003e we intrinsically serialize messages to be gossiped in single threaded way but we are talking about 1ms for doing so for all blobs. The networking layer is async and should go in parallel. I'll dig into it a little bit more but should be fine.",
        "created_at": "2023-10-26T16:17:31.646000+00:00",
        "attachments": null
    }
]