[
    {
        "author": "djrtwo",
        "category": "Testing",
        "parent": "",
        "content": "Hm, I see that",
        "created_at": "2022-03-21T00:03:26.416000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "Testing",
        "parent": "",
        "content": "But being able to query N seems... Valuable",
        "created_at": "2022-03-21T00:03:33.642000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "Testing",
        "parent": "",
        "content": "Either automatically up front",
        "created_at": "2022-03-21T00:03:40.223000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "Testing",
        "parent": "",
        "content": "Or by hand, \"go to your favorite explorer and check X\"",
        "created_at": "2022-03-21T00:03:55.024000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "Yeah I definitely think doing the verification is important, but technically that's doable today, but the usability side of it is so bad I can't imagine many people using it.",
        "created_at": "2022-03-21T00:08:49.509000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "Right now the easiest way I find to verify is actually to check you wind up at the right head block by manual comparison.",
        "created_at": "2022-03-21T00:09:35.769000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "A simple standard API *might* help if it means block explorers would expose APIs to get the state and to check.  If they didn't require an API key it would then be simple to give instructions which have all the providers we know about and the node can randomly select one to get the state from and then verify against the others and the user just has to copy/paste in the URLs.\n\nBut if they all require separate API keys it all becomes too hard very very quickly.",
        "created_at": "2022-03-21T00:11:47.346000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "Testing",
        "parent": "",
        "content": "Right. It might be that many would serve the root request without an API key",
        "created_at": "2022-03-21T00:13:31.997000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "Testing",
        "parent": "",
        "content": "Then some might gate the state",
        "created_at": "2022-03-21T00:13:39.759000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "Testing",
        "parent": "",
        "content": "Not sure though",
        "created_at": "2022-03-21T00:13:42.600000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "Yeah I suspect a simple API to check the state is likely to be made widely available and shouldn't be subject to abuse.  It may need to be quite limited so that it doesn't provide any other function that people would then abuse - potentially even just a call that provides (slot, block_root) and return true/false. Hopefully we could be more generic and provide slot and get block_root back though.\n\nProbably worth talking to the infrastructure providers (e.g. block explorers, infura etc) and see what they'd be happy to provide without an API key and what they wouldn't.  Then we can go from there.",
        "created_at": "2022-03-21T00:16:11.990000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Testing",
        "parent": "",
        "content": "Does tooling exist to make this easy to do (provide Teku's latest finalized state via URL or IPFS)?",
        "created_at": "2022-03-21T01:32:52.585000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "Yes, the standard REST API includes an API that can return the state as SSZ (it is under the debug category which is unfortunate...).",
        "created_at": "2022-03-21T01:33:54.943000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "I wouldn't recommend random users try to make this available publicly but I wouldn't recommend getting the state from some random user anyway.",
        "created_at": "2022-03-21T01:34:50.099000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Testing",
        "parent": "",
        "content": "It just returns a multi GB SSZ response over HTTP?",
        "created_at": "2022-03-21T01:34:58.498000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "It's not multi-GB.  The beacon chain state is quite small.",
        "created_at": "2022-03-21T01:35:15.745000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Testing",
        "parent": "",
        "content": "Oh, how big and what is the growth rate (post merge)?",
        "created_at": "2022-03-21T01:35:34.239000+00:00",
        "attachments": null
    },
    {
        "author": ".paulharris",
        "category": "Testing",
        "parent": "",
        "content": "last time i dumped it was around 19MB",
        "created_at": "2022-03-21T01:35:42.867000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "45M currently for MainNet.  It grows as more validators are added but the merge makes no difference to it.",
        "created_at": "2022-03-21T01:35:58.368000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Testing",
        "parent": "",
        "content": "So one can imagine a simple tool that just fetches and serves that every finalized block I guess.  That isn't too bad.",
        "created_at": "2022-03-21T01:36:43.666000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "Indeed... In fact I used to be that tool: https://github.com/ajsutton/eth2-states",
        "created_at": "2022-03-21T01:37:46.813000+00:00",
        "attachments": null
    },
    {
        "author": ".paulharris",
        "category": "Testing",
        "parent": "",
        "content": "having an endpoint that effectively got `states/finalized` and then set a cache time of something like 6 hours might be not a bad implementation, if it could be automagically validated as part of that",
        "created_at": "2022-03-21T01:40:10.586000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Testing",
        "parent": "",
        "content": "I would do `states/\u003chash\u003e` and then have `states/finalized` do a temporary redirect to the most recent finalized `states/\u003chash\u003e`.",
        "created_at": "2022-03-21T01:43:23.947000+00:00",
        "attachments": null
    },
    {
        "author": "sky.cactus",
        "category": "Testing",
        "parent": "",
        "content": "Can throw some metadata in the header, like block number and timestamp that it is a post-state for.",
        "created_at": "2022-03-21T01:45:10.584000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@!361447803194441738\u003e thanks for your explanations about the current bottlenecks of beacon sync (BLS verification + SHA2 hashing). I will assume this is correct unless we do profiling ourselves and prove otherwise ðŸ™‚",
        "created_at": "2022-03-21T08:38:23.391000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "No worries, thanks for following up.\n\nWe also spend time writing to disk which Iâ€™m sure can be improved. Weâ€™re working on DB schemas to address this and other issues, but I think they key takeaway here is that forward-sync speeds for CEs arenâ€™t as critical as they are for EEs. CLs *must* sync from some recent checkpoint (due to weak subjectivity attacks) so thereâ€™s limited value in optimising forward sync. It took some time for CE teams to get their head around this. I think Teku was the first to fully grasp this, to their credit.\n\nWhen it comes to going from empty DB to head, EE sync times will be orders of magnitude slower than the CE (thanks solely to checkpoint sync). Iâ€™m not up to date on the latest full-chain-sync times for EEs and CEs, but I understand theyâ€™re similar. Since we donâ€™t do Merkle hashing during backfill, the primary bottleneck there is BLS. \n\nAlthough I was trying to convince readers that weâ€™ve spent time optimising forward sync and that weâ€™ve got all the low hanging fruit we know of, I think the key message is that the real-world returns on optimising forward sync for CEs are marginal. The tasks that CEs and EEs perform are quite different and what would be a deal-breaker for an EE (e.g., slow forward sync) isnâ€™t necessarily a deal-breaker for a CE.",
        "created_at": "2022-03-21T09:12:07.763000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Testing",
        "parent": "",
        "content": "thanks again, I do agree with the approach of optimising only the things that actually matter. Also, I do realise that mindset shift is often required for people to understand each other's designs",
        "created_at": "2022-03-21T09:17:42.861000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "Regarding SHA2 improvements, I've spent my good part of the summer writing assembly libraries that we'll use in prysm, but not in initial sync, do you think hashing can be paralellized in initial sync in the sense that we can lay out a large buffer with all chunks we need to have hashed first and then pass to the hasher?",
        "created_at": "2022-03-21T10:20:49.216000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "Iâ€™m not sure I follow, but merkle hashing is tricky in that the input to some hashes canâ€™t be known until youâ€™ve computed others (e.g., you canâ€™t know the input values for two hashes until you know the hashes of their four children)",
        "created_at": "2022-03-21T10:32:27.431000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "LH does parallel hashing of separate trees (e.g., two different Validator objects or the block roots and the state roots)",
        "created_at": "2022-03-21T10:33:53.820000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "yeah this is a different issue that perhaps you haven't followed, I've talked to Sproul about this",
        "created_at": "2022-03-21T10:35:16.881000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "This is poorly described and Iâ€™m struggling to do better, LMK if it does make sense ðŸ˜…",
        "created_at": "2022-03-21T10:35:25.221000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "The issue is that currently all implementations of hashing hash 2 chunks at a time",
        "created_at": "2022-03-21T10:35:38.304000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "while most modern CPUs can do at least 8 pairs at the same time on each core",
        "created_at": "2022-03-21T10:35:53.167000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "Oh yeah, I recall this. Youâ€™re right that I havenâ€™t been following it and that Michael (Sproul) is across it.",
        "created_at": "2022-03-21T10:36:22.162000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "we've finally implemented this on prysm for state hashing, on AVX512 it's 12 times faster on large lists",
        "created_at": "2022-03-21T10:37:31.254000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "so for example to hash the validator registry we can do this about 10 times faster than the usual approach",
        "created_at": "2022-03-21T10:37:55.504000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "but the problem is that I wouldn't know how to use this on initial sync",
        "created_at": "2022-03-21T10:38:06.935000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "this requires laying out as many chunks as you want to hash to start with",
        "created_at": "2022-03-21T10:38:21.631000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "For prysm it doesn't really matter, I assume it is the same for other clients.",
        "created_at": "2022-03-21T10:38:33.188000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "You would still use the same hashing routines where its initial sync/normal sync",
        "created_at": "2022-03-21T10:38:52.760000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "yeah sure, but the point is that hashing a single block is not that much of a difference (execpt the execution payload perhaps if it's large) but if we are guaranteed to need to hash several blocks that we are receiving in a batch, perhaps we can use a trick like your validator list to lay them out contiguously and hash them all at the same time",
        "created_at": "2022-03-21T10:40:02.683000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "We still need to hash the state when doing initial sync, does that help?",
        "created_at": "2022-03-21T10:40:45.841000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "Definitely",
        "created_at": "2022-03-21T10:40:54.254000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "yeah, any large list is incredibly faster",
        "created_at": "2022-03-21T10:40:59.158000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "especially for lists like balances",
        "created_at": "2022-03-21T10:41:05.599000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "for just two chunks is still faster, 20%--30% faster",
        "created_at": "2022-03-21T10:41:30.307000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "for a different reason",
        "created_at": "2022-03-21T10:41:55.589000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "I think yes, for hashing blocks maybe a bit more thought will have to go through it but if it is a big enough bottleneck you can think of tricks to to make it conducive for hashing using the vectorized routines",
        "created_at": "2022-03-21T10:42:19.215000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "Michael has been working on representing the BeaconState as a data-sharing Merkle tree, like Proto was doing some time ago. Perhaps this is helpful there. Iâ€™ll let Michael chime in if is.",
        "created_at": "2022-03-21T10:42:32.748000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "I would say that if during lock-step Erigon's performance is near to `20ms` per block then it might not make sense to wait until CL is reaching the head and then do batch sync -- it won't speed up the overall sync process if a node is catching up after a restart. There should be an equilibrium point of when it start to make sense to use batch sync in favour of lock-step in this case. Inputs to finding this point are block processing time of CL, EL and a number of blocks to reach the head.\n\nIf a node is syncing from scratch then the following approach to batch sync is applicable:\n1) reverse sync headers from a weak subjectivity checkpoint down to genesis\n2) download block bodies, and at the same time\n3) wait for CL to reach out the head and then reverse download headers from the head down to WS checkpoint header\n4) block bodies downloader will seamlessly pull block bodies of these recent blocks as well\n\nIn my mental model (2) is unlikely to happen faster than (3) on Mainnet. But if CL always starts near the head it might not worth the complexity to do this double step reverse header sync. And batch-syncing to the checkpoint + lock-stepping thereafter might not be much slower than batch-syncing to the head + lock-stepping thereafter. As there will be say 100k blocks (2 weeks worth of blocks, it will probably be much less) between the checkpoint and the head by that time, and for EL that is processing a block in `20ms` it will take 30 minutes in lock-step. And (blind-guessing) probably 10 minutes in batch-sync. The net diff is 20 minutes.",
        "created_at": "2022-03-21T14:27:54.801000+00:00",
        "attachments": null
    },
    {
        "author": "timbeiko",
        "category": "Testing",
        "parent": "",
        "content": "ðŸ“£ ðŸ“£ ðŸ“£ \n\nHeads up: after discussion in a cross-client testing call, we've decided to restrict **posting** in this channel to client teams, researchers, and a handful of actively involved community contributors ( \u003c@!301186049323958275\u003e, \u003c@!144468805697929216\u003e, \u003c@!388751025789468672\u003e, \u003c@!203220829473996800\u003e). Everyone else can still read/write in \u003c#910910348922589184\u003e, but there was a desire to have a channel with less noise, and now this is it. \n\nIf you feel like you _should_ have write access but don't, please ping me, \u003c@!291925846556540928\u003e or \u003c@!420951281910284289\u003e and we'll sort it out. \n\nThanks!",
        "created_at": "2022-03-21T15:05:05.062000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "Testing",
        "parent": "",
        "content": "Was a long read, but I'm surprised to hear CL forward sync is so devaluated among other client teams. This may not be a popular opinion among early weak subjectivity sync fans, but CL forward sync is actually one of the most important metric showing the real performance of transition function because pre-caching strategies used by clients at post-sync-runtime doesn't help here. This real performance is critical in unhappy cases (we don't see much of these in the mainnet currently, but after The Merge this could be very different). And that's actually the reason why the network sees late blocks now. So not optimizing  CL forward sync isn't that great strategy.\n\nBTW, structural sharing mentioned above is a great strategy. Grandine uses it for years with multiple other optimizations. https://sifrai.com/grandine-0.2.0_beta3 - you can compare the archival sync performance with your favorite client to get an idea how much space for optimisation there is.",
        "created_at": "2022-03-21T15:17:18.219000+00:00",
        "attachments": null
    },
    {
        "author": "ryanleeschneider",
        "category": "Testing",
        "parent": "",
        "content": "RE: making SSZ state available via IPFS: while we don't at the moment have the spare engineering resources to build something like this I think we'd be very interested in it both existing and us running an instance.  I really like the idea of making these available over IPFS since then it somewhat divorces the trusting of the content vs. actually getting the content (e.g. I could ask multiple providers what the current content hash of the latest state is, and assuming they all agree just get the actual content from the IPFS network w/o having to verify the file myself afterwards (since presumably IPFS verifies the validity of content hashes).  So anyone that wants to provide that content would provide a `states/finalized` endpoint that points to an IPFS hash, and would pin that hash on the network at least until the next one was generated.",
        "created_at": "2022-03-21T15:37:05.893000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Testing",
        "parent": "",
        "content": "I think that a node will compute a merkle root of pulled beacon state to verify it anyway",
        "created_at": "2022-03-21T16:09:44.730000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "regarding archiving states and blocks, our current approach is era files - each era file contains 8192 slots of blocks and a state - this lines up with the historical roots (meaning each block can be verified by that state and each complete file can be (with a quirk) verified by the historical root in _any_ newer state: https://github.com/status-im/nimbus-eth2/blob/unstable/docs/e2store.md#era-files\n\neach era file is append-only while produced, idempotent and immutable once created - here's a sampling of my era directory:\n```\n-rw-r--r--. 1 arnetheduck arnetheduck 1.8M Dec 30 16:44 mainnet-00000-00001-4b363db9.era\n-rw-r--r--. 1 arnetheduck arnetheduck  18M Dec 30 16:44 mainnet-00001-00001-40cf2f3c.era\n-rw-r--r--. 1 arnetheduck arnetheduck  19M Dec 30 16:44 mainnet-00002-00001-74a3850f.era\n-rw-r--r--. 1 arnetheduck arnetheduck  18M Dec 30 16:44 mainnet-00003-00001-76b05bfe.era\n-rw-r--r--. 1 arnetheduck arnetheduck  20M Dec 30 16:44 mainnet-00004-00001-f82d21ce.era\n...\n-rw-r--r--. 1 arnetheduck arnetheduck 140M Feb 17 19:55 mainnet-00382-00001-563fafb7.era\n-rw-r--r--. 1 arnetheduck arnetheduck 140M Feb 17 19:55 mainnet-00383-00001-ac873f2c.era\n-rw-r--r--. 1 arnetheduck arnetheduck 138M Feb 17 19:55 mainnet-00384-00001-301dfcb3.era\n-rw-r--r--. 1 arnetheduck arnetheduck 139M Feb 17 19:55 mainnet-00385-00001-382ffae8.era\n-rw-r--r--. 1 arnetheduck arnetheduck 141M Feb 17 19:55 mainnet-00386-00001-93708f54.era\n```\nthe name is `CONFIG_NAME-era-count-historical_root.era` - these can be easily served over any medium as well or shared between nodes (a community member set up a simple http server serving that folder which is all it takes)",
        "created_at": "2022-03-21T17:12:08.240000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "it just so coincides that this lines up with sync committees as well, which lines up with the light client forward sync - ie even in a \"forward sync\" scenario, doing a light client sync to get to head, then backfilling via hash+one-signature is orders of magnitude faster than forward-syncing using the state transition",
        "created_at": "2022-03-21T17:13:44.122000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "we also have python code to read an era file: https://github.com/status-im/nimbus-eth2/blob/unstable/ncli/e2store.py",
        "created_at": "2022-03-21T17:17:31.817000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "each era file represents 27h of data, and I don't expect the 141mb per day to keep growing that much (outside of validator set growth) - mostly because most mainnet blocks are full already and this makes up the bulk of data in each file - everything is compressed with snappy (because all clients have to have snappy support anyone) - \u003c@!361447803194441738\u003e has talked about making attestations more    efficient size-wise - this would have direct and significant benefits here (there's lots of 100% redundant data in our attestation encoding)",
        "created_at": "2022-03-21T17:23:01.734000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Testing",
        "parent": "",
        "content": "if we can agree on a http scheme for these files in sometihng like the standard beacon api, that would be a low-effort step forward, outside of fancy ipfs solutions and the like",
        "created_at": "2022-03-21T17:23:59.102000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "Testing",
        "parent": "",
        "content": "do you have data that this is why late blocks are seen on the network today?\n\nI've seen data showing that this is just a subset of validators (rather than a netowrk-wide thing) -- pointing to these validators having clock or other local issues whre they send their blocks late\n\nalso note, that blocks are not fully validated before being passed along hops on the gossip network so again, long block processing times also shouldn't slow down block propogation",
        "created_at": "2022-03-21T18:13:14.432000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "Testing",
        "parent": "",
        "content": "Last time I checked it was clear that these issues are related to slow block processing - mostly around epoch transition and reorgs. Its not only about propagation, itâ€™s also about block production as it needs state transition too. Would be interesting to see the latest data. But the idea is that if state transition is slow, the client will struggle with its duties during unexpected scenarios that regular pre-caching techniques doesnâ€™t cover.",
        "created_at": "2022-03-21T19:20:26.694000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "If weâ€™re trying to reduce late blocks then weâ€™re concerned about the speed that nodes can produce and gossip-verify blocks.\n\nProduction is a separate beast to forward-sync, so I donâ€™t see why looking at forward-sync is the best strategy there. \n\nGossip verification is a very small subset of what happens during forward sync. For LH, time is almost exclusively consumed by with finding the proposer index and then verifying the BLS signature.\n\nI would say that spending time on forward-sync when we want to improve late blocks is a very indirect way.\n\nThe best improvements LH has made for block propagation times has been priming the proposer cache before the start of each epoch. This would have no effect in forward-sync.",
        "created_at": "2022-03-21T20:03:27.331000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "State transition is a small part of our block production times and it certainly doesnâ€™t take anywhere near the several seconds that weâ€™re seeing for late blocks.",
        "created_at": "2022-03-21T20:04:18.882000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "Sure, forward-sync has common components to following the head, but just because weâ€™re not focusing on forward-sync doesnâ€™t mean weâ€™re not trying to fix late blocks.",
        "created_at": "2022-03-21T20:04:55.548000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "Itâ€™s true that if state processing is slow then block production is slow, but I donâ€™t see any data to suggest thatâ€™s our problem now. Just because it could be a slowdown doesnâ€™t mean itâ€™s the slowdown affecting us. \n\nThereâ€™s plenty of other things happening with block production,  packing attestations is one of them. Thatâ€™s the longest part of our production process now.",
        "created_at": "2022-03-21T20:08:29.287000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "I would certainly be open to seeing some data that indicates that late blocks are due to state transition functions. This has not been my observation, though.",
        "created_at": "2022-03-21T20:10:24.355000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "Testing",
        "parent": "",
        "content": "Seems my data doesnâ€™t represent the current situation now. But how are you able to propose blocks fast in the cases where at the last second pre-cache becomes not relevant and you canâ€™t prepare the cache on time because state transition is slow?",
        "created_at": "2022-03-21T20:32:07.926000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "We run epoch processsing before the start of the epoch so our caches are primed. Epoch processing is very amenable to being run some time before you actually need it.",
        "created_at": "2022-03-21T20:35:19.853000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "IIRC, block processing and tree hashing take about 50-100ms for LH. Even if they took 10x that weâ€™d still be producing blocks a lot faster than the late ones that are causing issues.",
        "created_at": "2022-03-21T20:36:48.424000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "Testing",
        "parent": "",
        "content": "I mean what happens when this preprocessed epoch becomes irrelevant because the head changed",
        "created_at": "2022-03-21T20:39:49.961000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "Testing",
        "parent": "",
        "content": "Or the cpu was busy so it was not able to preprocess the epoch on time",
        "created_at": "2022-03-21T20:41:10.565000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Testing",
        "parent": "",
        "content": "That would require per epoch processing, but just because weâ€™re running it doesnâ€™t mean itâ€™s causing problems.\n\nI agree that faster state processing is always better, but I donâ€™t agree that itâ€™s whatâ€™s causing late blocks.",
        "created_at": "2022-03-21T20:41:29.540000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "Testing",
        "parent": "",
        "content": "Well, in these clock-based systems speed matters, but I hear your point that the data you have indicates that the late blocks are not because of slow state transition. Is that data available publicly?",
        "created_at": "2022-03-21T21:33:05.917000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "Even if it was state transition, optimising sync times is a poor proxy for that.  There are a number of optimisations like batching block imports that don't apply to block production.  If you want to make block production faster, profile block production, not some other vague proxy for it.",
        "created_at": "2022-03-21T21:40:38.707000+00:00",
        "attachments": null
    },
    {
        "author": "sauliusgrigaitis",
        "category": "Testing",
        "parent": "",
        "content": "In the worst case scenarios state transition performance matters a lot, just like in synching. But seems that there is also the different opinion - letâ€™s not optimise state transition, instead letâ€™s hope that precaching covers all the cases, even if it doesnâ€™t cover - thatâ€™s not the problem, because there is some other reason for the late blocks in the network now. Thatâ€™s not very good strategy in the long term IMO, but hopefully it will work ðŸ™‚",
        "created_at": "2022-03-21T21:59:28.163000+00:00",
        "attachments": null
    },
    {
        "author": "ajsutton",
        "category": "Testing",
        "parent": "",
        "content": "I think youâ€™re misrepresenting what weâ€™re saying. Initial sync is a lot more than just state transition. Proposing blocks is very different to initial sync. If you want to improve something itâ€™s critical that you measure what youâ€™re actually trying to improve. \nTeku for instance completes initial sync in a minute or two because we optimised away the vast majority of the work with checkpoint sync. That made us (at the time) the fastest syncing client by a mile but did absolutely nothing to improve block proposals. \n\nSo again if you want to speed up block proposals, measure block proposals and actually optimise that.",
        "created_at": "2022-03-21T23:09:51.125000+00:00",
        "attachments": null
    }
]