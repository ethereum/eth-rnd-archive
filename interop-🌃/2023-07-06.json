[
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "do we have any new images for besu? Lodestar/besu pair still failing with ```Jul-06 07:17:48.008[]                error: Process shutdown requested  code=PROTO_ARRAY_INVALID_LVH_EXECUTION_RESPONSE, lvhCode=ValidToInvalid, blockRoot=0x89a4a8a231daecf70bb82a08acc94c4b5d14747799fd2be1bd369c49cfeb72b5, execHash=0x80b0f0c0d6a74863854031f975749fe94badf31c3b4f31a60c5a814a5acfd905\nError: PROTO_ARRAY_INVALID_LVH_EXECUTION_RESPONSE```.",
        "created_at": "2023-07-06T07:22:04.429000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Afaik it's not fixed yet, probably related to worldstate",
        "created_at": "2023-07-06T07:28:35.271000+00:00",
        "attachments": null
    },
    {
        "author": "jtraglia",
        "category": "Testing",
        "parent": "",
        "content": "Hey everyone, I made 20 validator deposits to 4844-devnet-7 about 18 hours ago. I would have expected those validators to be active now, but they are not. Seems that Lighthouse/Lodestar handled the deposits just fine, but I think there's a problem with deposits on Teku/Prysm nodes (~45% of the network). In the Teku logs, there's this error (https://gist.github.com/jtraglia/883fb8fdf0cfd93734ced5b900b970d3). Thoughts?",
        "created_at": "2023-07-06T14:04:48.519000+00:00",
        "attachments": null
    },
    {
        "author": "jtraglia",
        "category": "Testing",
        "parent": "",
        "content": "```\n2023-07-06 13:52:05.649 FATAL - PLEASE CHECK YOUR ETH1 NODE | Encountered a problem retrieving deposit events from eth1 endpoint: Expected next deposit at index 2, but got 4\n```",
        "created_at": "2023-07-06T14:06:06.936000+00:00",
        "attachments": null
    },
    {
        "author": "robocopsgonemad",
        "category": "Testing",
        "parent": "",
        "content": "https://github.com/hyperledger/besu/pull/5680/files is under review, will be ready soon",
        "created_at": "2023-07-06T14:08:51.059000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "and prysm shows 0 deposits:\n```\ntime=\"2023-07-06 14:09:12\" level=debug msg=\"Synced new block\" block=0x070422f6... chainServiceProcessedTime=16.483848ms deposits=0 epoch=1370 finalizedEpoch=1368 finalizedRoot=0xf32ec528... justifiedEpoch=1369 justifiedRoot=0x725d8a8d... parentRoot=0xd0e4977f... prefix=blockchain sinceSlotStartTime=41.055833ms slot=43841 slotInEpoch=1 version=deneb\n```\n\nnot really sure how to dump the voting information from a prysm node or the data that prysm sees other than the logs",
        "created_at": "2023-07-06T14:10:00.394000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "these branch names are getting longer by the day ðŸ˜„",
        "created_at": "2023-07-06T14:13:21.095000+00:00",
        "attachments": null
    },
    {
        "author": "robocopsgonemad",
        "category": "Testing",
        "parent": "",
        "content": "don't build off of that pls, continue using 4844-devnet-5b, cuz thats a terrible name too",
        "created_at": "2023-07-06T14:15:44.104000+00:00",
        "attachments": null
    },
    {
        "author": "robocopsgonemad",
        "category": "Testing",
        "parent": "",
        "content": "ok, besu fix for a bug in excessDataGas pricing has been merged. it is going through CI now, but if you'd like to build your own and update devnet7, have at it",
        "created_at": "2023-07-06T14:27:33.855000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "need besu datadir wiped?",
        "created_at": "2023-07-06T14:30:18.700000+00:00",
        "attachments": null
    },
    {
        "author": "robocopsgonemad",
        "category": "Testing",
        "parent": "",
        "content": "nope",
        "created_at": "2023-07-06T14:31:08.107000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "big scary log trace on lighthouse-besu node:\n```\n2023-07-06 14:50:28.531+00:00 | nioEventLoopGroup-3-3 | INFO  | AbstractBlockProcessor | Block processing error: transaction invalid Internal Error in Besu - java.lang.NullPointerException: Cannot invoke \"java.util.Optional.isPresent()\" because the return value of \"org.hyperledger.besu.evm.frame.MessageFrame.getVersionedHashes()\" is null. Block 0x4871e925ed64a852e83578eb46bb73bed757121469630365d09c7adb48a35277 Transaction 0xf99bb86d4282343a29c0083e65cef6421b8cd85583b72343e33d5154d1887a45\n2023-07-06 14:50:28.532+00:00 | nioEventLoopGroup-3-3 | INFO  | MainnetBlockValidator | Invalid block 35256 (0x4871e925ed64a852e83578eb46bb73bed757121469630365d09c7adb48a35277): Optional[Block processing error: transaction invalid Internal Error in Besu - java.lang.NullPointerException: Cannot invoke \"java.util.Optional.isPresent()\" because the return value of \"org.hyperledger.besu.evm.frame.MessageFrame.getVersionedHashes()\" is null. Block 0x4871e925ed64a852e83578eb46bb73bed757121469630365d09c7adb48a35277 Transaction 0xf99bb86d4282343a29c0083e65cef6421b8cd85583b72343e33d5154d1887a45]\n2023-07-06 14:50:28.796+00:00 | nioEventLoopGroup-3-3 | ERROR | MainnetTransactionProcessor | Critical Exception Processing Transaction\njava.lang.NullPointerException: Cannot invoke \"java.util.Optional.isPresent()\" because the return value of \"org.hyperledger.besu.evm.frame.MessageFrame.getVersionedHashes()\" is null\n```",
        "created_at": "2023-07-06T14:51:08.045000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "but its doing a backwards sync now",
        "created_at": "2023-07-06T14:51:19.183000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "This is normally due to EL messing up with deposit logs. What we do is to restart teku to clear our cache",
        "created_at": "2023-07-06T15:00:38.650000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "hmm, pinging \u003c@360491619402776577\u003e in case we should collect something before restarting",
        "created_at": "2023-07-06T15:01:37.504000+00:00",
        "attachments": null
    },
    {
        "author": "mariusvanderwijden",
        "category": "Testing",
        "parent": "",
        "content": "Wdym with EL messing up with deposit logs?",
        "created_at": "2023-07-06T15:02:30.998000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "This has been a common issue with ELs. Teku expects a certain index number but the EL for some reason doesn't return what we expect (in this case we expect 2 and we got 4)",
        "created_at": "2023-07-06T15:05:46.074000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "there is some caching involved on teku side, so we normally restart teku after fixing on EL side (I remember besu had some problems there which required cleaning up logs cache and let it rebuilding)",
        "created_at": "2023-07-06T15:07:31.668000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "so should i go ahead with restarting teku or wait for some data collection?",
        "created_at": "2023-07-06T15:14:37.376000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "are all teku nodes with same errors?",
        "created_at": "2023-07-06T15:16:28.748000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "seems to just be teku-geth with that specific error",
        "created_at": "2023-07-06T15:18:23.160000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "```\n2023-07-05 20:37:00.001 INFO  - Syncing     *** Target slot: 38580, Head slot: 38579, Waiting for execution layer sync, Connected peers: 14\n2023-07-05 20:37:00.900 FATAL - PLEASE CHECK YOUR ETH1 NODE | Encountered a problem retrieving deposit events from eth1 endpoint: Expected next deposit at index 2, but got 4\ntech.pegasys.teku.ethereum.pow.api.InvalidDepositEventsException: Expected next deposit at index 2, but got 4\n```\n\nThe error first appeared while waiting geth to sync.",
        "created_at": "2023-07-06T15:23:34.041000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "might be helpful",
        "created_at": "2023-07-06T15:24:02.409000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "Yup, but sorry that might have side-tracked the core issue. There were deposits made and they didn't show up on the validator state. Looks like that log posted is only on teku-geth-1, which would be 200 keys (~20%). We should still be able to finalize votes and then include new validators.",
        "created_at": "2023-07-06T15:37:38.610000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@199561711278227457\u003e could you redeploy nethermind image please? Tx pool messages size has been fixed",
        "created_at": "2023-07-06T17:35:18.597000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "just deployed on all nethermind nodes",
        "created_at": "2023-07-06T19:26:04.333000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "Testing",
        "parent": "",
        "content": "Thanks!",
        "created_at": "2023-07-06T20:32:40.621000+00:00",
        "attachments": null
    },
    {
        "author": "__flcl",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@504202741933932544\u003e is there any chance teku may send 3 params in `engine_newPayloadV3`?",
        "created_at": "2023-07-06T20:33:29.944000+00:00",
        "attachments": null
    },
    {
        "author": "tbenr",
        "category": "Testing",
        "parent": "",
        "content": "If \u003c@199561711278227457\u003e refreshed the teku image yes. We have to stick to 23.6.1 version, current develop already have most of the new specs implemented",
        "created_at": "2023-07-06T20:40:17.885000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "ah! we're on current develop. let me pin it to use `consensys/teku:23.6.1`",
        "created_at": "2023-07-06T20:41:42.858000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "teku is now pinned to that version",
        "created_at": "2023-07-06T20:48:49.826000+00:00",
        "attachments": null
    }
]