[
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "tested lodestar and ethereumjs in local devnets, should be ready for devnet 8 cc \u003c@412614104222531604\u003e \u003c@199561711278227457\u003e",
        "created_at": "2023-08-02T09:23:58.531000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "let me cut images for you the same",
        "created_at": "2023-08-02T09:24:24.890000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "`g11tech/lodestar:36-7b0e9f` and `g11tech/ethereumjs:36-e11a93`",
        "created_at": "2023-08-02T09:52:44.185000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "no pretty tags?",
        "created_at": "2023-08-02T09:52:55.609000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "`g11tech/ethereumjs:36-d8-e11a93` and `g11tech/lodestar:36-d8-7b0e9f`",
        "created_at": "2023-08-02T09:55:42.637000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "nice, will update hive with the ethereumJS image",
        "created_at": "2023-08-02T09:57:44.394000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "we are yet to solve the docker stop issue with ethereumjs",
        "created_at": "2023-08-02T09:58:25.172000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "will update you with the fresh image shortly",
        "created_at": "2023-08-02T09:58:45.447000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "okay, ill wait for that one then",
        "created_at": "2023-08-02T10:00:30.648000+00:00",
        "attachments": null
    },
    {
        "author": "barnabasbusa",
        "category": "Testing",
        "parent": "",
        "content": "Rafael's PR was not working well?",
        "created_at": "2023-08-02T10:42:20.665000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "we wanted with something that is more native to docker like `tini`",
        "created_at": "2023-08-02T10:42:48.609000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "also if you use `--init` flag with docker container that resolves the problem as well but no way to set it in hive (but can be done for devnet 8 ansible still)",
        "created_at": "2023-08-02T10:43:42.421000+00:00",
        "attachments": null
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "We re-ran the experiment, this time with the patched teku version as well as 16c/32GB machines all around. While this network is significantly better, it seems like this still isn't enough for running such a large state. None of the clients are hitting CPU or RAM limits at this point, which to me would indicate that the duties simply takes too much effort to complete in time. \n\nThis however isn't true across the board, lighthouse and nimbus seem to perform without any errors in their logs. Prysm seems to keep up but some logs indicate there might be issues. Teku and Lodestar produce a decent number of logs indicating bottlenecks. \n\nSome detailed logs can be found here: https://notes.ethereum.org/@parithosh/bigboi-beaconchain-test-1\n\nI'll leave the network up for another 1h from this message and tear it down after, in case anyone wants to collect any information right now. I'll dump the logs of one of each client combos for future inspection if needed. \n\nLog dump: https://drive.google.com/drive/folders/1YXChbRiZG_qePmiJ9MjEMcP83Gsj0ehq?usp=sharing\n\nHowever, this doesn't help the original goal of finding out if Holesky can handle 1.5M validators without issues. So we'd still need to decide which of the two tests help us get there:\n1. Reduce validator set size from 2.1M to 1.5M, keep the number of nodes the same but it should be easier to perform duties as state is smaller + fewer keys/node.\n2. Keep the validator set size the same(2.1M), but switch the key distribution around to create an artificial stable network (i.e, lean on lighthouse/nimbus/prysm more instead of an equal-ish split).\n\nDo let me know which approach you all prefer, devoid of any consensus we'd go ahead with option 1.",
        "created_at": "2023-08-02T17:26:14.600000+00:00",
        "attachments": [
            {
                "type": "image/png",
                "origin_name": "Screenshot_2023-08-02_at_19.19.28.png",
                "content": "e3f280ae3847bfbc544d078c7ed17e3859b5a54d0af6967ea93ee93aaedfb9bb"
            }
        ]
    },
    {
        "author": "parithosh",
        "category": "Testing",
        "parent": "",
        "content": "^ curiously, even as block proposal rate moves up and down, the attestation rate is quite stable. Thats probably related to time spent to gossip the block and time allocated to attest/aggregate",
        "created_at": "2023-08-02T18:11:56.783000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "we are locally passing like 30/36 tests, with 2-3 tests where hive is sending wrong data (null for parent beacon block header) and a couple where we have broadcasting issues for blob txs",
        "created_at": "2023-08-02T19:31:40.445000+00:00",
        "attachments": null
    },
    {
        "author": "g11tech",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@508125616940515329\u003e has detailed update i think",
        "created_at": "2023-08-02T19:31:59.780000+00:00",
        "attachments": null
    },
    {
        "author": "jochembrouwer",
        "category": "Testing",
        "parent": "",
        "content": "Yess will give in a min",
        "created_at": "2023-08-02T19:32:08.579000+00:00",
        "attachments": null
    },
    {
        "author": "jochembrouwer",
        "category": "Testing",
        "parent": "",
        "content": "Also another point: in the execution API spec it is currently still allowed to call `engine_forkchoiceUpdatedV2` in Cancun. CC \u003c@425572898787426305\u003e",
        "created_at": "2023-08-02T19:33:52.165000+00:00",
        "attachments": null
    },
    {
        "author": "jochembrouwer",
        "category": "Testing",
        "parent": "",
        "content": "Maybe there is no file yet for EIP 4788 experimental updates for the devnet? Since the blob experimental one https://github.com/ethereum/execution-apis/blob/main/src/engine/experimental/blob-extension.md seems to be (only?) 4844 related?",
        "created_at": "2023-08-02T19:35:14.180000+00:00",
        "attachments": null
    },
    {
        "author": "jochembrouwer",
        "category": "Testing",
        "parent": "",
        "content": "Another problem on the test: `engine-blobs/NewPayloadV3 Before Cancun, 0x00 Data Fields, Empty Array Versioned Hashes` https://discord.com/channels/595666850260713488/688075293562503241/1136396481864142961 CC \u003c@892053833121923094\u003e \n\nI think the test wants to test that it is indeed unsupported fork, but it thus now calls an unavailable RPC method with the wrong data. \n\n(I think this test should be edited such that the `parentBeaconBlockRoot` should not be `null`, but a test where both these points are wrong should definitely be included!)",
        "created_at": "2023-08-02T20:35:38.956000+00:00",
        "attachments": null
    },
    {
        "author": "jochembrouwer",
        "category": "Testing",
        "parent": "",
        "content": "These two tests:\n```\nIncorrect BlobGasUsed: Non-Zero on Zero Blobs \nIncorrect BlobGasUsed: GAS_PER_BLOB on Zero Blobs \n```\n\nSeem to be wrong on hive side since `parentBlockBeaconRoot` is `null` in `newPayloadV3`",
        "created_at": "2023-08-02T20:44:49.533000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "The logs look ok on our end, the 2s late times are indicative of the network sending us late blocks/attestation than us processing them slower.  ~~Nvm ignore what I mentioned earlier, I looked at the full error logs which had some interesting errors from gossip~~",
        "created_at": "2023-08-02T22:21:25.666000+00:00",
        "attachments": null
    },
    {
        "author": "jgm",
        "category": "Testing",
        "parent": "",
        "content": "How about option 3) keep the validator set size at 2.1MM, and leave the network running for a while to allow individual client teams to hunt down and address the bottlenecks.\n\nReducing the validator set size to 1.8MM or 1.5MM may help a bit, but we're going to hit these numbers soon enough once non-genesis validators can join.  Better to address the issues at scale, in a devnet, prior to considering launching holesky.",
        "created_at": "2023-08-02T22:48:42.114000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@199561711278227457\u003e the logs that Nishant posted look good to me",
        "created_at": "2023-08-02T23:43:28.462000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "The only message I find that we are struggling to understand is \n```\ntime=\"2023-08-02 16:25:04\" level=debug msg=\"Gossip message was ignored\" agent=\"Lighthouse/v4.3.0-dfcb336/x86_64-linux\" error=\"epoch 3 root 0x094f99ab02d5b0d71f54fe3a2309d360eeb6ccca96f91d72c7f8b350533d836b: not a checkpoint in forkchoice\" gossip score=21.22931150205812 multiaddress=\"/ip4/144.126.197.69/tcp/59008\" peer id=16Uiu2HAm9GwUAyubQR85fa4RDaTmmqp6QKniDXVHoboKrx8dRBMc prefix=sync topic=\"/eth2/eec15ee2/beacon_attestation_63/ssz_snappy\"\n```\nLighthouse is sending this as checkpoint but we never saw that root.",
        "created_at": "2023-08-02T23:47:14.382000+00:00",
        "attachments": null
    },
    {
        "author": "nishant0",
        "category": "Testing",
        "parent": "",
        "content": "we did see the block",
        "created_at": "2023-08-02T23:49:26.842000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "oh sorry, jus that we don't have that as a checkpoint cause we have slot 64 as checkpoint",
        "created_at": "2023-08-02T23:52:10.524000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "\u003c@602753420033785856\u003e could you check if Lighthouse is struggling or in a very long fork? we have a block in 64 which is a direct descendant of 63 and Lighthouse keeps using 63 as a checkpoint",
        "created_at": "2023-08-02T23:54:12.038000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "but yeah, whatever this is, the processing time of these blocks shows we can't reallistically handle these networks in these computers at least. 63 took like 9 seconds to process ðŸ™‚",
        "created_at": "2023-08-02T23:55:27.999000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "I fully agree with this, if this network can be saved we better find out what's wrong",
        "created_at": "2023-08-02T23:59:04.260000+00:00",
        "attachments": null
    },
    {
        "author": "potuz",
        "category": "Testing",
        "parent": "",
        "content": "the logs we have don't really show anything substantially wrong with prysm, but we are forced to sync very old blocks and that definitely pushes the boundaries on state regeneration",
        "created_at": "2023-08-02T23:59:41.776000+00:00",
        "attachments": null
    }
]