[
    {
        "author": "pauld.9606",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "2) I was excited that in the medium article, the uncompressed kv witness had multiple \"tapes\". I would compress it differently than the article: I would have three tapes: leafs, hashes, and structure, with the structure compressed with entropy coding. I suspect that this is the theoretical best that we can do.\n\nThis is what I call \"segwit\" (needs a better name). \n\nBut having separate tapes is controversial because \u003c@456226577798135808\u003e had concerns that, based on his early prototypes, having separate tapes increases complexity too much. So he prefers having all tapes interleaved to a single tape. I hope that if we show \u003c@456226577798135808\u003e a spec with multiple tapes that is economic for thought and economic in size, then these concerns may disappear.\n\nI started writing such a spec. I will put a higher priority on this.",
        "created_at": "2020-07-15T00:01:33.930000+00:00",
        "attachments": []
    },
    {
        "author": "pauld.9606",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "3) A reason why witness spec design is difficult is because the design is affected by whether we have a witness cache, such as in regenesis. A cache changes the \"shape\" of witnesses -- with a cache, we have many isolated single-path witnesses, and without a cache, we have more emphasis on multi-proof deduplication. Following the old saying from computer architecture design, \"make the common case fast\", but replace \"fast\" with \"small\".",
        "created_at": "2020-07-15T00:03:35.193000+00:00",
        "attachments": []
    },
    {
        "author": "pauld.9606",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "(Done.)",
        "created_at": "2020-07-15T00:03:39.296000+00:00",
        "attachments": []
    },
    {
        "author": "_drinkcoffee",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Thanks for writing this up \u003c@!520564582196969472\u003e and thanks for your thoughtful comments \u003c@!682078300234973185\u003e .   A _long_ time ago my masters thesis was about how to compress indices for image compression for tree structured vector quantization. The idea of was that similar colours are nearby in an image, so encoding the delta of a colour indices would be better than just sending colour indices. That is, the encoder would encode the indices assuming that the decoder understood the current \"position\" in the tree, where the current position was the last index sent.\nIn our case, it would be like encoding the trie shown in \u003c@!520564582196969472\u003e  's post from left to right. You would have a value prefixed to each index to indicate how it should be interpreted: absolute and relative: up a certain number of bits in the tree followed by the relative index.",
        "created_at": "2020-07-15T03:17:27.624000+00:00",
        "attachments": []
    },
    {
        "author": "_drinkcoffee",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "For the indices: contract storage starts at 0, where the key is 256 bits long. I imagine a lot of the keys for a contract storage are going to be to 0x00, 0x20, etc. That is, the first 31 bytes would be 0. Maybe this fact can be used for compression; for instance having a special prefix for encoding these keys.",
        "created_at": "2020-07-15T03:21:23.934000+00:00",
        "attachments": []
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e For the indices: contract storage starts at 0, where the key is 256 bits long. I imagine a lot of the keys for a contract storage are going to be to 0x00, 0x20, etc. That is, the first 31 bytes would be 0. Maybe this fact can be used for compression; for instance having a special prefix for encoding these keys.\n\u003c@!636680106630316033\u003e that would be nice. However, these keys are passed through keccak256 before they are added to the trie, so usually it is quite pseudo-random",
        "created_at": "2020-07-15T06:13:31.201000+00:00",
        "attachments": []
    },
    {
        "author": "_drinkcoffee",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Ahh, I thought that was too easy - and yes, I do remember that now that you mention it.",
        "created_at": "2020-07-15T07:45:24.900000+00:00",
        "attachments": []
    },
    {
        "author": "mandrigin",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e 1) I suspect that the proposed kv witness encoding is less size efficient than the current witness spec. This is because the proposed kv witness (as well as the article's \"opcode witness\", which is different from the current witness spec) requires 34 (or more?) bytes per hash node. On the other hand, the current spec requires 33 bytes per hash node (this can be improved by adding some complexity). And hash nodes are the biggest bottleneck. The situation is similar for the rest of the encoding, where there is also more overhead.\n\u003e \n\u003e \u003c@!520564582196969472\u003e are you convinced by this ^ hand-wavy overhead analysis? Do you think that we should compare witnesses side-by-side, say for specific ethereum block numbers? Or is this just an iteration that doesn't need comparison yet?\n\u003c@!682078300234973185\u003e It might be. But why do you think it needs 34 bytes or more? Right now it serializes (apart from the key) just type byte and then the hash value itself, making it 33 bytes. But together with the key, it is of course is \u003ctype_byte\u003e \u003ckey_length\u003e \u003ckey\u003e \u003chash\u003e",
        "created_at": "2020-07-15T08:47:05.977000+00:00",
        "attachments": []
    },
    {
        "author": "mandrigin",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "anyway, this research was mostly to check if the size of a KV witness is in the ballpark of what we had before and it looks like it is",
        "created_at": "2020-07-15T08:47:39.369000+00:00",
        "attachments": []
    },
    {
        "author": "mandrigin",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e 2) I was excited that in the medium article, the uncompressed kv witness had multiple \"tapes\". I would compress it differently than the article: I would have three tapes: leafs, hashes, and structure, with the structure compressed with entropy coding. I suspect that this is the theoretical best that we can do.\n\n\u003c@!682078300234973185\u003e  yeah, splitting values and proofs semantically is a good idea in general. For the compressed witness semantics still puts them apart, it is just the serialization algorithm mixes them together to the prefix deduplication is more efficient. If we keep this \"splitting into tapes\" think a serialization implementation detail, it won't be that difficult I think, because after deserialization we will have something much simpler to work with.",
        "created_at": "2020-07-15T08:50:35.878000+00:00",
        "attachments": []
    },
    {
        "author": "mandrigin",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e 3) A reason why witness spec design is difficult is because the design is affected by whether we have a witness cache, such as in regenesis. A cache changes the \"shape\" of witnesses -- with a cache, we have many isolated single-path witnesses, and without a cache, we have more emphasis on multi-proof deduplication. Following the old saying from computer architecture design, \"make the common case fast\", but replace \"fast\" with \"small\".\n\u003c@!682078300234973185\u003e yeah, it would actually be interesting to see how the kv witnesses and our opcode witnesses, adjusted to the current spec would work with regenesis every 1M blocks for example... But that will require much more work to make this work... Of course, turbo-geth currently seems to be a good platform for such experiments, but I'm not sure I have quite enough time in July to work on it.",
        "created_at": "2020-07-15T08:52:33.421000+00:00",
        "attachments": []
    },
    {
        "author": "mandrigin",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e \u003c@!636680106630316033\u003e that would be nice. However, these keys are passed through keccak256 before they are added to the trie, so usually it is quite pseudo-random\n\u003c@456226577798135808\u003e I guess we can also experiment with plain keys for the witness data part (and keep keys for proofs hashed). Then we theoretically can use this algorithm",
        "created_at": "2020-07-15T08:53:47.847000+00:00",
        "attachments": []
    },
    {
        "author": "pauld.9606",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e But why do you think it needs 34 bytes or more?\n\u003c@!520564582196969472\u003e as you explained, I think that `\u003ctype_byte\u003e \u003ckey_length\u003e \u003ckey\u003e \u003chash\u003e` uses at least 34 or 35 bytes. If you can pack `\u003ctype_byte\u003e \u003ckey_length\u003e \u003ckey\u003e` into one byte for the common case of `key_length == 1 nibble`, then your encoding will be very close to the current witness spec.",
        "created_at": "2020-07-15T15:02:45.935000+00:00",
        "attachments": []
    },
    {
        "author": "pauld.9606",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e I'm not sure I have quite enough time in July to work on it.\n\u003c@!520564582196969472\u003e Please don't feel any obligation to work on this. Things like minimalness and canonicalness are still open questions, and because they depend on which witness spec we use, and because witness spec design depends on whether we have a regenesis cache, it may be tricky to compare different encodings. A hand-wavy overhead analysis is good enough for me for now.",
        "created_at": "2020-07-15T15:07:10.796000+00:00",
        "attachments": []
    },
    {
        "author": "mandrigin",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Ah, okay, I see the difference then. Yeah, I just wanted to check if the KV witness is in the ballpark and can be even considered useful taking into account sizes per block",
        "created_at": "2020-07-15T16:58:41.002000+00:00",
        "attachments": []
    },
    {
        "author": "pauld.9606",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Yes, KV witnesses will be very competitive in size, after the suggested tweaks.",
        "created_at": "2020-07-15T17:37:01.147000+00:00",
        "attachments": []
    },
    {
        "author": "pauld.9606",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Witness design is an iterative process with lessons learned from each proposal. We can pick the best parts of each proposal. For example, the optimizations discussed by \u003c@!425279588009246720\u003e  and \u003c@!364458974906548225\u003e , some of which are in the current spec, will likely make it to the final spec. The final spec will \"stand on the shoulders of giants\" as Newton would say, because many people contributed to it.",
        "created_at": "2020-07-15T17:39:12.321000+00:00",
        "attachments": []
    },
    {
        "author": "pauld.9606",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "What I hope to avoid is having a naive spec for consensus, while everyone uses a more size-efficient spec for networking.",
        "created_at": "2020-07-15T17:43:19.657000+00:00",
        "attachments": []
    },
    {
        "author": "mandrigin",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Yeah, the spec should be viable to just be implemented in real life",
        "created_at": "2020-07-15T17:56:08.734000+00:00",
        "attachments": []
    }
]