[
    {
        "author": "barnabemonnot",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "\u003c@!449019668296892420\u003e did a bit of research on the aggregates, including counting how many subsets are there, some results here: https://ethereum.github.io/rig/medalla-data-challenge/notebooks/explore.html#aggregate-attestations",
        "created_at": "2020-10-30T07:31:43.919000+00:00",
        "attachments": []
    },
    {
        "author": "barnabemonnot",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "counting redundant aggregates (which include attesting bits which were all previously included) is a bit tricky, but i am also counting \"myopic redundant\" aggregates, which are aggregates that were previously included (same attributes, same bits)",
        "created_at": "2020-10-30T07:33:02.484000+00:00",
        "attachments": []
    },
    {
        "author": "arnetheduck",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "nice! let me dig into it - and indeed - this is basically the bug / missing feature in clients, that it's somewhat tricky to maintain a data structure over which attestations are redundant because they've already been included in earlier block - a second bug of course is including redundant attestations within a single block when producing it - this should never happen and doesn't really require any difficult code",
        "created_at": "2020-10-30T10:35:40.823000+00:00",
        "attachments": []
    },
    {
        "author": "barnabemonnot",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Rooting out subset aggregates only needs info local to the block being produced, I think this is doable. Rooting out myopic redundant aggregations could be done by keeping the hash of each aggregate seen in the last 32 blocks, and looking up whether the aggregates that are going to be included in the new block match any of those hash. The harder one is redundant non-myopic aggregates, which are the sum of multiple past aggregates. No easy way to sieve these out apart from a big lookup table i guess",
        "created_at": "2020-10-30T12:10:43.505000+00:00",
        "attachments": []
    },
    {
        "author": "jgm",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Not sure it would be huge.  A map of `attestation_data_root =\u003e bitlist` should be sufficient, where as each attestation is processed it sets the bits that aren't already set.  Then you'd just need to discard it after 32 blocks, so a rolling array of these maps.",
        "created_at": "2020-10-30T12:15:48.540000+00:00",
        "attachments": []
    },
    {
        "author": "ajsutton",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Meredith implemented that for Teku recently. So in theory teku should only include attestations that add at least one new validator but I’m still not sure we haven’t missed a corner case in all the tracking required (eg when a block is reorged off the chain, the attestations in it should become eligible again but you don’t have enough data to know which validators were only in that attestation and which were kn others as well)",
        "created_at": "2020-10-30T20:08:33.696000+00:00",
        "attachments": []
    },
    {
        "author": "potuz",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "I didn't understand the sentence\n```\n We have 23.82 times more individual attestations than aggregates, meaning that if we were not aggregating, we would have 23.82 as much data on-chain.\n```",
        "created_at": "2020-10-30T23:10:30.542000+00:00",
        "attachments": []
    }
]