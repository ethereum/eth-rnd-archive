[
    {
        "author": "prestonvanloon",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "updated hackmd with a majority favoring voting mechanism: https://hackmd.io/@prestonvanloon/H1rfsG5iH",
        "created_at": "2019-11-15T02:02:30.377000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "We have two parts in eth1 data voting machinery: selection and actual voting.\n\nThis is a classic problem with a classic solution for it. Let's just allow each proposer to vote for a block hash which is identified by some deterministic function.\nWe can identify block with two ways:\n  - Match a timestamp of the beginning of voting period on eth1 chain and pick a block standing next to it\n  - Use block number at the beginning of the period and apply a transition to it to get more determinism across validators, like `(head.number - ETH1_FOLLOW_DISTANCE) % sqrt(ETH1_FOLLOW_DISTANCE)`\n\nFirst approach is vulnerable to time drifts. Second one is modular biased and doesn't work for numbers near to modular boundary.\n\nInstead of this straightforward solution we put a selection on chain which brings more complexity to the voting process. Because there is no decent solution to off chain block selection problem and on chain selection aids consensus around eth1 data between honest validators. On chain selection doesn't save from deep reorgs of eth1 chain. The only thing that seems to be a solution for reorgs problem is consecutive voting periods with different eth1 data.",
        "created_at": "2019-11-15T14:49:06.057000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "From my experience reorg of eth1 chain deeper than `16` blocks (even less) can hardly ever happen on the Main net. It used to have a thousand block reogrs only on Ropsten during hard forks. However, this assumption might not be held in the future when issuance will be reduced.\n\nThe other approach to voting is to make eth1 block hash a part of attestation. With this way we might go with a straightforward solution without a harm to consensus around eth1 data; this approach also has an ability for fast eth1 -\u003e eth2 bridge. But in the context of lazy validators who simply puts random garbage as eth1 vote could turn into an aggregation disaster.\n\nA question that is going beyond lazy validators and those who does not want to follow this voting process. What are the incentives of _uncoordinated rational majority_ of validators to onboard new ones?",
        "created_at": "2019-11-15T14:49:50.108000+00:00",
        "attachments": null
    },
    {
        "author": "m.kalinin",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Sorry for a lot of words. I tried to put all of my thoughts around eth1 data.",
        "created_at": "2019-11-15T14:50:37.067000+00:00",
        "attachments": null
    }
]