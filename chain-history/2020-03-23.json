[
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003c@!689161464829050960\u003e if I were to run with the DHT myself, I think I'd probably do it in the following phases.  I would plan to do all of these using libp2p\n\n1. aggressively minimal POC:\n  - kademlia network with the very rough mechanisms for arbitrary key/value style storage.\n2. minimal MVP\n  - kademlia network with scheme for storing and retrieving block headers in the network.\n3. extend MVP\n  - extend the network to support receipts and block bodies\n4. ???  (witnesses?  state?)\n  - whatever makes sense based on what we've learned.",
        "created_at": "2020-03-23T17:00:55.996000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "curious what you're thoughts are on this approach",
        "created_at": "2020-03-23T17:02:08.060000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "This sounds reasonable as a first step.",
        "created_at": "2020-03-23T17:11:44.341000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Anything state related is contingent on how we choose to rework state/witnesses",
        "created_at": "2020-03-23T17:12:15.812000+00:00",
        "attachments": []
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "That seems to line up with how I'd do it, \u003c@!364458974906548225\u003e.\n\nI think, in parallel to creating a minimal PoC, we should discuss how to address/chunk the different types of state.\n\nI'm worried that the naive approach of mapping a hashed account address to an account directly would generate too much network traffic.\n\nIf we assume a node donates 100MiB of storage, and we assume witnesses take up half of that, we can fit _roughly_ 41,000 accounts. That's 41,000 announcements of \"I have this account\", which seems like a ton of wasted bandwidth.\n\nOn the flip side, the storage trie is sparse, so simply slicing it into equally sized chunks seems unlikely to help.\n\n@mhswende linked a fork of geth that had some code for chunking I've been meaning to look into: https://github.com/karalabe/go-ethereum/tree/lescdn/trie",
        "created_at": "2020-03-23T19:51:42.006000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Here is the fork we used in mustekala - https://github.com/musteka-la/go-ethereum/tree/wip/slicer. We've exposed a GetSlice RPC method, but of most interest is probably the chunking functionality",
        "created_at": "2020-03-23T21:27:38.738000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "maybe we can do better by looking into whether we can allow multiple ways to store/announce the same data:\n\n- granular: announce each account that I have the data for (The `key` would be something like `/account/\u003chash(address)\u003e`)\n- bulk: announce that I have all of the accounts under the trie prefix `0xdeadbeef`.  (The `key` would be somethign like `/account/0xdeadbeef`\n\nIdeally both use the same underlying mechanism to avoid storage duplication complexity.  When looking up an account you might start by looking up the exact `key` for the account and then iteratively back your way up the key prefix looking for someone who has the *bulk* data.",
        "created_at": "2020-03-23T21:28:43.658000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Mabye a different way of saying that is:  \n\nCan we sidestep the *hard* problem of evenly dividing the state into chunks that are the right size by instead changing to protocol to allow individuals to choose their chunks however they choose and just have the protocol support a way of broadcasting what chunk you have.\n\nI think this introduces a different problem which is that assuming I want the acccount  under the imaginary key: `0xaabbcc` and someone has it inside of a chunk unde the prefix `0xaab` how do I efficiently discover they have it.  Is it ok for them to enumerate looking up `0xaabbcc`, then `0xaabbc`, then `0xaabb`, then finally `0xaab` where they finally find what they are looking for?  Or can we do better and make it possible to have them query the network for `0xaabbcc` and have the network be *smart* enough that it can find the record that advertised `0xaab` and match it against the request....",
        "created_at": "2020-03-23T21:36:15.626000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I think this complicates things a lot. I would have everybody on the same size prefix, this simplifies both lookups and announcements.",
        "created_at": "2020-03-23T21:37:23.347000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Having variable size prefixes requires a complicated lookup mechanism that I'm not even sure is possible in practice.",
        "created_at": "2020-03-23T21:38:01.864000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "With the right size prefix, the chunks will be small enough that it should not be an issues even for the smallest client.",
        "created_at": "2020-03-23T21:38:39.272000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I've just yet to see a scheme for chunking the trie data that doesn't rely on having a wholistic view of the entire trie to determine appropriate chunk boundaries.",
        "created_at": "2020-03-23T21:38:58.180000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "\u003e With the right size prefix, the chunks will be small enough that it should not be an issues even for the smallest client.\n\u003e \nThe idea of a \"right size prefix\" is a function of the total trie size though right?",
        "created_at": "2020-03-23T21:39:21.883000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "well, we know the theoretical max of a particular prefix size",
        "created_at": "2020-03-23T21:39:57.205000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "sure, for a full trie we know the maximum possible size of the data that could be under a prefix",
        "created_at": "2020-03-23T21:40:24.991000+00:00",
        "attachments": []
    },
    {
        "author": "samwilsn",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah, I was thinking of gossiping the total state size, then you can roughly guess what piece you need to download.",
        "created_at": "2020-03-23T21:41:35.608000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I don't claim the problem of chunking to be intractable but I've been loosely looking at this same problem for around 2 years and I haven't seen a good solution that allows a naive actor who does not have access to the state to be able to precisely identify a chunk of the state.  This applies to both acconuts and storage.  Hence, I've started looking for a way to side-step needing such a mechanism and instead look into whether the system can be designed to effectivly accomidate range queries and storage",
        "created_at": "2020-03-23T21:49:48.704000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Range queries are even more of a problem IMO, I haven't seen a DHT that supports that outside of a few thesis without an implementation.",
        "created_at": "2020-03-23T21:51:03.553000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "So I think there are two approaches to solve this and each has it's own shortcomings.\n\nApproach one is to chunk everything with a fixed prefix size. This is actually harder to accomplish that what it might appear at first, because we don't have a good way of telling what the full state is, the best we have is an address space, but since the trie can be hollow, there isn't really a way for us to tell that when a chunk is missing from the network is it because the trie is hollow for that range accounts or the chunk itself is missing, with a long enough prefix, it's possible to hit regions of the tree that are missing accounts in succession.\n\nOne way to solve this, is to have fallback nodes (full nodes?) that will step in when a chunk is missing and will be able to tell for sure if the chunk is missing from the network or is missing from the trie.\n\nApproach two is to introduce a lookup mechanism that will gradually backtrack towards shorter and shorter prefixes if the chunk can't be located, this helps solve the issue in  `1`, but it introduces it's own issues in the form of complicating the lookup mechanism. Which I'm not sure if it has any practical implementation, nor how well it scales to a network of 1000s of nodes running on commodity hardware and networks.\n\n\nI think approach 1, tho clunkier is probably the simplest one at this point.",
        "created_at": "2020-03-23T22:03:02.673000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I'm currently reading about https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Locality-preserving_hashing which appears to potentially solve the desired property of having headers that are close together hash to roughtly the same spot in the network.",
        "created_at": "2020-03-23T22:19:28.141000+00:00",
        "attachments": []
    },
    {
        "author": "dryajov",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Yeah, I've looked into it when doing my research, my question was and still is, in this model, what is the the granularity of what we're hashing? Is it account level or is it still chunk level (collection of accounts)?",
        "created_at": "2020-03-23T22:22:03.708000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Where I'm at right now is that the account space seems feasible.  We have un-answered questions but overall I don't see anything that appears to restrict us from finding a scheme that chunks the account data somewhat efficiently across the nodes of the network.   What I haven't wrapped my brain around is how to do that with contract storage.  They storage tries are imbalanced and differ greatly in total size.  I haven't had any inspiration thus far on how to evenly divide them up, much less chunk them.",
        "created_at": "2020-03-23T23:02:22.928000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "this is interesting: http://ceur-ws.org/Vol-741/DEM03_ForestieroMastroianni.pdf",
        "created_at": "2020-03-23T23:25:47.794000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "DHT structure that sorts the keys, automatically distributing the data evenly across the nodes",
        "created_at": "2020-03-23T23:31:36.560000+00:00",
        "attachments": []
    }
]