[
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e The signature would be the same, but the EIP doesn't currently talk about the hash.\n\u003e Long term, my preference would be that the hashes for all transaction types is the same, which is hash of the 2718 transaction (which would include the wrapped bytes).\n\u003e I dislike the idea of being stuck with two mechanisms for hashing transactions in all clients forever just because of a transient problem that dapps may face during the transition.\n\n\u003c@!301186049323958275\u003e given that the signature of type 0 is the same, the legacy hashing scheme will already have to implemented in order to verify the sig. So we're going to have two hashing schemes regardless. So imo changing the hashing format for tx type 0 seems to add undue complexity to the implementation, when we could just deprecate it at a later point if we want to have greater consistency among tx types",
        "created_at": "2020-06-29T13:50:27.254000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e given that the signature of type 0 is the same, the legacy hashing scheme will already have to implemented in order to verify the sig\nI don't believe this is true.",
        "created_at": "2020-06-29T14:24:49.959000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "The hash that is signed is not the hash of the signed transaction.",
        "created_at": "2020-06-29T14:24:57.600000+00:00",
        "attachments": []
    },
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "ah i see",
        "created_at": "2020-06-29T14:25:30.970000+00:00",
        "attachments": []
    },
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "that's a good point",
        "created_at": "2020-06-29T14:26:36.182000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Having two hashes for one transaction for a short period of time has high complexity, but it is complexity that can be deleted in short order, while having only the legacy hash has low complexity, but that complexity has to live forever.",
        "created_at": "2020-06-29T14:27:42.291000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I tend to prefer high complexity that can go away (doesn't accumulate technical debt) over low complexity that sits forever (accumulated technical debt).",
        "created_at": "2020-06-29T14:28:03.332000+00:00",
        "attachments": []
    },
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "why can the type 0 tx not simply be deprecated?",
        "created_at": "2020-06-29T14:28:18.723000+00:00",
        "attachments": []
    },
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "if we treat the payload exactly as a legacy tx, this code is already implemented",
        "created_at": "2020-06-29T14:28:42.282000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "You mean discourage people from using type 0 transactions *ever* and then in some future hard fork fully drop support for them?",
        "created_at": "2020-06-29T14:29:18.690000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Type 0 transactions are a tad smaller on the wire than equivalent type 1 transactions, though not by much.",
        "created_at": "2020-06-29T14:29:39.346000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "And I'm not sure if that warrants keeping them around.",
        "created_at": "2020-06-29T14:29:46.874000+00:00",
        "attachments": []
    },
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "i believe the code required to redirect the `.Hash()` function of tx type 0 to the legacy `.Hash()` function is very small",
        "created_at": "2020-06-29T14:33:13.754000+00:00",
        "attachments": []
    },
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "```go\nfunc (*tx TransactionType) Hash() common.Hash {\n  if tx.ty == 0 {\n    var legacyTx types.Transaction\n    rlp.DecodeBytes(tx.payload, \u0026legacyTx)\n    return legacyTx.Hash()\n  } else {\n    // standard hashing\n  }\n}\n```",
        "created_at": "2020-06-29T14:37:00.326000+00:00",
        "attachments": []
    },
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e I tend to prefer high complexity that can go away\nDefinitely agree, but the question is when could the old map of hashes go away? When do people stop referring to a tx? Etherscan never stops referring to txs by their hashes, so they'd need to run a migration to update the hashes from this fork. The complexity seems much higher than the technical debt that must be maintained",
        "created_at": "2020-06-29T14:44:55.674000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "In the transient 2-hash transaction model, there would be a finite and well bounded set of transactions that would need to be double keyed.",
        "created_at": "2020-06-29T14:49:46.052000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Any transaction submitted to the chain *after* fork block would only have the new hash, and any transaction mined prior to the fork block would only have the old hash.",
        "created_at": "2020-06-29T14:50:12.371000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Only transactions that were submitted prior to the fork and mined after the fork would need to be dual-hashed.",
        "created_at": "2020-06-29T14:50:27.994000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "And they would only need to be dual-hashed for as long as we think it is reasonable for people to still have them in memory from when they were in the pending pool.",
        "created_at": "2020-06-29T14:51:02.566000+00:00",
        "attachments": []
    },
    {
        "author": "lightclient",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "curious to hear other's thoughts, but imo its an awful lot of additional complexity that can be avoided with an extra 3-4 lines of code",
        "created_at": "2020-06-29T14:53:09.505000+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Technical debt is often made up of a whole lot of instances of 3-4 lines of code.  ðŸ™‚",
        "created_at": "2020-06-29T14:56:12.182000+00:00",
        "attachments": []
    }
]