[
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "Makes sense; I think it should be okay if we decompress within the guest program rather than compress. So the compression is done by the builder only and they can use whichever version they want given the compression format itself is stable\n\nCompressing within the guest program would likely mean that we would need to pin a library and version though",
    "created_at": "2026-02-02T13:55:23.076000+00:00",
    "attachments": []
  },
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "Encode also includes the packing, so if we are packing more bytes it contributes to the overhead",
    "created_at": "2026-02-02T13:56:04.334000+00:00",
    "attachments": []
  },
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "In a systems language like Rust, I think the packing would negligible though",
    "created_at": "2026-02-02T13:56:23.129000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "Yep that would be the idea but then how do you determine what the compressed size of each tx is?",
    "created_at": "2026-02-02T13:56:29.921000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "Ideally transactions should pay for compressed number of bytes their tx consumes",
    "created_at": "2026-02-02T13:57:03.803000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "maybe simpler to just pay for uncompressed number of bytes however",
    "created_at": "2026-02-02T13:57:25.617000+00:00",
    "attachments": []
  },
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "In the current EIP, I think transactions don't pay for the payload blobs -- if we added that then I think it would require some changes",
    "created_at": "2026-02-02T14:04:32.685000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "I think it will be needed to make the transaction inclusion process fair",
    "created_at": "2026-02-02T14:05:05.953000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "otherwise the economics of type 3 / non-type 3 are assymetrical",
    "created_at": "2026-02-02T14:05:24.207000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "that's essentially only charging type 3 transactions for blob resource consumption and letting non type-3 transactions get it for free",
    "created_at": "2026-02-02T14:07:29.634000+00:00",
    "attachments": []
  },
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "sorry was in a call",
    "created_at": "2026-02-02T14:49:52.180000+00:00",
    "attachments": []
  },
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "Yep I agree (noted in the PR), I think it would be good to get RIG's opinion on this because the non type-3 transactions are payload blobs which are produced by the builder",
    "created_at": "2026-02-02T14:50:45.084000+00:00",
    "attachments": []
  },
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "> RLP gave us slightly better results than SSZ, but this shouldn't be a stopper for SSZ adoption, if any.\nThis is RLP of the transaction list to be clear and not of the transaction. It likely does not make a significant difference in this case\n\n> Although it is rare, bitpack_254 led us to smaller blobs than naive_31. bitpack_254 took nearly 6 times more for decoding, which is roughly 6 milliseconds per blob.\nThe slowdown is mainly because of python overallocating for bits -- I think it should be roughly the same. Yep, I think bitpack can be made readable, I was originally thinking of the more denser complicated strategy where the packing is made based on the modulus\n\n> With current gas limit (60M), there are 1-2 payload blobs per block, and current target blob is 14. Let's see how this changes by the time BiB could be shipped.\nIsn't it roughly linear?\n\n> I've noticed that the data used is execution_payload not transactions, but it shouldn't matter much.\nCan you say where this was?",
    "created_at": "2026-02-02T14:56:42.813000+00:00",
    "attachments": []
  },
  {
    "author": "jih2nn",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "> > Although it is rare, bitpack_254 led us to smaller blobs than naive_31. bitpack_254 took nearly 6 times more for decoding, which is roughly 6 milliseconds per blob.\n> \n> The slowdown is mainly because of python overallocating for bits -- I think it should be roughly the same. Yep, I think bitpack can be made readable, I was originally thinking of the more denser complicated strategy where the packing is made based on the modulus\nYep my point is we should not waste space just for the sake of simplicity when we don't even know how complex it is. It is not late to make a decision after learning trade-offs.\n\n> > With current gas limit (60M), there are 1-2 payload blobs per block, and current target blob is 14. Let's see how this changes by the time BiB could be shipped.\n> \n> Isn't it roughly linear?\nGas limit and blob count might increase with different ratio, not sure.\n\n> > I've noticed that the data used is execution_payload not transactions, but it shouldn't matter much.\n> \n> Can you say where this was?\nnvm, I was skimming on mobile.",
    "created_at": "2026-02-02T15:14:08.525000+00:00",
    "attachments": []
  },
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "> Yep my point is we should not waste space just for the sake of simplicity when we don't even know how complex it is. It is not late to make a decision after learning trade-offs.\nMy comment in the PR was saying we should do the simple thing first, then benchmark to see if the complex thing even saves us anything significant; noting that we need to save enough bytes such that we remove a whole blob.\n\nIn this case, the bitpacking version might be okay though!\n\n> Gas limit and blob count might increase with different ratio, not sure.\nYep, I was mostly thinking of the size of the data that needs to go into blobs being roughly linear with the gas limit",
    "created_at": "2026-02-02T15:40:01.764000+00:00",
    "attachments": []
  },
  {
    "author": "jih2nn",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "> My comment in the PR was saying we should do the simple thing first, then benchmark to see if the complex thing even saves us anything significant; noting that we need to save enough bytes such that we remove a whole blob.\n> \n> In this case, the bitpacking version might be okay though!\nYeah agree with doing simple thing first. I'm saying that there will be a case that naive packing consumes one more blob and the packing algorithm, which can avoid such case no matter how rare that is, that I linked doesn't look that complex.\n\n> Yep, I was mostly thinking of the size of the data that needs to go into blobs being roughly linear with the gas limit\nAh, I was focusing on the ratio between gas limit and max blob count. i.e., how much payload blobs take up the blob count. I also think gas limit and block size will be linear.",
    "created_at": "2026-02-02T15:50:17.596000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": ">  Yep, I was mostly thinking of the size of the data that needs to go into blobs being roughly linear with the gas limit\nA quick comment on this. 0 bytes are cheap in call data. Currently, the protocol allows this because we use snappy compression when gossiping EL blocks. If we do not apply any compression, then we could have attacks in which a user introduces a bunch of cheap 0 bytes in the call data. reference: https://github.com/ethereum/EIPs/blob/master/EIPS/eip-7623.md#rationale",
    "created_at": "2026-02-02T15:52:55.870000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "As an aside maybe we should consider benchmarking snappy compression since its already used in the CL?",
    "created_at": "2026-02-02T15:53:34.156000+00:00",
    "attachments": []
  },
  {
    "author": "kevaundray",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "Ah right, does this lean towards always charging for the uncompressed data?",
    "created_at": "2026-02-02T15:57:56.446000+00:00",
    "attachments": []
  },
  {
    "author": "taulepton_",
    "category": "Cross-layer",
    "parent": "l1-zkevm-protocol",
    "content": "I would need to give this more thought as I'm not sure what the best approach is just yet. But this is certainly something that we need to consider.",
    "created_at": "2026-02-02T15:59:45.418000+00:00",
    "attachments": []
  }
]