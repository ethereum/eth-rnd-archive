[
    {
        "author": "boma_naps",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Technically No, they don't need to hold all the 2D peerDAS system to build, they can use hashes of blobs instead. but there are some tradeoff regardless of which is been used like the need to make sure the blobs are available and that the network itself has sufficient redundancy to enable validators to verify the data, also there should be enough data across the network for validators to sample and reconstruct blobs, as this blobs increases, so will bandwidth and computational needs grow here by affecting the scalability of single builder which leads to my dumb question: What happens if redundancy or distribution fails, will block verification fail as well?",
        "created_at": "2024-12-21T12:32:50.026000+00:00",
        "attachments": null
    },
    {
        "author": "notnotstorm",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "I'm looking for a handwavey formulaic understanding of what limits the max blobs per block\n\nlets say we compute redunancy with this formula:\n```\nredundancy = n_validators * n_blobs_per_validator / n_blobs_per_block\n```\nwithout peerdas, `n_blobs_per_validator=n_blobs_per_block`, and so `redundancy=n_validators`\n\nwe can rearrange formula to compute max blobs per block for a given level of redunancy. for example lets say we want `redundancy\u003e1000`, `n_validators=100000`, `n_blobs_per_validator=6`. then:\n`n_blobs_per_block \u003c n_validators * n_blobs_per_validator / redundancy = 600`\n\nis this the right way to think about the problem?",
        "created_at": "2024-12-21T21:12:24.598000+00:00",
        "attachments": null
    }
]