[
    {
        "author": "jimmygchen",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Hi \u003c@539495253418180618\u003e \u003c@520034910149410861\u003e From the ACD calll there was a proposal to ship peerdas without sampling. Is there any write-up on this? I'm curious what network parameter changes are required and the impact on security. \n\nI think most clients that have participated in devnet-1 have basic sampling implemented (without applying it to fork choice). Is there any concerns around implementation complexity of the current proposal on sampling \u0026 fork choice?",
        "created_at": "2024-07-31T07:10:15.184000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Hey \u003c@466006586477772831\u003e I will have a write up as soon as I come back home, most likely by Friday ðŸ™‚ In the meantime happy to answer questions here",
        "created_at": "2024-07-31T07:11:35.044000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "The concern is about the implementation complexity of getting sampling right, being sure of all edge cases, DoS protection etc...",
        "created_at": "2024-07-31T07:12:15.501000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Not so specific to the fork choice PR (which anyway uses sampling with a huge delay)",
        "created_at": "2024-07-31T07:12:31.881000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "It's more that having to have sampling at all implies having to think about all of the complexity that comes with it and being super confident that it doesn't cause problems, even if it is not in the critical path",
        "created_at": "2024-07-31T07:13:23.488000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Of course I/we don't have the view the implementers have, so it would be great to hear everyone's perspective on this",
        "created_at": "2024-07-31T07:13:51.223000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Thanks! looking forward to that. \nMy only concern is the security, if nodes don't perform sampling, then I think to achieve 99% confidence is to custody at least 50% of data, unless we're willing to make assumption on the rest of the network, but I haven't spent as much time thinking about this as you have, so keen to hear your thoughts",
        "created_at": "2024-07-31T07:14:14.655000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Ah right, so that gets back to your question about parameters. The idea would be to increase the baseline custody requirement (the one for full nodes) to 8 or 12 or 16 or something which we are happy with",
        "created_at": "2024-07-31T07:15:19.701000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "This means that we can say things like \"at most X% of the full nodes can be tricked into seeing a chain as available when it is not\", where X is low",
        "created_at": "2024-07-31T07:16:05.440000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Note that this is actually only relevant to full nodes, as validators already have an increased custody requirement anyway",
        "created_at": "2024-07-31T07:16:26.612000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "The price we pay is that increasing the custody requirement means less scalability (peer sampling is cheaper than mesh propagation because no duplication), but with 8 out of 128 we still get an 8x bandwidth reduction (at equal blob count). Even with custody = 16, it's a 4x bandwidth reduction",
        "created_at": "2024-07-31T07:17:47.102000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Either way, plenty of room to increase blob count by conservative amounts in the short term, which is anyway the plan",
        "created_at": "2024-07-31T07:18:13.637000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Basically dropping peer sampling means going back to one path which had been proposed at some point, which is to have SubnetDAS as a stepping stone (the current protocol minus peer sampling is precisely SubnetDAS, where custody = subnet sampling)",
        "created_at": "2024-07-31T07:19:53.127000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "(Saying this to emphasize that it's something that had been thought about from the beginning)",
        "created_at": "2024-07-31T07:20:28.544000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "If you have any insights on implementation complexity and whether or not this can be a useful stepping stone, would love to hear them ðŸ™‚",
        "created_at": "2024-07-31T07:21:22.424000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "\u003e This means that we can say things like \"at most X% of the full nodes can be tricked into seeing a chain as available when it is not\", where X is low\n\nso this seems fine for the network as a whole, however for a node to be confident that they're not tricked (e.g. usage with high value), they would have to run a supernode right?",
        "created_at": "2024-07-31T07:22:59.785000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Right, great question ðŸ™‚ Basically, the situation we are worried about there is exclusively one where an unavailable chain has been finalized, and it ultimately ends up not being the canonical chain because a social fork is coordinated and moves away from the unavailable chain. The reason why this is the only scenario we care about is that there's no point in doing sampling on a non finalized chain, from the perspective of transaction confirmation security: before a chain is finalized, you are relying on honest majority assumptions anyway, and if those hold it is just not possible for an unavailable chain to look confirmed",
        "created_at": "2024-07-31T07:27:38.459000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "I think to make sampling robust and optimized will take some time, but i don't see that being a huge amount of work. Although if not including it doesn't sacrifice security too much, reducing scope is good. At least the networking side of things are a bit more flexible in terms of upgrades",
        "created_at": "2024-07-31T07:27:58.486000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "So one aspect of the question is, the scenario we are trying to protect against is the absolute catastrophe scenario where a supermajority of the validators has misbehaved and voted to finalize something unavailable. Given we are in such a horrible scenario, the question is then, does peer sampling protect a full node much more than subnet sampling? I don't think that's really the case in practice, since peer sampling does not come with anonymous routing of queries",
        "created_at": "2024-07-31T07:31:47.543000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Like, we are talking about a situation where we basically can consider almost the whole network as malicious, and where a supermajority of validators is willing to go as far as forcing a social fork. If they want to double spend a full node, is it actually much harder to do so if the full node does peer sampling versus subnet sampling? Sure, subnet subscriptions are public, and it's easy to just make available exactly only the data that someone wants, while at least some of the peer sampling queries might be directed to a few good nodes (I say few because of the premise of being in an essentially adversarial network). On the other hand, the adversary has full control of what data is made available, and can just wait for the queries directed to honest nodes to timeout, so that eventually they are redirected to adversarial nodes, at which point the requested data is made available",
        "created_at": "2024-07-31T07:36:25.247000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "I am writing on the phone, sorry if this is not the clearest explanation",
        "created_at": "2024-07-31T07:36:40.212000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "\u003e the situation we are worried about there is exclusively one where an unavailable chain has been finalized\n\nI think this is the main scenario that we're trying to prevent from doing sampling right? Doesn't sampling reduce the likelihood of this from happening? Honest validators are less likely to vote on unavailble blocks (if we don't consider the case where majority of validators behaves maliciously)",
        "created_at": "2024-07-31T07:38:51.326000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Main points:\n- we should only worry about full nodes being tricked by a supermajority attack that leads to \"social finality reversion\", a super extreme case. Otherwise, a full node that just follows the finalized chain is fine \n- peer sampling does not actually offer much stronger protection in this scenario. Imho the main strength of peer sampling is actually just that it's more efficient in bandwidth usage, at least until someone figures out a way to make queries private, which is a long-term research project \n- any use case that has a lot of value at risk of double spend is likely just going to download all the data (e.g. exchanges)",
        "created_at": "2024-07-31T07:39:48.089000+00:00",
        "attachments": null
    },
    {
        "author": "jimmygchen",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "thanks for explaining, I'll need some time to digest this properly, so i might be a bit slow responding!",
        "created_at": "2024-07-31T07:40:15.528000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Yes, sampling protects us (as in, the network as a whole) from this scenario happening at all, *as long as there are sufficiently many honest validators*. Ofc, if instead there is a supermajority of malicious validators, they can finalize something unavailable. A key point here is this though: for this purpose, it is *completely* irrelevant how the samples get to validators (peer vs subnet). As long as the number of samples is high enough (e.g. 8 or 12 or 16 or whatever, depending on desired security), we get what we want",
        "created_at": "2024-07-31T07:42:43.435000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "What sampling also does (again, irrespective of which kind of sampling) is protecting us (again, the network as a whole) from a malicious supermajority of validators which finalizes something unavailable. Basically, if the validator set is bad and the bad scenario cannot be avoided, full node sampling guarantees that most of the network will know something is wrong and sound the alarm",
        "created_at": "2024-07-31T07:44:26.809000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "What sampling does *not* do is give strong protections to a *specific* full node, in the case when such a worst case attack actually happens (again, requires malicious supermajority of validators, willing to cause a social fork). If this happens and the corrupted validator set wants to double spend a specific non-supernode full node, they can likely do it, *regardless of whether the full node does peer or subnet sampling*",
        "created_at": "2024-07-31T07:46:16.776000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Let me know if this makes sense ðŸ™‚ Take your time ofc",
        "created_at": "2024-07-31T07:46:51.884000+00:00",
        "attachments": null
    },
    {
        "author": "fradamt",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Note also that we can protect against this scenario even more strongly if we make validator custody be a public assignment, e.g. validator 1 is known to have to custody column groups {1, 10, 15..., 78, 127}. With that, this kind of attack would be accountable, in the sense that the social fork would be able to point out exactly which validators lied about availability, and could decide to slash them, so that the attack could in principle be very costly, up to as costly as double finality",
        "created_at": "2024-07-31T07:49:43.808000+00:00",
        "attachments": null
    }
]