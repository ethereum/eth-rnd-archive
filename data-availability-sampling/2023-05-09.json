[
    {
        "author": "emhane",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "if we can't estimate this number beforehand, we need a nice way to dynamically adjust the number of nodes that store a sample. perhaps we can count the hops we had to make to find our sample and decide smthg like \"all samples should be retrieved in X hops\". then taking the average of total hops to a single sample for the retrieval of the last Y different samples, and based on this average hops, adjust the size of our own sibling list.\n\nonly adjusting the own sibling list length rather than triggering a peer to adjust their list, is a nice way to avoid the 2 (or more) messages for the price of 1 message attack vector that p2ps have to be cautious of. triggering a peer to adjust their list, means we can tell a peer to publish samples to N peers with 1 message. furthermore, that 1 messages, if piggy backed on the \"give me sample S\" request, would be a request, which means that this 1 messages is not bounded by the health checks of the k-buckets, as a peer doesn't have to be in our k-buckets to send us a request. adjusting our own list based on the average hops to our samples, then serves as a metric for the size of the network relative to the distribution of samples. the inspiration for this is the traceroute protocol.\n\napart from \"how many peers will fetch a single sample, mostly (more interested in mostly than average here, i.e. focus to cater the most common case if there is a significantly more common case)\", i think the problem also definitely needs to be analysed based on: what time period can we expect samples to be retrieved, mostly? for example, mostly during the following 2 seconds after a sample is committed to the chain, with the rare occasion that a sample is fetch later until that state is allowed to be forgotten? or, mostly some minute after the sample is committed?",
        "created_at": "2023-05-09T10:15:05.515000+00:00",
        "attachments": null
    }
]