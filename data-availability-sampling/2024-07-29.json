[
    {
        "author": "cskiraly",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "I think you misunderstood what I said. I don‚Äôt want to transition from 2D to 1D erasure code, neither \u003c@466006586477772831\u003e wants to, as I understand. The question was, at least in my interpretation, about writing 2D ready code upfront, and I said (based on my experience) that it is not that complex, and once you do it, you can use it in ‚Äú1D mode‚Äù if you have/want to.\n\nYou raise the issue of **reconstruction**: reconstruction might feel different at first sight, but actually it is simple from the code perspective. You can apply the same basic RS reconstruction in both 1D and 2D (and even more dimensional) cases, since you can anyway reconstruct dimension-by-dimension. Reconstructing the whole block from cells can be a bit more tricky, but with tricky I mean you need one more for loop. That's it.\n\u2028\nYou also raise the issue of the sample size, or more in general, how to **dynamically change blob-count block-by-block.** This is indeed something that was not yet discussed for the 2D case in sufficient detail, so let me give it a shot.\n\nWith 1D EC, we are having EC in the row dimension (encoding individual blobs), and we are stacking blobs on top of each other in the column dimension. Not having an EC on the column dimension, or in other words having it as an unprotected dimension allows us to change the blob count block-by-block.\n\nAt first sight, it might seem that the 2D structure is much more rigid. Each dimension has its fixed size, you do not have an unprotected dimension, so we can‚Äôt modulate the blob count.",
        "created_at": "2024-07-29T00:47:06.195000+00:00",
        "attachments": []
    },
    {
        "author": "cskiraly",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Here are a few ways of still having a variable blob count with a 2D RS encoding:\n\n- Padding: One could fill only as much rows as there are blobs (B), and pad the rest. Then, do the 2D erasure coding. Importantly, padding is virtual, padded cells do not need to be transmitted. The downside of this is that the size of parity data would be the same all the time, independent of B. Another downside is that sample count would depend on B (but we can derive the formula). Finally, network traffic on row topics would be unbalanced.\n\n- Padding with shuffling: you can solve the latter downside mentioned above by randomly shuffling rows using public randomness. Still, it remains that you would have half of each column as parity data even if B is small, and you would have sampling dependent on B.\n\n- Dynamic code size in the column dimension: With B blobs, use a column erasure code of K=B, N=2B. This solves the overhead problem, but it has the downside that it needs to handle EC with sizes that are not powers of 2. It can be done AFAIK, but more complex.\n\n- Dynamic cell size: this might sound a bit complex, but essentially you can encode individual blobs using a 2D RS code, and then stack them. This allows row/column based repair. Since each cell would contain pieces from every single blob (like in the case of 1D columns), we would not be able to do partial txpool based reconstruction, or distributed building.\n\nOverall I agree that there are still research questions on the 2D case. Still, writing code that can handle the 2D encoding, dispersal, and sampling is not much more difficult, and I‚Äôm happy If someone else also wants to experiment with it. I think having an understanding of what‚Äôs needed in the 2D case helps to write cleaner code that is easier to adapt later.",
        "created_at": "2024-07-29T00:47:21.373000+00:00",
        "attachments": []
    },
    {
        "author": "popef",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "\u003e I think you misunderstood what I said. I don‚Äôt want to transition from 2D to 1D erasure code, neither @Jimmy Chen wants to, as I understand. The question was, at least in my interpretation, about writing 2D ready code upfront, and I said (based on my experience) that it is not that complex, and once you do it, you can use it in ‚Äú1D mode‚Äù if you have/want to.\n\nOh, sorry about that then.",
        "created_at": "2024-07-29T03:59:37.125000+00:00",
        "attachments": []
    },
    {
        "author": "jimmygchen",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "actually i think those are good points you both raised and some of them I've not thought about. I do tend to agree that we are going to have a lot to do when upgrading to 2D. The gossip and rpc interfaces are likely completely different too. i think change may be as big as our transition from Deneb to 1D, and having two sets of peerdas logic to coexist in the code base is going to increase complexity a lot - and we'd have to keep them both in the code base for quite a long time.",
        "created_at": "2024-07-29T05:54:24.351000+00:00",
        "attachments": []
    },
    {
        "author": "jimmygchen",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "\u003e I think having an understanding of what‚Äôs needed in the 2D case helps to write cleaner code that is easier to adapt later.\nyeah i think right now we're not thinking as much about the 2D case, so it may require quite significant changes - although im not too worried about the refactor, more about how complex the codebase will become and how much extra effort it's going to take to do it incrementally",
        "created_at": "2024-07-29T05:57:31.018000+00:00",
        "attachments": []
    },
    {
        "author": "linoscope",
        "category": "Consensus R\u0026D",
        "parent": "",
        "content": "Hi everyone! I have written a blog post on data availability sampling and Danksharding. Let me know if you have any feedback or comments, especially around the Danksharding part, my understanding may be wrong or outdated üôè\n\nhttps://mirror.xyz/linoscope.eth/m-GtfDd_sux7B2j_3l4ddffJRfhoAFbfhLPoxBCFuzk",
        "created_at": "2024-07-29T17:36:15.186000+00:00",
        "attachments": []
    }
]