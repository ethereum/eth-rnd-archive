[
    {
        "author": "0xraino",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e one technical challenge of running such scheme is that in order to keep the replication factor of the snapshots high, ideally the snapshot can be used (in an overlay fashion) as the input database for the block processing and other stuff. So that you don't just download, import, and delete, but you keep seeding it\n\u003c@456226577798135808\u003e Could you further explain this idea further? In order for the cleints not to process it into a client-specific representation (and thus might as well keep the snapshot around seeding it), does that amount to all clients supporting this snapshot format as a new database format? Or is it a weaker constraint than that?",
        "created_at": "2020-07-14T14:52:30.359000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "From my understanding of how BitTorrent works (from using it occasionally) is that if you want to seed a file, you need it to be constantly present and accessible to the BitTorrent software. So the easiest thing to do would be to download the file from BitTorrent, parse it, import the snapshot into the database, and keep seeding the file. From that point on, the file is useless for your node, but it is very useful for other people trying to download the file, because the more nodes seeding the file, the faster downloads people will get on average. So the \"overlay\" is an optimisation suggestion that would allow to make the file not just useful for others, but for your node, which also incidentally makes it more likely you won't just delete the file (to save disk space) and stop seeding.",
        "created_at": "2020-07-14T14:57:22.127000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "But overlay approach has downsides of course - it would mean that the file will basically be a database, which can be efficiently accessed (get value by key), as well as merged with another database when the snapshots roll over",
        "created_at": "2020-07-14T14:59:05.557000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "it requires some trial, error and research to figure out what works",
        "created_at": "2020-07-14T14:59:30.468000+00:00",
        "attachments": null
    },
    {
        "author": "thiago_crowdtainer",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003c@456226577798135808\u003e have you given any thought about achieving some data structure such that, the intersecting data between snapshot 1 and snapshot 2, when “chunked” for distribution (thinking of swarm approach here), outputs the same chunk hashes for the parts of the data that cover the same blocks ? This way, Data more in the “past” would be more broadly distributed , therefore only the “newest” data would require dedicated “seeders” for a certain amount of time",
        "created_at": "2020-07-14T15:05:01.980000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "yes. I have been thinking about copy-on-write B-tree structure where each chunk would be reasonable size. But this seems a bit more complex than having independent files every 1m blocks (or more frequent). I am still doubtful whether IPFS or Swarm can be used for this, because I would really want to side-step the incentivisation issue (which is not resolved in ether IPFS or Swarm), by simply requiring that everyone who downloaded the files will seed them",
        "created_at": "2020-07-14T15:09:06.856000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I do not know whether you can configure IPFS or Swarm to work like BitTorrent, but according to my knowledge, the answer is No",
        "created_at": "2020-07-14T15:10:08.681000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "From what I understand, in IPFS and Swarm, you cannot choose to seed certain content, it needs to be decided by the network",
        "created_at": "2020-07-14T15:10:51.017000+00:00",
        "attachments": null
    },
    {
        "author": "vorot93",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "do we actually want to preserve all snapshots forever?",
        "created_at": "2020-07-14T15:12:17.024000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "no",
        "created_at": "2020-07-14T15:16:12.317000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "only for the time they are useful, which is until the next snapshot is available",
        "created_at": "2020-07-14T15:16:26.605000+00:00",
        "attachments": null
    },
    {
        "author": "thiago_crowdtainer",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Depends what you mean with “configure to work as bittorrent”; if you mean having a node holding onto some data so that it is available, then both ipfs and swarm have the feature (they call it “pinning”). If you mean work without incentives, well, that is already the case for all of them (since both swarm and ipfs incentivisation protocols are still being implemented,) so in effect it’s similar to torrent as in there’s no guarantee about the “seeder uptime” - though of course, BitTorrent is way more stable than say swarm at this point",
        "created_at": "2020-07-14T15:20:38.963000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I saw pinning in the Swarm documentation, but I could not figure out whether it is what you say it is. Because from the doc it seemed like pinning only makes it available to YOU locally in case the chunks disappear from the network. Which is not the same",
        "created_at": "2020-07-14T15:21:59.332000+00:00",
        "attachments": null
    },
    {
        "author": "thiago_crowdtainer",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Ahh true, I think you’re right. Though they have something in the pipeline to deliver a whole file directly from a peer, I’m not sure that is implemented yet - I can try to ask",
        "created_at": "2020-07-14T15:23:11.752000+00:00",
        "attachments": null
    },
    {
        "author": "thiago_crowdtainer",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I would be curious to know if you’re aware of any means to split the chain into separate files, but still have it in an indexable format somehow, that would be the ideal situation I guess",
        "created_at": "2020-07-14T15:23:33.264000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "splitting the chain and splitting the state are very different problems",
        "created_at": "2020-07-14T15:24:29.511000+00:00",
        "attachments": null
    },
    {
        "author": "thiago_crowdtainer",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Though honestly everything is in flux right now, so for simplicity reasons maybe better off starting with normal BitTorrent as you proposed",
        "created_at": "2020-07-14T15:24:55.010000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "and even if we split the state into chunks, the overlay will become much more complicated, because the local B-tree will always be different from remote B-tree, I think it will be a nightmare, to be honest",
        "created_at": "2020-07-14T15:25:51.221000+00:00",
        "attachments": null
    },
    {
        "author": "thiago_crowdtainer",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Fair enough",
        "created_at": "2020-07-14T15:44:14.298000+00:00",
        "attachments": null
    },
    {
        "author": "timbeiko",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003e The single biggest obstacle to swapping the clients is database incompatibility, although importing RLP is an option in OpenEthereum (don’t know if it still works)\n\nJust catching up here, but one other thing we've seen is the integration cost is non-trivial for things like JSON-RPC and tracing APIs. If a company uses, say, Geth, and their application relies on a non-standardized call response, then the \"cost\" of adding Besu is finding all those edge cases, and making either their application more flexible, or us updating Besu to match Geth.",
        "created_at": "2020-07-14T17:25:02.726000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Yes, this in an important point, thank you",
        "created_at": "2020-07-14T19:33:40.277000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I have another question, which I do not have the answer to yet. To what extent things like Parity Substrate, Cosmos SDK and similar, contribute to an architecture of a client implementation making it easier for teams to work with?",
        "created_at": "2020-07-14T19:36:28.075000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "And another question, more to eth 2.0 designers - was the idea of something like eth2.0 substrate ever considered?",
        "created_at": "2020-07-14T19:38:03.699000+00:00",
        "attachments": null
    },
    {
        "author": "Deleted User",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "here is the version of the question I put on Twitter and Gitter: To what extent things like Parity Substrate, Cosmos SDK and similar, contribute to an architecture of a client implementation which is easier for larger dev teams to work with? Does the existence of such substrate/SDK make it more likely for one implementation to dominate?",
        "created_at": "2020-07-14T21:23:25.818000+00:00",
        "attachments": null
    }
]