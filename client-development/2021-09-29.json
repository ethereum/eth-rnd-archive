[
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "https://discord.com/channels/595666850260713488/812719315136675850/892447475229675530\n\u003c@683653554631868440\u003e That seems like a feature set that doesn't need to be part of the core protocol.  Definitely a useful thing, but I don't see why we need every client to build support for inspecting/iterating over history.",
        "created_at": "2021-09-29T03:43:09+00:00",
        "attachments": []
    },
    {
        "author": "sky.cactus",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "That is to say, archive nodes are useful, but they aren't something that everyone needs and I don't think we should be burdening the core protocol with archival node only features.",
        "created_at": "2021-09-29T03:43:42.720000+00:00",
        "attachments": []
    },
    {
        "author": "gcolvin",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I agree.",
        "created_at": "2021-09-29T05:45:21.919000+00:00",
        "attachments": []
    },
    {
        "author": "norswap",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "here's something I wrote about sync modes\ndoes it seem like an accurate description of the philosophy behind \"always download all blocks\"?\n\n\u003e A quick remark: geth always fetches all blocks (including block bodies — Nethermind allows just\n\u003e getting the headers). This is not strictly necessary.\n\u003e \n\u003e Theoretically, you at least need to download all headers to validate the chain. But you don't need\n\u003e to keep these headers around in storage. You will need to keep the N most recent blocks to handle\n\u003e re-orgs (the bigger N, the bigger the reorg you can handle without resorting to sync).\n\u003e \n\u003e If you can find a block hash off-chain that you trust, you don't even need to validate the chain —\n\u003e you can just start by trusting this block and downloading the state for it.\n\u003e \n\u003e I speculate there are two reasons why geth always downloads all blocks (including block bodies),\n\u003e even though skipping old blocks makes sync faster and reduces storage requirements for clients:\n\u003e \n\u003e 1. Skipping old blocks means that are less clients able to assist new clients with a \"proper\"\n\u003e    sync (i.e. one that at least validates proof-of-work + the chain of hashes).\n\u003e 2. Starting from a trusted hash is problematic because you need to get this hash from somewhere.\n\u003e    Inevitably people will trust one or two places to supply this hash, which opens a big centralized\n\u003e    attack vector. In practice, it probably won't matter becaue the network is already running.\n\u003e    Still, this is a bad norm to promote for what a trustless decentralized system.",
        "created_at": "2021-09-29T13:00:07.690000+00:00",
        "attachments": []
    },
    {
        "author": "norswap",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Also I know clients have some reorg caps -- the maximum depth of a re-org that can be handled. Does anybody know where I can find this value for various clients?",
        "created_at": "2021-09-29T13:05:33.666000+00:00",
        "attachments": []
    },
    {
        "author": "ghosts_in_the_code",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "found this\nhttps://medium.com/etc-core/reorg-caps-and-confirmation-delays-a-primer-on-finality-arbitration-cc9284ea9727\ngeth still uses 90k and 30k blocks in their code I guess (`FullImmutabilityThreshold`, `LightImmutabilityThreshold`)\nhttps://github.com/ethereum/go-ethereum/blob/master/params/network_params.go",
        "created_at": "2021-09-29T15:10:45.431000+00:00",
        "attachments": []
    },
    {
        "author": "ghosts_in_the_code",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "90k isn't that big (~2 weeks) so I'm not sure if they wanna legitimise deeper reorgs than that.",
        "created_at": "2021-09-29T15:17:18.865000+00:00",
        "attachments": []
    },
    {
        "author": "pipermerriam",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "A real 90k block reorg would be a massive event. There are legitimate cases where an individual client might end up with poor visibly into the network and end up on the wrong fork and end up with a deep reorg which is what I suspect these values are there to support",
        "created_at": "2021-09-29T22:13:21.324000+00:00",
        "attachments": []
    }
]