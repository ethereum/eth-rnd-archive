[
    {
        "author": "bridge-bot",
        "category": "Execution Layer",
        "parent": "",
        "content": "\u003celiastzr\u003e so if i understand correctly\nprover completeness is a big challenge for l1 liveness\nas important as prover soundness",
        "created_at": "2025-06-27T07:00:45.409000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Execution Layer",
        "parent": "",
        "content": "\u003cbarnabemonnot\u003e re: **@alexanderlhicks**\n\u003e Yes, the point I was trying to make at the start of this thread was basically this: although soundness bugs are what everyone thinks of with respect to zkVM security, issues with the witness generator or prover (intended or not) also matter once you consider integrating zkVMs into a system like the L1, so we shouldn't only care about soundness. Whilst we can formally verify circuits/verifiers, we're very unlikely to verify GPU provers, for example, so we need to deal with the case that a bug happens with probability p that prevents even an honest prover from proving an otherwise valid block.\nAs you put it: \"We should ensure that this reliance is as wide as possible, i.e., that there always exists a builder ready to deliver this block. This is not the case if we can find ourselves in a situation where only super computer-sized nodes are able to deliver, for some reason, but this can be easily mitigated by setting a network throughput limit to a level that guarantees a wide enough market, even if this limit exceeds the capabilities of local builders.\" Currently, however, we shouldn't assume that we can fall back on local provers, even with the current throughput limit, until proving costs come down another 10x or more, and one reason to use zkVMs is to increase the throughput limit so proving time will likely always take up about as much time as possible unless we explicitly add a buffer. This will particularly be true early on, which is also when software will be less tested and more likely to contain bugs.\nI am not necessarily thinking we need to fall back on \"local\" provers (in the sense of home staking nodes), but more like we should have a node somewhere, possibly not a staking one, possibly not a home operator, who should be able to make the proof and make it available",
        "created_at": "2025-06-27T10:47:59.920000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Execution Layer",
        "parent": "",
        "content": "\u003cbarnabemonnot\u003e re: **@alexanderlhicks**\n\u003e Once proving is strictly required, would a block that has not been proven be part of the chain such that it could be reorged?\nI've not looked too deep into it but there was ideas around reducing throughput when proofs are not coming early enough, basically you try to match the chain throughput back with the proving throughput, if there is for some reason a disconnect between the two. But ofc if the reason you can't prove is a bug rather than a large object, this doesn't help",
        "created_at": "2025-06-27T10:49:30.688000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Execution Layer",
        "parent": "",
        "content": "\u003celiastzr\u003e i think the separation isnâ€™t local vs. distant but rather diversity of backends such that there are CPU provers, GPU provers such that if one has a completeness problem we can fallback",
        "created_at": "2025-06-27T11:07:52.943000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Execution Layer",
        "parent": "",
        "content": "\u003csoispoke\u003e re: **@barnabemonnot**\n\u003e It sounds like this reduces to making ourselves very confident that we can never output a block that only a few parties could prove, and all these few parties refuse to prove it. I discuss such a scenario in the fifth bullet of this section https://ethresear.ch/t/decoupling-throughput-from-local-building/22004#p-53517-block-production-liveness-7\n\nActually it's a case where FOCIL could hurt, in the sense that it forces more inputs to the block. The only insurance really is to set the max throughput to a level where we have the property that one prover will step in with very high confidence. This way even if FOCIL fattens the block, it will always remain provable under that property. If a proof does not come, can the block simply be reorged then?\njust trying to make sure I understand, why would FOCIL fatten the block? with the conditional property the block can't be larger because of FOCIL at all",
        "created_at": "2025-06-27T11:10:23.723000+00:00",
        "attachments": null
    },
    {
        "author": "bridge-bot",
        "category": "Execution Layer",
        "parent": "",
        "content": "\u003cbarnabemonnot\u003e re: **@soispoke**\n\u003e just trying to make sure I understand, why would FOCIL fatten the block? with the conditional property the block can't be larger because of FOCIL at all\nThat's what is assumed, that it can't be larger than the limit that is set anyways. Was just a question regarding censorship, but I think it has more to do with can't prove rather than don't want to prove, in this case",
        "created_at": "2025-06-27T11:31:43.458000+00:00",
        "attachments": null
    }
]