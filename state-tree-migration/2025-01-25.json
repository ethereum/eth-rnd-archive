[
    {
        "author": "sophia_24798",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Does EIP-7736 make more sense to do in the same fork as the migration to a new state tree or in a subsequent fork? I’m assuming it could be refactored to work with the new binary tree proposal",
        "created_at": "2025-01-25T20:46:14.682000+00:00",
        "attachments": []
    },
    {
        "author": "jsign",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "imo, it oculd be done after. but it would be ideal to have a defined plan on how state expiry will work (e.g: if this leaf level expiry sounds convincing or not). \n\n\u003e  I’m assuming it could be refactored to work with the new binary tree proposal\nThe the eip has a small section hinting potential ways to adapt it: https://eips.ethereum.org/EIPS/eip-7864#state-expiry\nso it would be quite important to decide if that extra field or subtree will be used, so becomes an official part of the spec... even if we don't activate state expiry in the same fork.",
        "created_at": "2025-01-25T21:09:01.246000+00:00",
        "attachments": []
    },
    {
        "author": "sophia_24798",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "To be clear, you think the timing doesn’t matter from the standpoint of the migration as long as the new tree is designed with it in mind?",
        "created_at": "2025-01-25T21:13:15.537000+00:00",
        "attachments": []
    },
    {
        "author": "jsign",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I just say it's not mandatory to be activated in the same fork, but important to know to be sure the tree can support it in the future. It could matter if we decide to do state expiry in the same fork, yes. If we do that, the state conversion is an opportunity to maybe not migrate state that's too old already (i.e. prune at that point). Or maybe keep the MPT frozen, only do writes to the new tree, and after some time whatever is in the MPT would automatically expire or similar (and at that point we drop the mpt), which has a similar effect (but has other tradeoffs -- can be complicated, inefficient, or complicate too long creating proofs)",
        "created_at": "2025-01-25T21:16:44.807000+00:00",
        "attachments": []
    },
    {
        "author": "sophia_24798",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Okay. I assumed something along those lines: that we could benefit from coupling them in the same fork depending on how it’s done",
        "created_at": "2025-01-25T21:20:03.122000+00:00",
        "attachments": []
    },
    {
        "author": "jsign",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "_ultra mega fork coming..._ :P",
        "created_at": "2025-01-25T21:20:45.260000+00:00",
        "attachments": []
    },
    {
        "author": "sophia_24798",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Well, maybe it makes the migration time shorter",
        "created_at": "2025-01-25T21:21:28.279000+00:00",
        "attachments": []
    },
    {
        "author": "jsign",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Definitely sounds like a good opportunity to consider it. But of course requires a big discussion of pros and cons for sure.",
        "created_at": "2025-01-25T21:22:38.414000+00:00",
        "attachments": []
    },
    {
        "author": "sophia_24798",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I think state expiry seems less compelling in the near term if the only argument is disk requirements since we should have rolling history expiry before then. But I would guess the window of one year in 7736 was chosen arbitrarily. I am going to coordinate some analysis to see what makes sense in terms of either time or state items. It’s possible with recent database designs like the one from Layer Zero this could allow for clients to implement in-memory merklization",
        "created_at": "2025-01-25T21:25:16.453000+00:00",
        "attachments": []
    },
    {
        "author": "gballet",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "disk size isn't the only issue that state expiry solves. It also solves the database slowdown as the state grows",
        "created_at": "2025-01-25T22:17:29.791000+00:00",
        "attachments": []
    },
    {
        "author": "gballet",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "you can't do the transition and the expiry at the same time, but you could enable expiry as soon as the transition completes",
        "created_at": "2025-01-25T22:17:52.014000+00:00",
        "attachments": []
    },
    {
        "author": "gballet",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "hmmm yeah but if you expire an unconverted mpt, how do you resurect? that's the problem with MPT state expiry : we never could figure out anything that would work. 7736 is a simple approach, but it doesn't work for the current MPT",
        "created_at": "2025-01-25T22:19:18.734000+00:00",
        "attachments": []
    },
    {
        "author": "jsign",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "The resurrection would have to be an mpt root if it's coming from that era, but could be quite big and have added complexity of having to handle two kinds of revivals (potentially forever)",
        "created_at": "2025-01-25T22:22:22.011000+00:00",
        "attachments": []
    },
    {
        "author": "gballet",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "yup, and \"legacy\" revival was a huge problem a lot of people tried to crack and never could. One proposal from Vitalik, back in the day, was to actually try to revive a verkle-converted frozen state (or binary if that makes sense) but I seem to remember that implied some form of addres space extension",
        "created_at": "2025-01-25T22:24:18.672000+00:00",
        "attachments": []
    },
    {
        "author": "jsign",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003c@353136597522448385\u003e, what if if we actually migrate it to the new tree but immediately expire it in the new tree during the conversion (i mean for data tha already satisfies the expiration policy). so internal nodes are actually created and you revive leaves only in the new tree (i.e 7736 style).",
        "created_at": "2025-01-25T22:24:22.079000+00:00",
        "attachments": []
    },
    {
        "author": "sophia_24798",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "I was curious about this. It seems weird at the least",
        "created_at": "2025-01-25T22:24:41.409000+00:00",
        "attachments": []
    },
    {
        "author": "gballet",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "yeah we could do that, although I don't really see the benefit: you will still need to compute all the commitments, so no savings on the computation, and then you need to store all the stem nodes anyway. IO will be lighter, but only because the written nodes will be smaller. The amount of writes will be the same.\n\nThere are also a big challenges in doing this:  1) you need to make sure that if a value is expired in the overlay tree, the legacy one isn't being read in the previous tree and 2) because the hash order can be changed, two leaves that are \"close\" in the new tree (i.e. in the same 256 group) are not close at all in keccack order. So during the conversion, we find the need to \"reopen\" groups. If something is expired, you can't do that. So that's a complication for sure.",
        "created_at": "2025-01-25T22:31:39.846000+00:00",
        "attachments": []
    },
    {
        "author": "gballet",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Honestly I think it's simpler to separate the two things, the state expiry can be a small fork after the big fork. We can figure out how to do both at the same time (and we might by the time EOF ships) but as a single fork it would make things even more complex, and they really don't need to be.",
        "created_at": "2025-01-25T22:33:28.456000+00:00",
        "attachments": []
    },
    {
        "author": "jsign",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "The main benefit is mostly already remove state that is expired (reg your 2) point, i don't think we need to reopen groups, since we can calculate upfront which stems (new tree) satisfy the expiration policy (all 256 values are expired)).\nSo yeah, despite it's doable/solvable, if frontrunning the pruning (disk size wise) isn't worth it, I agree it's better to not deal with that in the conversion and let time start running after it with the expected mechanism.",
        "created_at": "2025-01-25T22:37:21.091000+00:00",
        "attachments": []
    },
    {
        "author": "gballet",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "the reason why we need to reopen the group, is to compute the final commitment. I don't think you can go around storing all data until you know the account has been converted, especially in a binary tree context.",
        "created_at": "2025-01-25T22:39:21.938000+00:00",
        "attachments": []
    },
    {
        "author": "jsign",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Something else about state expiry is that state revivals will also increase bandwidth usage, so depending on how blobs scaling is making progress there's a timing decision on resource usage.",
        "created_at": "2025-01-25T22:39:27.282000+00:00",
        "attachments": []
    },
    {
        "author": "gballet",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "right",
        "created_at": "2025-01-25T22:39:39.454000+00:00",
        "attachments": []
    }
]