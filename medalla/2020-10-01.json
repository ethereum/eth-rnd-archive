[
    {
        "author": "donschoe",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Good morning, my disk was full when I woke up this morning. It turns out that it was due to my 5 clients on Medalla.\n\n* Lighthouse 146 GB\n* Nimbus 130 GB\n* Lodestar 60.8 GB\n* Teku 12.3 GB\n* Prysm 4.3 GB\n\nAre Teku and Prysm pruning? Why is there such a high difference in database size required?",
        "created_at": "2020-10-01T07:15:18.234000+00:00",
        "attachments": null
    },
    {
        "author": "paulhauner",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "We had a pruning bug fixed several versions ago. You need to resync to benefit from reduced size.",
        "created_at": "2020-10-01T07:36:43.663000+00:00",
        "attachments": null
    },
    {
        "author": "benjaminion",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Teku prunes by default. Our DB (RocksDB) is a little eccentric - it grows for a while and every few hours gets compacted down to a fraction of the size. So the size you see depends on where in this cycle you measure it.",
        "created_at": "2020-10-01T07:46:06.772000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "nimbus has some recent pruning fixes as well - resync would take care of it - there's probably log files in there as well that might be over the top - we're working to reduce those",
        "created_at": "2020-10-01T12:03:22.305000+00:00",
        "attachments": null
    },
    {
        "author": "terence0083",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "Prysm by default saves beacon state to DB every 2048 slots. 4.3GB sounds about right to me. This assumes the beacon nodes don't have to serve much historical requests because it's slower to generate historical states. For \"histroical data friendly\" beacon node, we can adjust `--slots-per-archived-point` (e.g. setting that to 64 means saving a beacon state to db every 64 slots) LH has a great doc on this: https://lighthouse-book.sigmaprime.io/advanced_database.html My apology that prysm is lacking this doc, we'll get it done before mainnnet",
        "created_at": "2020-10-01T13:37:54.487000+00:00",
        "attachments": null
    },
    {
        "author": "djrtwo",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "can confirm that recent lighthouse data-dir is 6GB on default settings and recent release",
        "created_at": "2020-10-01T13:39:32.657000+00:00",
        "attachments": null
    },
    {
        "author": "donschoe",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "ok, I'm resyncing all of them. the databases are as old as Medalla ðŸ™‚",
        "created_at": "2020-10-01T14:32:06.560000+00:00",
        "attachments": null
    },
    {
        "author": "donschoe",
        "category": "Channel Graveyard",
        "parent": "",
        "content": "I've compared sync metrics again, this time 400k slots on Medalla.\n\nHere's the data: https://github.com/q9f/eth2-bench-2020-10",
        "created_at": "2020-10-01T14:47:25.347000+00:00",
        "attachments": null
    }
]