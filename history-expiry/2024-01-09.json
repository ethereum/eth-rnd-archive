[
    {
        "author": "arnetheduck",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "snappy is not a deterministic compression algorithm overall - ie two snappy implementations can compress the same data to different compressed outputs (even different versions of the same library do this - it depends on which hashing function is used internally in the library among other things).\n\nframed has a few general benefits - it comes with a quick-and-dirty crc32 for checking integrity (we don't _really_ need it given we could decode the data), it puts a cap on memory requirements for reading (ie the data can be streamed in blocks), it is used for serving data in the consensus layer already meaning no recompression is needed",
        "created_at": "2024-01-09T04:52:02.080000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "framed also carries a crc32 of the uncompressed data as far as checksums go - obviously, all data is also covered by ethereum hashes",
        "created_at": "2024-01-09T04:52:55.627000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "regarding compressed vs uncompressed, the latter are _generally_ used by the compressor to store data that doesn't compress well - it would be trivial to require that all frames are compressed or that some specific threshold is used to select between them, but that brings us back to the original problem, which is that the compression level is determined by the snappy library in use (and it would be unwise to require a specific library I think - each language has its own, some languages have multiple)",
        "created_at": "2024-01-09T04:56:18.852000+00:00",
        "attachments": null
    },
    {
        "author": "ak4222",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Thanks for the clarification! Do you believe there is value in considering alternative Compression algos?",
        "created_at": "2024-01-09T11:44:20.855000+00:00",
        "attachments": null
    },
    {
        "author": "ak4222",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Given two assumptions:",
        "created_at": "2024-01-09T11:45:22.868000+00:00",
        "attachments": null
    },
    {
        "author": "ak4222",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "pre-merge data will not be served on the consensus layer",
        "created_at": "2024-01-09T11:45:25.596000+00:00",
        "attachments": null
    },
    {
        "author": "ak4222",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "an alternative like Zstandard, could provide deterministic output that can be checksummed (I could be wrong about Zstandard, but that is my understanding of it)",
        "created_at": "2024-01-09T11:46:30.067000+00:00",
        "attachments": null
    },
    {
        "author": "ak4222",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "\u003c@449019668296892420\u003e I really appreciate your input!",
        "created_at": "2024-01-09T11:48:17.017000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "no",
        "created_at": "2024-01-09T11:49:21.652000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "zstd has even more variability in output - this is true of nearly all compression algorthms that trade memory for performance (both ways) - in any case, in something like portal (and really, in most storage solutions / cases), you want to identify data by its uncompressed canonical hash, not post-compression checksum",
        "created_at": "2024-01-09T11:51:15.052000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "choosing compression is always a tradeoff between many different and often competing interests - however, for archival, I believe a primary concern is to keep it simple for posterity to access and snappy is a trivial compression algorithm whose decoding can be implemented in a page of code - it's also not terrible in terms of how well it compresses - the main thing that speaks for it is its simplicity and availability",
        "created_at": "2024-01-09T11:55:47.815000+00:00",
        "attachments": null
    },
    {
        "author": "arnetheduck",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "ie 5-10 years from now, there will be another player that will have replaced zstd in the fancy-new-compression department (just like gz, bz2, lzma etc before it).. I would (probably) have gone with zlib actually if it wasn't for the fact that snappy already necessarily exists in all clients",
        "created_at": "2024-01-09T11:58:04.395000+00:00",
        "attachments": null
    },
    {
        "author": "ak4222",
        "category": "Execution R\u0026D",
        "parent": "",
        "content": "Thanks for the input!",
        "created_at": "2024-01-09T12:02:11.911000+00:00",
        "attachments": null
    }
]